# ========================================
# Application Settings
# ========================================
APP_ENV=development

# Vector store persistence
# Path where FAISS indices will be saved
VECTOR_STORE_PATH=./data/vector_store

# ========================================
# LLM Provider Configuration
# ========================================
# Choose LLM provider: "fake", "openai", or "openrouter"
LLM_PROVIDER=fake
LLM_MODEL=openai/gpt-3.5-turbo

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000  # 建议值: 简短回答=500, 标准=2000, 详细=4096
LLM_TOP_P=1.0
LLM_FREQUENCY_PENALTY=0.0
LLM_PRESENCE_PENALTY=0.0
LLM_N=1
LLM_STREAM=false

# Reliability
LLM_TIMEOUT=60.0
LLM_MAX_RETRIES=3

# ========================================
# Embedding Provider Configuration
# ========================================
# Choose embedding provider: "fake", "openai", or "openrouter"
EMBEDDING_PROVIDER=fake

# Embedding model name (provider-specific)
EMBEDDING_MODEL=openai/text-embedding-3-small

# Optional: Specify embedding dimensions (if supported by model)
# EMBEDDING_DIMENSIONS=1536

# For testing/development (when EMBEDDING_PROVIDER=fake)
FAKE_EMBEDDING_SIZE=768

# Embedding parameters
# EMBEDDING_ENCODING_FORMAT=float
# EMBEDDING_TIMEOUT=60.0
# EMBEDDING_MAX_RETRIES=3

# ========================================
# Retriever Configuration
# ========================================
# Retriever type: "vector" or "hybrid"
#   - vector: Pure semantic search (faster, good for general queries)
#   - hybrid: BM25 + Vector fusion (better accuracy, recommended for production)
#             Especially good for: technical terms, proper nouns, exact matches
#             Requires: pip install rank-bm25
RETRIEVER_TYPE=vector

# Number of documents to retrieve
VECTOR_TOP_K=4

# Search type for vector retriever: "similarity" or "mmr"
#   - similarity: Standard semantic similarity
#   - mmr: Maximum Marginal Relevance (for diversity)
RETRIEVER_SEARCH_TYPE=similarity

# Minimum similarity threshold (0-1)
RETRIEVER_SIMILARITY_THRESHOLD=0.7

# MMR lambda parameter for diversity (0-1)
# Higher = more diversity, Lower = more relevance
# RETRIEVER_LAMBDA_MULT=0.5

# --- Hybrid Retriever Parameters ---
# Only used when RETRIEVER_TYPE=hybrid
# Weights for hybrid retrieval (normalized automatically, don't need to sum to 1.0)
RETRIEVER_VECTOR_WEIGHT=0.7  # Semantic search weight (recommended: 0.6-0.8)
RETRIEVER_BM25_WEIGHT=0.3     # Keyword search weight (recommended: 0.2-0.4)

# RRF (Reciprocal Rank Fusion) constant for score fusion
# Higher = more equal weighting across ranks (recommended: 60)
RETRIEVER_RRF_K=60

# ========================================
# Memory Configuration (Conversational RAG)
# ========================================
# Conversation-level memory (short-term, within same conversation)
MEMORY_CONVERSATION_LIMIT=2
MEMORY_CONVERSATION_THRESHOLD=0.75

# Topic-level memory (long-term, across conversations in same topic)
MEMORY_TOPIC_LIMIT=3
MEMORY_TOPIC_THRESHOLD=0.70

# Total memory limit
MEMORY_MAX_TOTAL=5

# Enable/disable memory retrieval
MEMORY_ENABLE_CONVERSATION=true
MEMORY_ENABLE_TOPIC=true

# ========================================
# Database Configuration
# ========================================
# PostgreSQL connection URL
# Format: postgresql://[user]:[password]@[host]:[port]/[database]
DATABASE_URL=postgresql://postgres:password@localhost:5432/knowledge_platform

# Connection pool settings
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
# DB_ECHO=false  # Set to true to log all SQL queries (development only)

# ========================================
# Cache Configuration (Redis)
# ========================================
# Enable Redis caching for query results
CACHE_ENABLED=false

# Redis connection URL
REDIS_URL=redis://localhost:6379

# Redis connection pool size
REDIS_MAX_CONNECTIONS=10

# Cache TTL in seconds (default: 1 hour)
CACHE_TTL_SECONDS=3600

# ========================================
# Reranker Configuration
# ========================================
# Enable document reranking for higher accuracy (recommended for production)
#   - Reranking uses Cross-Encoder models to precisely score query-document relevance
#   - Significantly improves top result quality (40%+ accuracy gain)
#   - Slower than initial retrieval, so use as 2nd stage after retrieval
#   - Requires: pip install sentence-transformers
RERANKER_ENABLED=false

# Reranker type: "cross_encoder" (llm/cohere coming soon)
RERANKER_TYPE=cross_encoder

# Cross-encoder model (when RERANKER_TYPE=cross_encoder)
#   Available models (from fast to slow, quality increases):
#   - cross-encoder/ms-marco-TinyBERT-L-2-v2      (fastest, good quality)
#   - cross-encoder/ms-marco-MiniLM-L-6-v2        (balanced, recommended)
#   - cross-encoder/ms-marco-MiniLM-L-12-v2       (best quality, slower)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Number of final results after reranking
#   Tip: Set VECTOR_TOP_K higher (20-30) to give reranker more candidates
RERANKER_TOP_N=5

# Batch size for cross-encoder inference
#   Higher = faster throughput, but more memory
#   Reduce if you get OOM errors
RERANKER_BATCH_SIZE=32

# ========================================
# Observability Configuration
# ========================================

# --- LangSmith Tracing ---
# Enable LangSmith for tracing and debugging
LANGSMITH_ENABLED=false

# LangSmith API key (get from https://smith.langchain.com)
# LANGCHAIN_API_KEY=lsv2_your_langsmith_api_key_here

# LangSmith project name
LANGCHAIN_PROJECT=rag-production

# --- Prometheus Metrics ---
# Enable Prometheus metrics collection
PROMETHEUS_ENABLED=false

# Prometheus metrics port (separate from main API)
PROMETHEUS_PORT=9090

# --- Logging ---
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: "json" or "text"
LOG_FORMAT=json

# ========================================
# OpenAI Configuration
# ========================================
# Uncomment and set these when EMBEDDING_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_BASE_URL=https://api.openai.com/v1

# ========================================
# OpenRouter Configuration
# ========================================
# Uncomment and set these when EMBEDDING_PROVIDER=openrouter
# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-your-openrouter-api-key-here

# Optional: For OpenRouter rankings (recommended)
# OPENROUTER_SITE_URL=https://yoursite.com
# OPENROUTER_SITE_NAME=Your App Name

# ========================================
# Document Parser Configuration
# ========================================
# Choose document parser: "default" (standard loaders) or "langextract" (AI-powered)
DOCUMENT_PARSER_TYPE=langextract

# LangExtract Configuration (when DOCUMENT_PARSER_TYPE=langextract)
# Get your API key from: https://aistudio.google.com/apikey (for Gemini)
LANGEXTRACT_MODEL_ID=gemini-2.5-flash
LANGEXTRACT_API_KEY=your-langextract-api-key-here
LANGEXTRACT_PROVIDER=gemini
LANGEXTRACT_BASE_URL=
LANGEXTRACT_USE_SCHEMA=true
LANGEXTRACT_FENCE_OUTPUT=false

# Enable/disable AI enhancement for document parsing
PARSER_ENABLE_ENHANCED=true

# Note: If using OpenRouter for LangExtract, it will automatically use
# OPENROUTER_API_KEY, OPENROUTER_SITE_URL, and OPENROUTER_SITE_NAME
# from the OpenRouter configuration section above

# ========================================
# Example Configurations
# ========================================

# --- Example 1: Development (Fake Providers) ---
# LLM_PROVIDER=fake
# EMBEDDING_PROVIDER=fake
# FAKE_EMBEDDING_SIZE=768

# --- Example 2: OpenAI (LLM + Embeddings) ---
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=openai/text-embedding-3-small
# OPENAI_API_KEY=sk-your-key-here

# --- Example 3: OpenRouter with GPT-3.5 + Google Gemini Embeddings ---
# LLM_PROVIDER=openrouter
# LLM_MODEL=openai/gpt-3.5-turbo
# EMBEDDING_PROVIDER=openrouter
# EMBEDDING_MODEL=google/gemini-embedding-001
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# OPENROUTER_SITE_URL=https://yoursite.com
# OPENROUTER_SITE_NAME=Your App

# --- Example 4: OpenRouter with Claude + OpenAI Embeddings ---
# LLM_PROVIDER=openrouter
# LLM_MODEL=anthropic/claude-3-sonnet
# LLM_TEMPERATURE=0.5
# EMBEDDING_PROVIDER=openrouter
# EMBEDDING_MODEL=openai/text-embedding-3-small
# EMBEDDING_DIMENSIONS=1536
# OPENROUTER_API_KEY=sk-or-v1-your-key-here

# --- Example 5: LangExtract with Gemini for Document Parsing ---
# DOCUMENT_PARSER_TYPE=langextract
# LANGEXTRACT_MODEL_ID=gemini-2.5-flash
# LANGEXTRACT_API_KEY=your-gemini-api-key
# LANGEXTRACT_PROVIDER=gemini
# PARSER_ENABLE_ENHANCED=true

# --- Example 6: LangExtract with OpenAI for Document Parsing ---
# DOCUMENT_PARSER_TYPE=langextract
# LANGEXTRACT_MODEL_ID=gpt-4o
# LANGEXTRACT_API_KEY=your-openai-api-key
# LANGEXTRACT_PROVIDER=openai
# LANGEXTRACT_FENCE_OUTPUT=true
# LANGEXTRACT_USE_SCHEMA=false

# --- Example 7: LangExtract with OpenRouter (Claude) ---
# DOCUMENT_PARSER_TYPE=langextract
# LANGEXTRACT_MODEL_ID=anthropic/claude-3-5-sonnet
# LANGEXTRACT_PROVIDER=openrouter
# LANGEXTRACT_BASE_URL=https://openrouter.ai/api/v1
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# OPENROUTER_SITE_URL=https://yoursite.com
# OPENROUTER_SITE_NAME=Your App
# PARSER_ENABLE_ENHANCED=true

# --- Example 8: LangExtract with OpenRouter (GPT-4) ---
# DOCUMENT_PARSER_TYPE=langextract
# LANGEXTRACT_MODEL_ID=openai/gpt-4-turbo
# LANGEXTRACT_PROVIDER=openrouter
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# PARSER_ENABLE_ENHANCED=true

# --- Example 9: Disable AI Parser (Use Standard Loaders) ---
# DOCUMENT_PARSER_TYPE=default
# PARSER_ENABLE_ENHANCED=false

# --- Example 10: Hybrid Retriever for Better Accuracy ---
# RETRIEVER_TYPE=hybrid
# RETRIEVER_VECTOR_WEIGHT=0.7
# RETRIEVER_BM25_WEIGHT=0.3
# RETRIEVER_RRF_K=60
# VECTOR_TOP_K=5
# Note: Requires 'pip install rank-bm25'

# --- Example 11: Hybrid Retriever for Technical Documentation ---
# RETRIEVER_TYPE=hybrid
# RETRIEVER_VECTOR_WEIGHT=0.6  # Lower vector weight
# RETRIEVER_BM25_WEIGHT=0.4     # Higher keyword weight for exact matches
# VECTOR_TOP_K=8                # More candidates for better fusion

# --- Example 12: Production Setup with All Enhancements ---
# # LLM & Embeddings
# LLM_PROVIDER=openrouter
# LLM_MODEL=anthropic/claude-3-5-sonnet
# EMBEDDING_PROVIDER=openrouter
# EMBEDDING_MODEL=openai/text-embedding-3-small
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# 
# # Hybrid Retriever + Reranker
# RETRIEVER_TYPE=hybrid
# RETRIEVER_VECTOR_WEIGHT=0.7
# RETRIEVER_BM25_WEIGHT=0.3
# VECTOR_TOP_K=10
# RERANKER_ENABLED=true
# RERANKER_TYPE=cross_encoder
# RERANKER_TOP_N=5
# 
# # Caching & Observability
# CACHE_ENABLED=true
# REDIS_URL=redis://localhost:6379
# LANGSMITH_ENABLED=true
# LANGCHAIN_API_KEY=lsv2_your_key_here
# PROMETHEUS_ENABLED=true

