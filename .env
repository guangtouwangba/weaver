# OpenAI API Key (for GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models)
ANTHROPIC_API_KEY=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

# Vector Database Configuration
VECTOR_DB_PATH=./data/vector_db
MAX_PAPERS_PER_QUERY=50
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Agent Configuration
DEFAULT_MODEL=gpt-4o-mini
RESEARCH_TOPICS_CACHE_TTL=3600

# Logging
LOG_LEVEL=INFO


# Research Agent RAG System Configuration
# Copy this file to .env and fill in your actual values

# ===== API KEYS =====
# At least one API key is required
OPENAI_API_KEY=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
DEEPSEEK_API_KEY=sk-e522840d73e44f9d99e40cd3f9507944
ANTHROPIC_API_KEY=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

# ===== PROVIDER CONFIGURATION =====
# Default provider for all agents (openai, deepseek, anthropic)
DEFAULT_PROVIDER=deepseek

# ===== MODEL CONFIGURATION =====
# Default models for each provider
OPENAI_MODEL=gpt-4o-mini
DEEPSEEK_MODEL=deepseek-chat
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ===== INDIVIDUAL AGENT CONFIGURATION =====
# Option 1: Individual environment variables (simpler)
# Setting ALL agents to use DeepSeek
GOOGLE_ENGINEER_PROVIDER=deepseek
GOOGLE_ENGINEER_MODEL=deepseek-chat
MIT_RESEARCHER_PROVIDER=deepseek
MIT_RESEARCHER_MODEL=deepseek-chat
INDUSTRY_EXPERT_PROVIDER=deepseek
INDUSTRY_EXPERT_MODEL=deepseek-chat
PAPER_ANALYST_PROVIDER=deepseek
PAPER_ANALYST_MODEL=deepseek-chat

# Option 2: JSON configuration (more flexible, overrides individual vars)
# AGENT_CONFIGS={"google_engineer": {"provider": "openai", "model": "gpt-4o"}, "mit_researcher": {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"}}

# ===== SEARCH CONFIGURATION =====
# Minimum similarity threshold for vector search results
MIN_SIMILARITY_THRESHOLD=0.5

# Enable ArXiv fallback search when local results are insufficient
ENABLE_ARXIV_FALLBACK=true

# Maximum papers to fetch from ArXiv fallback
ARXIV_FALLBACK_MAX_PAPERS=10

# Enable AI-powered query expansion
ENABLE_QUERY_EXPANSION=true

# Maximum number of query expansions to generate
MAX_QUERY_EXPANSIONS=3

# ===== AGENT DISCUSSION CONFIGURATION =====
# Enable multi-agent discussions and insights
ENABLE_AGENT_DISCUSSIONS=true

# Default selected agents (comma-separated)
DEFAULT_SELECTED_AGENTS=Google Engineer,MIT Researcher,Industry Expert,Paper Analyst

# ===== RESEARCH PARAMETERS =====
# Default maximum papers per research session
DEFAULT_MAX_PAPERS=20

# Default setting for including recent papers
DEFAULT_INCLUDE_RECENT=true


# Log file path
LOG_FILE=logs/research_agent.log

# ===== SERVER CONFIGURATION =====
# Streamlit server settings
STREAMLIT_HOST=localhost
STREAMLIT_PORT=8501



# ===== VECTOR DATABASE CONFIGURATION =====
# Choose your vector database provider: chroma, pinecone, weaviate, qdrant
VECTOR_DB_TYPE=pinecone
VECTOR_DB_COLLECTION=research-papers

# ChromaDB Configuration (Local/Self-hosted)
VECTOR_DB_PATH=./data/vector_db
CHROMA_HOST=
CHROMA_PORT=8000

# Pinecone Configuration (Cloud)
PINECONE_API_KEY=8c863bb4-44ed-429d-8ca1-9fc3b57f7b75
PINECONE_INDEX_NAME=research-papers
PINECONE_ENVIRONMENT=us-west1
PINECONE_DIMENSION=1024

# Weaviate Configuration (Cloud/Self-hosted)
WEAVIATE_URL=https://your-cluster.weaviate.network
WEAVIATE_API_KEY=your_weaviate_api_key_here
WEAVIATE_CLASS_NAME=ResearchPaper

# Qdrant Configuration (Cloud/Self-hosted)
QDRANT_HOST=your-cluster.qdrant.io
QDRANT_PORT=6333
QDRANT_API_KEY=your_qdrant_api_key_here
QDRANT_COLLECTION=research-papers
QDRANT_VECTOR_SIZE=384

# ===== EMBEDDING MODEL CONFIGURATION =====
# Choose your embedding provider: openai, anthropic, huggingface, deepseek
EMBEDDING_MODEL_TYPE=openai

# OpenAI Embeddings
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Embeddings
ANTHROPIC_EMBEDDING_MODEL=claude-3-haiku-20240307

# HuggingFace Embeddings (Local)
HUGGINGFACE_EMBEDDING_MODEL=all-MiniLM-L6-v2

# DeepSeek Embeddings
DEEPSEEK_EMBEDDING_MODEL=deepseek-embedding

# ===== PROCESSING CONFIGURATION =====


POSTGRES_HOST=127.0.0.1
POSTGRES_PORT=5433
POSTGRES_DB=research_agent
POSTGRES_USER=research_user
POSTGRES_PASSWORD=research_password
POSTGRES_SSL_MODE=disable
POSTGRES_MAX_CONNECTIONS=20
POSTGRES_CONNECTION_TIMEOUT=30


WEAVIATE_HOST=localhost
WEAVIATE_PORT=8088
WEAVIATE_SCHEME=http
WEAVIATE_URL=
WEAVIATE_API_KEY=your_weaviate_api_key_here
WEAVIATE_CLASS_NAME=ResearchPaper

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=redis_password
REDIS_DB=0
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_TIMEOUT=5
REDIS_SOCKET_KEEPALIVE=true

# ===== CELERY CONFIGURATION =====
# Celery broker and result backend (using Redis)
CELERY_BROKER_URL=redis://:redis_password@localhost:6379/0
CELERY_RESULT_BACKEND=redis://:redis_password@localhost:6379/0
CELERY_TASK_SERIALIZER=json
CELERY_RESULT_SERIALIZER=json
CELERY_ACCEPT_CONTENT=json
CELERY_TIMEZONE=UTC
CELERY_ENABLE_UTC=true
CELERY_WORKER_PREFETCH_MULTIPLIER=1
CELERY_TASK_ACKS_LATE=true
CELERY_RESULT_EXPIRES=3600
CELERY_TASK_DEFAULT_RETRY_DELAY=60
CELERY_TASK_MAX_RETRIES=3