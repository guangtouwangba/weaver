# ====================================
# Backend Environment Variables
# ====================================
# Copy this file to .env and fill in the values

# ====================================
# Database Configuration
# ====================================
# PostgreSQL connection string with asyncpg driver
DATABASE_URL=postgresql+asyncpg://research_rag:research_rag_dev@localhost:5432/research_rag

# Database Client Type
# "postgres" - Direct PostgreSQL connection (default for local development)
# "supabase" - Supabase SDK (for production/cloud)
# "sqlalchemy" - SQLAlchemy wrapper (for backward compatibility)
DATABASE_CLIENT_TYPE=postgres

# For Supabase (production):
# ====================================
# Transaction Mode (Recommended) - Port 6543
# ====================================
# Uses connection pooling, supports more concurrent connections
# Best for: Applications with high concurrency
# Format: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres
# Example:
# DATABASE_URL=postgresql://postgres.abcdefghijklmnop:[password]@aws-0-us-east-1.pooler.supabase.com:6543/postgres

# ====================================
# Session Mode (Alternative) - Port 5432
# ====================================
# Direct connection, limited concurrent connections
# Best for: Long-running connections, migrations
# Format: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:5432/postgres
# Example:
# DATABASE_URL=postgresql://postgres.abcdefghijklmnop:[password]@aws-0-us-east-1.pooler.supabase.com:5432/postgres

# ====================================
# OpenRouter API Configuration
# ====================================
# Get your API key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# LLM Model (default: gpt-4o-mini)
# Other options: openai/gpt-4o, anthropic/claude-3.5-sonnet, etc.
LLM_MODEL=openai/gpt-4o-mini

# Embedding Model (OpenRouter supports OpenAI embedding models)
# Must use format: openai/text-embedding-3-small
EMBEDDING_MODEL=openai/text-embedding-3-small

# ====================================
# OpenAI API Configuration (Optional)
# ====================================
# Optional: Use OpenAI API directly instead of OpenRouter for embeddings
# Get your API key at: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-key-here

# ====================================
# Storage Configuration
# ====================================
# Local file upload directory
UPLOAD_DIR=./data/uploads

# Supabase Storage (optional, for production)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
STORAGE_BUCKET=documents

# ====================================
# CORS Configuration
# ====================================
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# For production:
# CORS_ORIGINS=https://your-frontend-domain.zeabur.app

# ====================================
# Environment & Logging
# ====================================
# Environment: development | production
ENVIRONMENT=development

# Log level: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# ====================================
# Loki Logging Configuration
# ====================================
# Enable/disable Loki logging
LOKI_ENABLED=false

# Option 1: Local Loki (via docker-compose)
# LOKI_URL=http://localhost:3100/loki/api/v1/push
# LOKI_ENABLED=true

# Option 2: Remote Loki on Zeabur (for debugging production issues locally)
# LOKI_URL=https://your-loki-domain.zeabur.app/loki/api/v1/push
# LOKI_ENABLED=true

# Option 3: Zeabur internal network (for deployed services)
LOKI_URL=https://research-rag-loki.zeabur.app/loki/api/v1/push
# LOKI_ENABLED=true

# ====================================
# Langfuse - LLM Observability
# ====================================
# Enable Langfuse tracing for LLM calls
# See: https://langfuse.com/docs/integrations/langchain
LANGFUSE_ENABLED=false

# Langfuse API keys (get from your Langfuse project settings)
# For Langfuse Cloud: https://cloud.langfuse.com
# For self-hosted: Your Langfuse instance URL
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key

# Langfuse host URL
# - Langfuse Cloud: https://cloud.langfuse.com (default)
# - Self-hosted on Zeabur: https://langfuse-xxx.zeabur.app
# - Local development: http://localhost:3000
LANGFUSE_HOST=https://cloud.langfuse.com

# ====================================
# Retrieval Configuration
# ====================================
# Number of top similar documents to retrieve for RAG
# Higher values = more context but slower and more expensive
# Recommended: 3-10 for most use cases
RETRIEVAL_TOP_K=5

# Minimum similarity threshold (0.0 to 1.0)
# Only return chunks with similarity >= this value
# 0.0 = no filter (return all top_k results)
# 0.7 = only return highly similar chunks (recommended for quality)
RETRIEVAL_MIN_SIMILARITY=0.0

# ====================================
# Evaluation - Ragas
# ====================================
# Enable real-time RAG evaluation after each query
# This will automatically evaluate answer quality using Ragas metrics
EVALUATION_ENABLED=false

# Sample rate: proportion of queries to evaluate (0.0 to 1.0)
# 0.1 = evaluate 10% of queries (recommended for cost control)
# 1.0 = evaluate all queries (expensive but comprehensive)
EVALUATION_SAMPLE_RATE=0.1

# ====================================
# Long Context RAG Mode Configuration
# ====================================
# RAG mode: traditional | long_context | auto
# - traditional: Use chunk-based retrieval (default)
# - long_context: Use full document context (NotebookLM style)
# - auto: Automatically choose based on document size
RAG_MODE=traditional

# Safety ratio for long context mode (0.0 to 1.0)
# Use this percentage of model's context window to leave room for prompt and response
# 0.55 = use 55% of context window (conservative, recommended)
# 0.80 = use 80% of context window (aggressive)
LONG_CONTEXT_SAFETY_RATIO=0.55

# Minimum tokens to use long context mode
# Documents smaller than this will use traditional retrieval (more efficient)
# Recommended: 10000 tokens (about 25,000 characters for mixed content)
LONG_CONTEXT_MIN_TOKENS=10000

# Enable citation grounding (requires LLM to cite sources)
# When enabled, LLM will add citation markers to each factual claim
ENABLE_CITATION_GROUNDING=true

# Citation format: inline | structured | both
# - inline: [doc_id:page:start:end] format
# - structured: JSON format with detailed citation objects
# - both: Support both formats
CITATION_FORMAT=both

# ====================================
# Development Tips
# ====================================
# 1. Never commit .env file to git
# 2. Update this .env.example when adding new variables
# 3. Use strong passwords for production databases
# 4. Keep API keys secure and rotate regularly

