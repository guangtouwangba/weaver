# ===================================
# RAG Knowledge Management System
# 环境变量配置示例
# ===================================
# 
# 复制此文件为 .env 并根据实际情况修改配置值
# 注意：.env 文件包含敏感信息，不应提交到版本控制系统
#

# ===================================
# 应用基础配置
# ===================================

# 应用名称
APP_NAME=RAG API

# 应用版本
APP_VERSION=2.0.0

# 调试模式 (true/false)
DEBUG=false

# 服务器主机
HOST=0.0.0.0

# 服务器端口
PORT=8000

# 运行环境 (development/testing/staging/production)
ENVIRONMENT=development

# ===================================
# CORS 配置
# ===================================

# 允许的CORS源（逗号分隔）
CORS_ORIGINS=*

# 允许的CORS方法（逗号分隔）
CORS_METHODS=*

# 允许的CORS头（逗号分隔）
CORS_HEADERS=*

# ===================================
# 数据库配置
# ===================================

# 数据库连接URL（可选，会覆盖下面的详细配置）
# DATABASE_URL=postgresql+asyncpg://username:password@localhost:5432/ragdb

# 数据库主机
DATABASE__HOST=localhost

# 数据库端口
DATABASE__PORT=5432

# 数据库名称
DATABASE__NAME=ragdb

# 数据库用户名（留空使用系统用户名）
DATABASE__USER=

# 数据库密码
DATABASE__PASSWORD=

# 数据库驱动 (asyncpg/psycopg2)
DATABASE__DRIVER=asyncpg

# 连接池大小
DATABASE__POOL_SIZE=5

# 最大溢出连接数
DATABASE__MAX_OVERFLOW=10

# 连接池超时时间（秒）
DATABASE__POOL_TIMEOUT=30

# 连接回收时间（秒）
DATABASE__POOL_RECYCLE=3600

# 是否显示SQL日志
DATABASE__ECHO=false

# ===================================
# 存储配置
# ===================================

# 存储提供商 (local/minio/s3/gcs/oss)
STORAGE__PROVIDER=local

# 存储桶名称
STORAGE__BUCKET_NAME=rag-files

# 本地存储路径
STORAGE__LOCAL_PATH=./storage

# MinIO 配置
STORAGE__MINIO_ENDPOINT=localhost:9000
STORAGE__MINIO_ACCESS_KEY=minioadmin
STORAGE__MINIO_SECRET_KEY=minioadmin123
STORAGE__MINIO_SECURE=false

# AWS S3 配置
STORAGE__AWS_ACCESS_KEY_ID=
STORAGE__AWS_SECRET_ACCESS_KEY=
STORAGE__AWS_REGION=us-east-1

# ===================================
# 日志配置
# ===================================

# 日志级别 (DEBUG/INFO/WARNING/ERROR/CRITICAL)
LOGGING__LEVEL=INFO

# 日志格式
LOGGING__FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# 日志文件路径（留空输出到控制台）
LOGGING__FILE=

# 日志文件最大大小（MB）
LOGGING__MAX_SIZE=10

# 日志文件备份数量
LOGGING__BACKUP_COUNT=5

# ===================================
# Redis 配置
# ===================================

# Redis 主机
REDIS__HOST=localhost

# Redis 端口
REDIS__PORT=6379

# Redis 数据库编号
REDIS__DB=0

# Redis 密码
REDIS__PASSWORD=

# Redis 用户名
REDIS__USERNAME=

# 最大连接数
REDIS__MAX_CONNECTIONS=50

# Socket 超时时间（秒）
REDIS__SOCKET_TIMEOUT=5.0

# Socket 连接超时时间（秒）
REDIS__SOCKET_CONNECT_TIMEOUT=5.0

# 是否启用 Socket keepalive
REDIS__SOCKET_KEEPALIVE=true

# 超时时是否重试
REDIS__RETRY_ON_TIMEOUT=true

# 是否使用 SSL
REDIS__SSL=false

# SSL 密钥文件路径
REDIS__SSL_KEYFILE=

# SSL 证书文件路径
REDIS__SSL_CERTFILE=

# SSL 证书要求
REDIS__SSL_CERT_REQS=required

# SSL CA 证书路径
REDIS__SSL_CA_CERTS=

# 是否检查 SSL 主机名
REDIS__SSL_CHECK_HOSTNAME=false

# 默认过期时间（秒）
REDIS__DEFAULT_TTL=3600

# 键前缀
REDIS__KEY_PREFIX=rag:

# 序列化器 (json/pickle/msgpack)
REDIS__SERIALIZER=json

# 是否压缩数据
REDIS__COMPRESS=false

# 压缩阈值（字节）
REDIS__COMPRESS_THRESHOLD=1024

# 是否解码响应
REDIS__DECODE_RESPONSES=true

# 编码
REDIS__ENCODING=utf-8

# 编码错误处理
REDIS__ENCODING_ERRORS=strict

# 健康检查间隔（秒）
REDIS__HEALTH_CHECK_INTERVAL=30

# 健康检查阈值
REDIS__HEALTH_CHECK_THRESHOLD=50

# ===================================
# Celery 配置
# ===================================

# 消息代理 URL
CELERY__BROKER_URL=redis://localhost:6379/0

# 结果后端 URL
CELERY__RESULT_BACKEND=redis://localhost:6379/1

# Celery 应用名称
CELERY__APP_NAME=rag_tasks

# 任务序列化器
CELERY__TASK_SERIALIZER=json

# 结果序列化器
CELERY__RESULT_SERIALIZER=json

# 接受的内容类型（逗号分隔）
CELERY__ACCEPT_CONTENT=json

# 时区
CELERY__TIMEZONE=Asia/Shanghai

# 是否启用 UTC
CELERY__ENABLE_UTC=true

# 结果过期时间（秒）
CELERY__RESULT_EXPIRES=3600

# 工作进程并发数
CELERY__WORKER_CONCURRENCY=4

# 每个子进程最大任务数
CELERY__WORKER_MAX_TASKS_PER_CHILD=1000

# 任务时间限制（秒）
CELERY__TASK_TIME_LIMIT=3600

# 任务软时间限制（秒）
CELERY__TASK_SOFT_TIME_LIMIT=3000

# 工作进程预取倍数
CELERY__WORKER_PREFETCH_MULTIPLIER=4

# 工作进程发送任务事件
CELERY__WORKER_SEND_TASK_EVENTS=true

# 任务发送已发送事件
CELERY__TASK_SEND_SENT_EVENT=true

# 工作进程心跳间隔（秒）
CELERY__WORKER_HEARTBEAT=30

# ===================================
# AI Services Configuration
# ===================================

# Embedding Service Provider (openai/huggingface/local)
AI__EMBEDDING__PROVIDER=openai

# Chat Service Provider (openai/anthropic/huggingface/local)
AI__CHAT__PROVIDER=openai

# Global AI Settings
AI__ENABLE_CACHING=true
AI__CACHE_TTL=3600
AI__ENABLE_FALLBACK=true
AI__RATE_LIMIT_REQUESTS_PER_MINUTE=60
AI__RATE_LIMIT_TOKENS_PER_MINUTE=100000

# Embedding Settings
AI__EMBEDDING__CHUNK_SIZE=1000
AI__EMBEDDING__OVERLAP=200
AI__EMBEDDING__BATCH_SIZE=100

# Chat Settings
AI__CHAT__MAX_CONTEXT_LENGTH=4000
AI__CHAT__SYSTEM_PROMPT=You are a helpful AI assistant.

# ===================================
# OpenAI Configuration
# ===================================

# OpenAI API Key (Required for OpenAI provider)
AI__EMBEDDING__OPENAI__API_KEY=
AI__CHAT__OPENAI__API_KEY=

# OpenAI API Base URL (Optional, for custom endpoints)
AI__EMBEDDING__OPENAI__API_BASE=
AI__CHAT__OPENAI__API_BASE=

# OpenAI Organization ID (Optional)
AI__EMBEDDING__OPENAI__ORGANIZATION=
AI__CHAT__OPENAI__ORGANIZATION=

# OpenAI Model Configuration
AI__EMBEDDING__OPENAI__EMBEDDING_MODEL=text-embedding-ada-002
AI__CHAT__OPENAI__CHAT_MODEL=gpt-3.5-turbo

# OpenAI Request Configuration
AI__EMBEDDING__OPENAI__MAX_TOKENS=1024
AI__EMBEDDING__OPENAI__TEMPERATURE=0.7
AI__EMBEDDING__OPENAI__TIMEOUT=60
AI__EMBEDDING__OPENAI__MAX_RETRIES=3

AI__CHAT__OPENAI__MAX_TOKENS=1024
AI__CHAT__OPENAI__TEMPERATURE=0.7
AI__CHAT__OPENAI__TIMEOUT=60
AI__CHAT__OPENAI__MAX_RETRIES=3

# ===================================
# Anthropic Configuration
# ===================================

# Anthropic API Key (Required for Anthropic provider)
AI__CHAT__ANTHROPIC__API_KEY=

# Anthropic API Base URL (Optional)
AI__CHAT__ANTHROPIC__API_BASE=

# Anthropic Model Configuration
AI__CHAT__ANTHROPIC__CHAT_MODEL=claude-3-sonnet-20240229

# Anthropic Request Configuration
AI__CHAT__ANTHROPIC__MAX_TOKENS=1024
AI__CHAT__ANTHROPIC__TEMPERATURE=0.7
AI__CHAT__ANTHROPIC__TIMEOUT=60
AI__CHAT__ANTHROPIC__MAX_RETRIES=3

# ===================================
# HuggingFace Configuration
# ===================================

# HuggingFace API Key (Optional, for private models)
AI__EMBEDDING__HUGGINGFACE__API_KEY=
AI__CHAT__HUGGINGFACE__API_KEY=

# HuggingFace API Base URL
AI__EMBEDDING__HUGGINGFACE__API_BASE=https://api-inference.huggingface.co
AI__CHAT__HUGGINGFACE__API_BASE=https://api-inference.huggingface.co

# HuggingFace Model Configuration
AI__EMBEDDING__HUGGINGFACE__EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
AI__CHAT__HUGGINGFACE__CHAT_MODEL=microsoft/DialoGPT-medium

# HuggingFace Request Configuration
AI__EMBEDDING__HUGGINGFACE__TIMEOUT=60
AI__EMBEDDING__HUGGINGFACE__MAX_RETRIES=3
AI__CHAT__HUGGINGFACE__TIMEOUT=60
AI__CHAT__HUGGINGFACE__MAX_RETRIES=3

# ===================================
# Local LLM Configuration (Ollama, etc.)
# ===================================

# Local LLM API Base URL
AI__EMBEDDING__LOCAL__API_BASE=http://localhost:11434
AI__CHAT__LOCAL__API_BASE=http://localhost:11434

# Local LLM Model Configuration
AI__EMBEDDING__LOCAL__EMBEDDING_MODEL=nomic-embed-text
AI__CHAT__LOCAL__CHAT_MODEL=llama2

# Local LLM Request Configuration
AI__EMBEDDING__LOCAL__TIMEOUT=120
AI__EMBEDDING__LOCAL__MAX_RETRIES=3
AI__CHAT__LOCAL__TIMEOUT=120
AI__CHAT__LOCAL__MAX_RETRIES=3
