# Research Agent RAG System Configuration
# Copy this file to .env and fill in your actual values

# ===== API KEYS =====
# At least one API key is required
OPENAI_API_KEY=your_openai_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ===== PROVIDER CONFIGURATION =====
# Default provider for all agents (openai, deepseek, anthropic)
DEFAULT_PROVIDER=openai

# ===== MODEL CONFIGURATION =====
# Default models for each provider
OPENAI_MODEL=gpt-4o-mini
DEEPSEEK_MODEL=deepseek-chat
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ===== INDIVIDUAL AGENT CONFIGURATION =====
# Option 1: Individual environment variables (simpler)
GOOGLE_ENGINEER_PROVIDER=openai
GOOGLE_ENGINEER_MODEL=gpt-4o-mini
MIT_RESEARCHER_PROVIDER=anthropic
MIT_RESEARCHER_MODEL=claude-3-5-sonnet-20241022
INDUSTRY_EXPERT_PROVIDER=deepseek
INDUSTRY_EXPERT_MODEL=deepseek-chat
PAPER_ANALYST_PROVIDER=openai
PAPER_ANALYST_MODEL=gpt-4o

# Option 2: JSON configuration (more flexible, overrides individual vars)
# AGENT_CONFIGS={"google_engineer": {"provider": "openai", "model": "gpt-4o"}, "mit_researcher": {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"}}

# ===== SEARCH CONFIGURATION =====
# Minimum similarity threshold for vector search results
MIN_SIMILARITY_THRESHOLD=0.5

# Enable ArXiv fallback search when local results are insufficient
ENABLE_ARXIV_FALLBACK=true

# Maximum papers to fetch from ArXiv fallback
ARXIV_FALLBACK_MAX_PAPERS=10

# Enable AI-powered query expansion
ENABLE_QUERY_EXPANSION=true

# Maximum number of query expansions to generate
MAX_QUERY_EXPANSIONS=3

# ===== AGENT DISCUSSION CONFIGURATION =====
# Enable multi-agent discussions and insights
ENABLE_AGENT_DISCUSSIONS=true

# Default selected agents (comma-separated)
DEFAULT_SELECTED_AGENTS=Google Engineer,MIT Researcher,Industry Expert,Paper Analyst

# ===== RESEARCH PARAMETERS =====
# Default maximum papers per research session
DEFAULT_MAX_PAPERS=20

# Default setting for including recent papers
DEFAULT_INCLUDE_RECENT=true

# ===== DATABASE CONFIGURATION =====

# PostgreSQL Configuration
# For local: localhost, for Docker: postgres, for cloud: your-db-host.com
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=research_agent
POSTGRES_USER=research_user
POSTGRES_PASSWORD=your_postgres_password_here
POSTGRES_SSL_MODE=disable
POSTGRES_MAX_CONNECTIONS=20
POSTGRES_CONNECTION_TIMEOUT=30

# Redis Configuration  
# For local: localhost, for Docker: redis, for cloud: your-redis-host.com
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password_here
REDIS_DB=0
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_TIMEOUT=5
REDIS_SOCKET_KEEPALIVE=true

# ===== VECTOR DATABASE CONFIGURATION =====
# Choose your vector database provider: chroma, pinecone, weaviate, qdrant
VECTOR_DB_TYPE=chroma
VECTOR_DB_COLLECTION=research-papers

# ChromaDB Configuration (Local/Self-hosted)
VECTOR_DB_PATH=./data/vector_db
CHROMA_HOST=
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=research-papers

# Weaviate Configuration (Docker/Cloud/Self-hosted)
# For Docker: weaviate, for local: localhost, for cloud: your-cluster.weaviate.network
WEAVIATE_HOST=localhost
WEAVIATE_PORT=8080
WEAVIATE_SCHEME=http
WEAVIATE_URL=
WEAVIATE_API_KEY=your_weaviate_api_key_here
WEAVIATE_CLASS_NAME=ResearchPaper

# Pinecone Configuration (Cloud)
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=research-papers
PINECONE_ENVIRONMENT=us-west1-gcp-free
PINECONE_DIMENSION=384

# Qdrant Configuration (Docker/Cloud/Self-hosted)
# For Docker: qdrant, for local: localhost, for cloud: your-cluster.qdrant.io
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=your_qdrant_api_key_here
QDRANT_COLLECTION_NAME=research-papers
QDRANT_VECTOR_SIZE=384

# ===== EMBEDDING MODEL CONFIGURATION =====
# Choose your embedding provider: openai, anthropic, huggingface, deepseek
EMBEDDING_MODEL_TYPE=openai

# OpenAI Embeddings
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Embeddings
ANTHROPIC_EMBEDDING_MODEL=claude-3-haiku-20240307

# HuggingFace Embeddings (Local)
HUGGINGFACE_EMBEDDING_MODEL=all-MiniLM-L6-v2

# DeepSeek Embeddings
DEEPSEEK_EMBEDDING_MODEL=deepseek-embedding

# ===== PROCESSING CONFIGURATION =====
# Maximum papers per query
MAX_PAPERS_PER_QUERY=50

# Text chunking parameters
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ===== LOGGING CONFIGURATION =====
# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Log file path
LOG_FILE=logs/research_agent.log

# ===== SERVER CONFIGURATION =====
# FastAPI server settings (production)
HOST=0.0.0.0
PORT=8000
WORKERS=1

# CORS configuration (comma-separated origins)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Legacy Streamlit server settings (deprecated)
STREAMLIT_HOST=localhost
STREAMLIT_PORT=8501

# ===== DEPLOYMENT CONFIGURATION =====
# Deployment mode: local, docker, production
DEPLOYMENT_MODE=local

# Docker-specific settings (when DEPLOYMENT_MODE=docker)
NODE_ENV=production
API_BASE_URL=http://research-agent-backend:8000

# ===== SECURITY CONFIGURATION =====
# Enable API key authentication for production
ENABLE_API_KEY_AUTH=false

# Enable rate limiting
ENABLE_RATE_LIMITING=false

# Debug mode (verbose logging and error details)
DEBUG_MODE=true

# ===== ADVANCED CONFIGURATION =====
# Cache TTL for research topics (seconds)
RESEARCH_TOPICS_CACHE_TTL=3600

# ===== QUICK SETUP EXAMPLES =====
# 
# FOR LOCAL DEVELOPMENT:
# POSTGRES_HOST=localhost
# REDIS_HOST=localhost  
# VECTOR_DB_TYPE=chroma
# CHROMA_HOST=
#
# FOR DOCKER SETUP:
# POSTGRES_HOST=postgres
# REDIS_HOST=redis
# WEAVIATE_HOST=weaviate
# VECTOR_DB_TYPE=weaviate
# DEPLOYMENT_MODE=docker
#
# FOR PRODUCTION:
# POSTGRES_HOST=your-db-host.com
# POSTGRES_SSL_MODE=require
# REDIS_HOST=your-redis-host.com
# VECTOR_DB_TYPE=pinecone
# PINECONE_API_KEY=your_key
# ENABLE_API_KEY_AUTH=true
# DEBUG_MODE=false