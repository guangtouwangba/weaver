"""Vector store helpers and graph nodes."""

import os
from pathlib import Path

from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS

from rag_core.chains.embeddings import build_embedding_function
from rag_core.core.models import Document as RAGDocument
from rag_core.graphs.state import DocumentIngestState, QueryState
from rag_core.retrievers.factory import RetrieverFactory
from rag_core.retrievers.vector_retriever import VectorRetriever
from shared_config.settings import AppSettings

_VECTOR_INDEX: FAISS | None = None
_RETRIEVER: VectorRetriever | None = None
_VECTOR_STORE_LOADED: bool = False  # Track if we've attempted to load from disk


def load_vector_store() -> FAISS | None:
    """Load vector store from disk if it exists.
    
    Returns:
        Loaded FAISS instance or None if no persisted store found.
    """
    global _VECTOR_INDEX, _VECTOR_STORE_LOADED
    
    if _VECTOR_STORE_LOADED:
        # Already attempted to load
        return _VECTOR_INDEX
    
    _VECTOR_STORE_LOADED = True
    settings = AppSettings()
    store_path = settings.vector_store_path
    
    # Check if the vector store directory exists
    if not os.path.exists(store_path):
        print(f"ğŸ“‚ å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨: {store_path}")
        print(f"   å°†åœ¨é¦–æ¬¡æ·»åŠ æ–‡æ¡£æ—¶åˆ›å»º")
        return None
    
    # Check if the index file exists
    index_file = os.path.join(store_path, "index.faiss")
    if not os.path.exists(index_file):
        print(f"ğŸ“‚ å‘é‡åº“ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file}")
        return None
    
    try:
        print(f"ğŸ’¾ æ­£åœ¨ä»ç£ç›˜åŠ è½½å‘é‡åº“...")
        print(f"   è·¯å¾„: {store_path}")
        
        embedding_function = build_embedding_function(settings)
        _VECTOR_INDEX = FAISS.load_local(
            store_path,
            embedding_function,
            allow_dangerous_deserialization=True  # Required for loading pickle files
        )
        
        total_docs = _VECTOR_INDEX.index.ntotal if _VECTOR_INDEX else 0
        print(f"âœ… å‘é‡åº“åŠ è½½æˆåŠŸ!")
        print(f"   â””â”€ å·²åŠ è½½ {total_docs} ä¸ªå‘é‡")
        
        return _VECTOR_INDEX
        
    except Exception as e:
        print(f"âŒ åŠ è½½å‘é‡åº“å¤±è´¥: {e}")
        print(f"   å°†åˆ›å»ºæ–°çš„å‘é‡åº“")
        _VECTOR_INDEX = None
        return None


def save_vector_store() -> None:
    """Save vector store to disk."""
    global _VECTOR_INDEX
    
    if _VECTOR_INDEX is None:
        print(f"âš ï¸  å‘é‡åº“ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
        return
    
    settings = AppSettings()
    store_path = settings.vector_store_path
    
    try:
        # Create directory if it doesn't exist
        Path(store_path).mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å‘é‡åº“åˆ°ç£ç›˜...")
        print(f"   è·¯å¾„: {store_path}")
        
        _VECTOR_INDEX.save_local(store_path)
        
        total_docs = _VECTOR_INDEX.index.ntotal if _VECTOR_INDEX else 0
        print(f"âœ… å‘é‡åº“ä¿å­˜æˆåŠŸ!")
        print(f"   â””â”€ å·²ä¿å­˜ {total_docs} ä¸ªå‘é‡")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å‘é‡åº“å¤±è´¥: {e}")
        raise


def get_vector_store() -> FAISS | None:
    """Get existing vector store, loading from disk if necessary."""
    global _VECTOR_INDEX, _VECTOR_STORE_LOADED
    
    # Try to load from disk if not yet loaded
    if not _VECTOR_STORE_LOADED:
        load_vector_store()
    
    return _VECTOR_INDEX


def build_vector_store(settings: AppSettings) -> FAISS | None:
    """Return existing vector store or load from disk if available."""
    return get_vector_store()


def get_retriever() -> VectorRetriever:
    """Get or create retriever instance.

    Returns:
        VectorRetriever instance with current vector store.
    """
    global _RETRIEVER, _VECTOR_INDEX

    if _RETRIEVER is None:
        # Create retriever using factory
        _RETRIEVER = RetrieverFactory.create_from_settings(vector_store=_VECTOR_INDEX)
    elif _RETRIEVER.get_vector_store() != _VECTOR_INDEX:
        # Update vector store if it changed
        _RETRIEVER.set_vector_store(_VECTOR_INDEX)

    return _RETRIEVER


def retrieve_documents(state: QueryState) -> QueryState:
    """Retrieve relevant documents for the given question using RetrieverInterface.

    This function uses the new RetrieverInterface for better abstraction and
    easier testing/mocking.
    
    Supports filtering by document_ids if provided in state.
    """
    global _VECTOR_INDEX

    print(f"\nğŸ” å¼€å§‹æ£€ç´¢æ–‡æ¡£...")
    print(f"  â”œâ”€ é—®é¢˜: {state.question}")
    print(f"  â”œâ”€ Top-K: {state.retriever_top_k}")
    
    # Try to load vector store from disk if not already loaded
    if _VECTOR_INDEX is None:
        print(f"  â”œâ”€ å‘é‡åº“æœªåŠ è½½ï¼Œå°è¯•ä»ç£ç›˜åŠ è½½...")
        get_vector_store()  # This will attempt to load from disk
    
    print(f"  â””â”€ å‘é‡åº“çŠ¶æ€: {'å·²åˆå§‹åŒ–' if _VECTOR_INDEX else 'æœªåˆå§‹åŒ–'}")
    
    if _VECTOR_INDEX is None:
        # No documents have been ingested yet
        print(f"  âš ï¸  å‘é‡åº“ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
        return state.model_copy(update={"documents": []})
    
    # æ˜¾ç¤ºå‘é‡åº“ç»Ÿè®¡ä¿¡æ¯
    total_vectors = _VECTOR_INDEX.index.ntotal if _VECTOR_INDEX else 0
    print(f"  â”œâ”€ å‘é‡åº“ä¸­æ€»æ–‡æ¡£æ•°: {total_vectors}")
    
    # Perform synchronous retrieval using FAISS directly
    try:
        # Use FAISS similarity search with scores (synchronous)
        results = _VECTOR_INDEX.similarity_search_with_score(
            state.question, 
            k=state.retriever_top_k
        )
        
        # Convert to RAG Document format
        from rag_core.core.models import Document as RAGDocument
        rag_documents = []
        for doc, score in results:
            # FAISS returns distance, convert to similarity score (0-1)
            similarity_score = 1.0 / (1.0 + score)
            rag_documents.append(
                RAGDocument(
                    page_content=doc.page_content,
                    metadata=doc.metadata,
                    score=similarity_score,
                )
            )
        
        print(f"  â”œâ”€ æ£€ç´¢åˆ° {len(rag_documents)} ä¸ªç›¸å…³chunks")
    except Exception as e:
        print(f"  âŒ æ£€ç´¢å¤±è´¥: {e}")
        rag_documents = []
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯
    if rag_documents:
        print(f"  â”œâ”€ æ£€ç´¢ç»“æœé¢„è§ˆ:")
        for i, doc in enumerate(rag_documents[:3], 1):
            doc_id = doc.metadata.get("document_id", "N/A")
            filename = doc.metadata.get("filename", "N/A")
            content_preview = doc.page_content[:50] + "..." if len(doc.page_content) > 50 else doc.page_content
            print(f"      [{i}] Doc ID: {doc_id[:8]}..., File: {filename}")
            print(f"          Content: {content_preview}")

    # Filter by document_ids if provided
    if state.document_ids:
        print(f"  â”œâ”€ åº”ç”¨æ–‡æ¡£è¿‡æ»¤: {len(state.document_ids)} ä¸ªæŒ‡å®šæ–‡æ¡£")
        for doc_id in state.document_ids:
            print(f"      â€¢ {doc_id}")
        
        original_count = len(rag_documents)
        rag_documents = [
            doc for doc in rag_documents
            if doc.metadata.get("document_id") in state.document_ids
        ]
        print(f"  â”œâ”€ è¿‡æ»¤ç»“æœ: {original_count} â†’ {len(rag_documents)} ä¸ªchunks")
        
        if original_count > 0 and len(rag_documents) == 0:
            print(f"  âš ï¸  è­¦å‘Š: è¿‡æ»¤åæ²¡æœ‰ç»“æœï¼å¯èƒ½åŸå› :")
            print(f"      â€¢ æŒ‡å®šçš„document_idsä¸å­˜åœ¨")
            print(f"      â€¢ metadataä¸­çš„document_idå­—æ®µç¼ºå¤±")

    # Convert RAGDocument to dict format for state
    formatted = [doc.model_dump() for doc in rag_documents]
    
    print(f"  â””â”€ æœ€ç»ˆè¿”å› {len(formatted)} ä¸ªæ–‡æ¡£\n")

    return state.model_copy(update={"documents": formatted})


async def persist_embeddings(state: DocumentIngestState) -> DocumentIngestState:
    """Write vectors to the FAISS store and save to disk."""
    global _VECTOR_INDEX
    
    if not state.chunks or not state.embeddings:
        raise ValueError("embed step must run before persistence")
    
    print(f"ğŸ’¾ å¼€å§‹æŒä¹…åŒ–å‘é‡...")
    print(f"  â”œâ”€ Embeddings æ•°é‡: {len(state.embeddings)}")
    print(f"  â”œâ”€ Chunks æ•°é‡: {len(state.chunks)}")
    
    settings = AppSettings()  # type: ignore[arg-type]
    embedding_function = build_embedding_function(settings)
    docs = [Document(page_content=chunk, metadata=state.metadata) for chunk in state.chunks]
    
    if _VECTOR_INDEX is None:
        # Try to load existing index first
        load_vector_store()
    
    if _VECTOR_INDEX is None:
        # Initialize FAISS index with first batch of documents
        print(f"  â”œâ”€ åˆå§‹åŒ–æ–°çš„ FAISS ç´¢å¼•...")
        _VECTOR_INDEX = FAISS.from_documents(docs, embedding=embedding_function)
        print(f"  âœ“ FAISS ç´¢å¼•åˆ›å»ºæˆåŠŸ")
    else:
        # Add to existing index
        print(f"  â”œâ”€ æ·»åŠ åˆ°ç°æœ‰ FAISS ç´¢å¼•...")
        _VECTOR_INDEX.add_documents(docs)
        print(f"  âœ“ å‘é‡æ·»åŠ æˆåŠŸ")
    
    total_docs = _VECTOR_INDEX.index.ntotal if _VECTOR_INDEX else 0
    print(f"  â”œâ”€ ç´¢å¼•ä¸­æ€»æ–‡æ¡£æ•°: {total_docs}")
    
    # Save to disk
    print(f"  â”œâ”€ ä¿å­˜å‘é‡åº“åˆ°ç£ç›˜...")
    save_vector_store()
    
    print(f"âœ… å‘é‡æŒä¹…åŒ–å®Œæˆ!")
    
    return state
