"""Memory nodes for conversational QA with vector similarity search."""

import asyncio
from typing import Optional
from uuid import UUID

from langchain_core.prompts import ChatPromptTemplate
from sqlalchemy.orm import Session

from rag_core.chains.llm import build_llm
from rag_core.chains.embeddings import build_embedding_function
from rag_core.graphs.state import QueryState
from rag_core.services.conversation_service import ConversationService
from rag_core.services.message_service import MessageService
from rag_core.storage.database import SessionLocal
from shared_config.settings import AppSettings


# Prompt for contextualizing questions
CONTEXTUALIZE_Q_SYSTEM_PROMPT = """
ç»™å®šå¯¹è¯å†å²å’Œæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼ˆå¯èƒ½å¼•ç”¨äº†å¯¹è¯å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œ
è¯·å°†è¯¥é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œä½¿å…¶ä¸ä¾èµ–å¯¹è¯å†å²ä¹Ÿèƒ½ç†è§£ã€‚

è§„åˆ™ï¼š
1. å°†ä»£è¯ï¼ˆå®ƒã€è¿™ä¸ªã€é‚£ä¸ªã€åˆšæ‰çš„ï¼‰æ›¿æ¢ä¸ºå®é™…çš„å®ä½“
2. è¡¥å……çœç•¥çš„ä¸»è¯­ã€å®¾è¯­æˆ–èƒŒæ™¯ä¿¡æ¯
3. ä¿æŒé—®é¢˜çš„åŸæ„å’Œè¯­æ°”
4. å¦‚æœé—®é¢˜å·²ç»å®Œæ•´ç‹¬ç«‹ï¼Œç›´æ¥è¿”å›åŸé—®é¢˜ï¼ˆä¸è¦æ·»åŠ é¢å¤–å†…å®¹ï¼‰

è¯·åªè¿”å›é‡å†™åçš„é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚
"""

CONTEXTUALIZE_Q_USER_PROMPT = """
å¯¹è¯å†å²ï¼š
{chat_history}

æœ€æ–°é—®é¢˜ï¼š{question}

ç‹¬ç«‹é—®é¢˜ï¼š
"""


def load_memory_node(state: QueryState) -> dict:
    """
    Load recent chat history from database (short-term memory).
    
    Args:
        state: Graph state containing conversation_id
        
    Returns:
        Updated state with chat_history
    """
    conversation_id = state.conversation_id
    
    if not conversation_id:
        print("ğŸ“š [Memory] æ— å¯¹è¯IDï¼Œè·³è¿‡å†å²åŠ è½½")
        return {"chat_history": []}
    
    # Get database session
    db = SessionLocal()
    try:
        # Load recent messages (last 10) - SHORT-TERM MEMORY
        messages = MessageService.get_recent_messages(db, conversation_id, limit=10)
        
        # Format as chat history
        chat_history = []
        for msg in messages:
            chat_history.append({
                "role": msg.role,
                "content": msg.content
            })
        
        print(f"ğŸ“š [Short-term Memory] åŠ è½½äº† {len(chat_history)} æ¡æœ€è¿‘æ¶ˆæ¯")
        return {"chat_history": chat_history}
        
    finally:
        db.close()


def retrieve_long_term_memory_node(state: QueryState) -> dict:
    """
    Retrieve relevant messages from long-term memory using vector similarity.
    This supplements short-term memory for long conversations.
    
    Args:
        state: Graph state containing conversation_id and question
        
    Returns:
        Updated state with long_term_memory
    """
    conversation_id = state.conversation_id
    question = state.question
    
    if not conversation_id:
        print("ğŸ” [Long-term Memory] æ— å¯¹è¯IDï¼Œè·³è¿‡")
        return {"long_term_memory": []}
    
    # Get database session
    db = SessionLocal()
    try:
        # Generate embedding for current question
        settings = AppSettings()  # type: ignore[arg-type]
        embedding_fn = build_embedding_function(settings)
        
        print("ğŸ” [Long-term Memory] ç”ŸæˆæŸ¥è¯¢embedding...")
        try:
            query_embedding = asyncio.run(embedding_fn.aembed_query(question))
        except Exception as e:
            print(f"âš ï¸ [Long-term Memory] Embeddingç”Ÿæˆå¤±è´¥: {e}")
            return {"long_term_memory": []}
        
        # Find similar messages (top 3, similarity > 0.7)
        print("ğŸ” [Long-term Memory] æœç´¢ç›¸ä¼¼å†å²...")
        similar_messages = MessageService.find_similar_messages(
            db,
            conversation_id=conversation_id,
            query_embedding=query_embedding,
            limit=3,
            similarity_threshold=0.7
        )
        
        # Format as memory entries
        long_term_memory = []
        for msg in similar_messages:
            long_term_memory.append({
                "role": msg.role,
                "content": msg.content
            })
        
        print(f"ğŸ” [Long-term Memory] æ£€ç´¢åˆ° {len(long_term_memory)} æ¡ç›¸å…³å†å²")
        return {"long_term_memory": long_term_memory}
        
    finally:
        db.close()


def contextualize_query_node(state: QueryState) -> dict:
    """
    Rewrite the question based on chat history (short-term + long-term) to make it standalone.
    
    Args:
        state: Graph state containing question, chat_history, and long_term_memory
        
    Returns:
        Updated state with contextualized_question
    """
    question = state.question
    chat_history = state.chat_history or []
    long_term_memory = state.long_term_memory or []
    
    # If no history, use original question
    if not chat_history and not long_term_memory:
        print("ğŸ”„ [Contextualize] æ— å†å²è®°å½•ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜")
        return {"contextualized_question": question}
    
    # Merge memories: long-term first (context), then short-term (recent)
    all_memory = []
    
    if long_term_memory:
        print(f"ğŸ”„ [Contextualize] åŒ…å« {len(long_term_memory)} æ¡é•¿æœŸè®°å¿†ï¼ˆç›¸ä¼¼å†å²ï¼‰")
        all_memory.append("ã€ç›¸å…³å†å²ã€‘")
        for msg in long_term_memory:
            all_memory.append(f"[{msg['role']}]: {msg['content']}")
    
    if chat_history:
        print(f"ğŸ”„ [Contextualize] åŒ…å« {len(chat_history)} æ¡çŸ­æœŸè®°å¿†ï¼ˆæœ€è¿‘å¯¹è¯ï¼‰")
        if long_term_memory:
            all_memory.append("\nã€æœ€è¿‘å¯¹è¯ã€‘")
        for msg in chat_history:
            all_memory.append(f"[{msg['role']}]: {msg['content']}")
    
    # Format memory for prompt
    history_str = "\n".join(all_memory)
    
    # Create prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", CONTEXTUALIZE_Q_SYSTEM_PROMPT),
        ("human", CONTEXTUALIZE_Q_USER_PROMPT),
    ])
    
    # Get LLM
    settings = AppSettings()  # type: ignore[arg-type]
    llm = build_llm(settings)
    
    # Chain: prompt | llm
    chain = prompt | llm
    
    # Invoke
    print(f"ğŸ”„ [Contextualize] æ­£åœ¨åŸºäºå†å²é‡å†™é—®é¢˜...")
    print(f"  åŸå§‹é—®é¢˜: {question}")
    
    response = chain.invoke({
        "chat_history": history_str,
        "question": question
    })
    
    # Extract content
    contextualized = response.content.strip() if hasattr(response, 'content') else str(response).strip()
    
    print(f"  é‡å†™é—®é¢˜: {contextualized}")
    
    return {"contextualized_question": contextualized}


def save_memory_node(state: QueryState) -> dict:
    """
    Save the QA interaction to conversation history with embeddings.
    
    Args:
        state: Graph state containing conversation_id, question, answer, documents
        
    Returns:
        Updated state with conversation_id (creates new conversation if needed)
    """
    conversation_id = state.conversation_id
    question = state.question
    answer = state.answer or ""
    documents = state.documents or []
    topic_id = state.topic_id  # Optional: for creating new conversation
    
    # Get database session
    db = SessionLocal()
    try:
        # Check if conversation exists, or create a new one
        needs_new_conversation = False
        
        if conversation_id:
            # Verify the conversation exists in the database
            existing = ConversationService.get_conversation(db, conversation_id)
            if not existing:
                print(f"âš ï¸ [Memory] Conversation {conversation_id} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„")
                needs_new_conversation = True
        else:
            needs_new_conversation = True
        
        # Create new conversation if needed
        if needs_new_conversation:
            if not topic_id:
                print("âš ï¸ [Memory] æ— å¯¹è¯IDä¸”æ— topic_idï¼Œè·³è¿‡ä¿å­˜")
                return {}
            
            # Verify topic exists before creating conversation
            from rag_core.services.topic_service import TopicService
            topic = TopicService.get_topic(db, topic_id)
            if not topic:
                print(f"âŒ [Memory] Topic {topic_id} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºå¯¹è¯ï¼Œè·³è¿‡ä¿å­˜")
                print(f"   æç¤ºï¼šè¯·ç¡®ä¿å‰ç«¯ä¼ é€’äº†æ­£ç¡®çš„topic_idï¼Œæˆ–å…ˆåˆ›å»ºtopic")
                return {}
            
            from domain_models import ConversationCreate
            
            # Generate title from question (first 50 chars)
            title = question[:50] + "..." if len(question) > 50 else question
            
            conversation_data = ConversationCreate(
                topic_id=UUID(topic_id),
                title=title
            )
            conversation = ConversationService.create_conversation(db, conversation_data)
            conversation_id = str(conversation.id)
            print(f"ğŸ’¾ [Memory] åˆ›å»ºæ–°å¯¹è¯: {conversation_id}")
        
        # Generate embeddings for question and answer
        settings = AppSettings()  # type: ignore[arg-type]
        embedding_fn = build_embedding_function(settings)
        
        print("ğŸ”® [Memory] ç”Ÿæˆæ¶ˆæ¯embedding...")
        
        # Generate embeddings (synchronously - LangGraph nodes are sync)
        # We'll use asyncio.run to call async embed functions
        try:
            question_embedding = asyncio.run(embedding_fn.aembed_query(question))
            answer_embedding = asyncio.run(embedding_fn.aembed_query(answer))
        except Exception as e:
            print(f"âš ï¸ [Memory] Embeddingç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ä¸ä¿å­˜embedding")
            question_embedding = None
            answer_embedding = None
        
        # Save user message with embedding
        MessageService.create_message(
            db,
            conversation_id=conversation_id,
            role="user",
            content=question,
            embedding=question_embedding
        )
        
        # Prepare sources
        sources = []
        for doc in documents:
            sources.append({
                "content": doc.get("page_content", ""),
                "metadata": doc.get("metadata", {})
            })
        
        # Save assistant message with embedding
        MessageService.create_message(
            db,
            conversation_id=conversation_id,
            role="assistant",
            content=answer,
            sources=sources if sources else None,
            embedding=answer_embedding
        )
        
        print(f"ğŸ’¾ [Memory] ä¿å­˜äº†ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯ï¼ˆå«embeddingï¼‰åˆ°å¯¹è¯: {conversation_id}")
        
        return {"conversation_id": conversation_id}
        
    finally:
        db.close()

