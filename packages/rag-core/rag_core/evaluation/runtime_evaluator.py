"""
è¿è¡Œæ—¶ RAGAS è¯„ä¼°æœåŠ¡

æä¾›å¤šç§è¯„ä¼°ç­–ç•¥ï¼š
1. é‡‡æ ·è¯„ä¼° - æŒ‰æ¯”ä¾‹è¯„ä¼°éƒ¨åˆ†æŸ¥è¯¢
2. å¼‚æ­¥è¯„ä¼° - åå°è¯„ä¼°ï¼Œä¸é˜»å¡å“åº”
3. å®šæœŸæ‰¹é‡è¯„ä¼° - å®šæ—¶æ”¶é›†æ•°æ®æ‰¹é‡è¯„ä¼°
"""

import asyncio
import json
import logging
import random
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Callable, Dict, List, Optional

from .dataset import EvaluationDataset, EvaluationSample
from .ragas_evaluator import EvaluationMetrics, RAGASEvaluator

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class EvaluationMode(str, Enum):
    """è¯„ä¼°æ¨¡å¼"""

    DISABLED = "disabled"  # ç¦ç”¨è¯„ä¼°
    SAMPLING = "sampling"  # é‡‡æ ·è¯„ä¼°ï¼ˆæ¨èï¼‰
    ASYNC_ALL = "async_all"  # å¼‚æ­¥è¯„ä¼°æ‰€æœ‰æŸ¥è¯¢
    BATCH = "batch"  # æ‰¹é‡è¯„ä¼°æ¨¡å¼


@dataclass
class RuntimeEvaluationConfig:
    """è¿è¡Œæ—¶è¯„ä¼°é…ç½®"""

    # è¯„ä¼°æ¨¡å¼
    mode: EvaluationMode = EvaluationMode.SAMPLING

    # é‡‡æ ·ç‡ (0.0-1.0)
    sampling_rate: float = 0.1  # é»˜è®¤è¯„ä¼° 10% çš„æŸ¥è¯¢

    # è¯„ä¼°æŒ‡æ ‡
    metrics: List[EvaluationMetrics] = None

    # æ‰¹é‡è¯„ä¼°é…ç½®
    batch_size: int = 10  # æ”¶é›†å¤šå°‘ä¸ªæ ·æœ¬åæ‰¹é‡è¯„ä¼°
    batch_interval: int = 3600  # æ‰¹é‡è¯„ä¼°é—´éš”ï¼ˆç§’ï¼‰

    # å­˜å‚¨é…ç½®
    storage_dir: Path = Path("data/evaluation/runtime")
    max_samples_in_memory: int = 1000  # å†…å­˜ä¸­æœ€å¤šä¿ç•™æ ·æœ¬æ•°

    # ç»“æœå­˜å‚¨
    save_results: bool = True
    results_file: str = "runtime_evaluation_results.jsonl"

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.metrics is None:
            # é»˜è®¤ä½¿ç”¨å¿«é€Ÿè¯„ä¼°æŒ‡æ ‡ï¼ˆä¸éœ€è¦ ground truthï¼‰
            self.metrics = [
                EvaluationMetrics.FAITHFULNESS,
                EvaluationMetrics.ANSWER_RELEVANCY,
            ]

        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        self.storage_dir.mkdir(parents=True, exist_ok=True)


@dataclass
class QueryRecord:
    """æŸ¥è¯¢è®°å½•"""

    query_id: str
    question: str
    answer: str
    contexts: List[str]
    ground_truth: Optional[str] = None
    metadata: Optional[Dict] = None
    timestamp: str = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()


class RuntimeEvaluator:
    """è¿è¡Œæ—¶è¯„ä¼°å™¨"""

    def __init__(
        self,
        config: RuntimeEvaluationConfig,
        evaluator: RAGASEvaluator,
        on_evaluation_complete: Optional[Callable] = None,
    ):
        """
        åˆå§‹åŒ–è¿è¡Œæ—¶è¯„ä¼°å™¨ã€‚

        Args:
            config: è¯„ä¼°é…ç½®
            evaluator: RAGAS è¯„ä¼°å™¨å®ä¾‹
            on_evaluation_complete: è¯„ä¼°å®Œæˆåçš„å›è°ƒå‡½æ•°
        """
        self.config = config
        self.evaluator = evaluator
        self.on_evaluation_complete = on_evaluation_complete

        # å¾…è¯„ä¼°çš„æŸ¥è¯¢é˜Ÿåˆ—
        self.pending_queries: List[QueryRecord] = []

        # è¯„ä¼°ç»“æœç¼“å­˜
        self.recent_results: List[Dict] = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "evaluated_queries": 0,
            "skipped_queries": 0,
            "evaluation_errors": 0,
        }

        # åå°ä»»åŠ¡
        self._background_task: Optional[asyncio.Task] = None
        self._is_running = False

        logger.info("=" * 60)
        logger.info("âœ… RuntimeEvaluator åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ¨¡å¼: {config.mode.value}")
        logger.info(f"   é‡‡æ ·ç‡: {config.sampling_rate * 100:.1f}%")
        logger.info(f"   æŒ‡æ ‡: {[m.value for m in config.metrics]}")
        logger.info(f"   å­˜å‚¨ç›®å½•: {config.storage_dir}")
        logger.info(f"   æ‰¹é‡å¤§å°: {config.batch_size}")
        logger.info("=" * 60)

    async def start(self):
        """å¯åŠ¨åå°è¯„ä¼°ä»»åŠ¡"""
        if self._is_running:
            return

        self._is_running = True

        if self.config.mode == EvaluationMode.BATCH:
            # å¯åŠ¨å®šæœŸæ‰¹é‡è¯„ä¼°ä»»åŠ¡
            self._background_task = asyncio.create_task(self._batch_evaluation_loop())
            logger.info(f"ğŸš€ å·²å¯åŠ¨æ‰¹é‡è¯„ä¼°ä»»åŠ¡ï¼ˆé—´éš”: {self.config.batch_interval}sï¼‰")

    async def stop(self):
        """åœæ­¢åå°è¯„ä¼°ä»»åŠ¡"""
        self._is_running = False

        if self._background_task:
            self._background_task.cancel()
            try:
                await self._background_task
            except asyncio.CancelledError:
                pass

        # è¯„ä¼°å‰©ä½™çš„æŸ¥è¯¢
        if self.pending_queries:
            logger.info(f"ğŸ”„ è¯„ä¼°å‰©ä½™çš„ {len(self.pending_queries)} ä¸ªæŸ¥è¯¢...")
            await self._evaluate_batch(self.pending_queries)
            self.pending_queries.clear()

        logger.info("ğŸ›‘ RuntimeEvaluator å·²åœæ­¢")

    def should_evaluate(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¯„ä¼°å½“å‰æŸ¥è¯¢ï¼ˆç”¨äºé‡‡æ ·ï¼‰"""
        if self.config.mode == EvaluationMode.DISABLED:
            return False
        elif self.config.mode == EvaluationMode.ASYNC_ALL:
            return True
        elif self.config.mode == EvaluationMode.SAMPLING:
            return random.random() < self.config.sampling_rate
        elif self.config.mode == EvaluationMode.BATCH:
            return True  # æ‰¹é‡æ¨¡å¼å…ˆæ”¶é›†ï¼Œåè¯„ä¼°
        return False

    async def record_query(
        self,
        query_id: str,
        question: str,
        answer: str,
        contexts: List[str],
        ground_truth: Optional[str] = None,
        metadata: Optional[Dict] = None,
    ):
        """
        è®°å½•ä¸€æ¬¡æŸ¥è¯¢ï¼Œå¹¶æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¯„ä¼°ã€‚

        Args:
            query_id: æŸ¥è¯¢å”¯ä¸€ ID
            question: ç”¨æˆ·é—®é¢˜
            answer: RAG ç³»ç»Ÿçš„ç­”æ¡ˆ
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            ground_truth: å‚è€ƒç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        self.stats["total_queries"] += 1

        logger.debug(f"ğŸ“ è®°å½•æŸ¥è¯¢: query_id={query_id}, question_len={len(question)}, contexts={len(contexts)}")

        # åˆ›å»ºæŸ¥è¯¢è®°å½•
        record = QueryRecord(
            query_id=query_id,
            question=question,
            answer=answer,
            contexts=contexts,
            ground_truth=ground_truth,
            metadata=metadata or {},
        )

        # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¯„ä¼°
        should_eval = self.should_evaluate()
        if not should_eval:
            self.stats["skipped_queries"] += 1
            logger.debug(f"â­ï¸  è·³è¿‡è¯„ä¼°: query_id={query_id} (é‡‡æ ·æœªé€‰ä¸­)")
            return

        logger.info(f"âœ… é€‰ä¸­è¯„ä¼°: query_id={query_id}, mode={self.config.mode.value}")

        # æ ¹æ®æ¨¡å¼å¤„ç†
        if self.config.mode == EvaluationMode.BATCH:
            # æ‰¹é‡æ¨¡å¼ï¼šåŠ å…¥é˜Ÿåˆ—
            self.pending_queries.append(record)
            logger.debug(f"ğŸ“¦ åŠ å…¥æ‰¹é‡é˜Ÿåˆ—: {len(self.pending_queries)}/{self.config.batch_size}")

            # å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œç«‹å³è¯„ä¼°
            if len(self.pending_queries) >= self.config.batch_size:
                logger.info(f"ğŸ¯ æ‰¹é‡é˜Ÿåˆ—å·²æ»¡ï¼Œå¼€å§‹è¯„ä¼°...")
                await self._evaluate_batch(self.pending_queries)
                self.pending_queries.clear()

        else:
            # é‡‡æ ·/å¼‚æ­¥æ¨¡å¼ï¼šç«‹å³å¼‚æ­¥è¯„ä¼°
            logger.debug(f"ğŸš€ å¯åŠ¨å¼‚æ­¥è¯„ä¼°: query_id={query_id}")
            asyncio.create_task(self._evaluate_single(record))

    async def _evaluate_single(self, record: QueryRecord):
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        logger.info(f"ğŸ¯ å¼€å§‹å•ä¸ªè¯„ä¼°: query_id={record.query_id}")

        try:
            # åˆ›å»ºä¸´æ—¶æ•°æ®é›†
            dataset = EvaluationDataset(name="runtime_single")
            dataset.add_sample(
                question=record.question,
                answer=record.answer,
                contexts=record.contexts,
                ground_truth=record.ground_truth,
                metadata=record.metadata,
            )

            logger.debug(f"   æŒ‡æ ‡: {[m.value for m in self.config.metrics]}")

            # è¯„ä¼° - åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œä»¥é¿å… uvloop åµŒå¥—äº‹ä»¶å¾ªç¯é—®é¢˜
            logger.debug(f"   å¼€å§‹ RAGAS è¯„ä¼°...")
            loop = asyncio.get_event_loop()

            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„ evaluate è°ƒç”¨
            results = await loop.run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                lambda: self.evaluator.evaluate_sync(dataset, metrics=self.config.metrics),
            )

            logger.info(f"   âœ… è¯„ä¼°å®Œæˆ: {results.scores}")

            # è®°å½•ç»“æœ
            result = {
                "query_id": record.query_id,
                "timestamp": record.timestamp,
                "scores": results.scores,
                "question": record.question[:100] + "..." if len(record.question) > 100 else record.question,
            }

            self._store_result(result)
            self.stats["evaluated_queries"] += 1

            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: query_id={record.query_id}")

            # è°ƒç”¨å›è°ƒ
            if self.on_evaluation_complete:
                logger.debug(f"ğŸ”” è§¦å‘å›è°ƒ: query_id={record.query_id}")
                await self.on_evaluation_complete(result)

        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°å¤±è´¥ (query_id={record.query_id}): {e}", exc_info=True)
            self.stats["evaluation_errors"] += 1

    async def _evaluate_batch(self, records: List[QueryRecord]):
        """æ‰¹é‡è¯„ä¼°å¤šä¸ªæŸ¥è¯¢"""
        if not records:
            return

        logger.info("=" * 60)
        logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡è¯„ä¼° {len(records)} ä¸ªæŸ¥è¯¢...")
        logger.info(f"   æŒ‡æ ‡: {[m.value for m in self.config.metrics]}")

        try:
            # åˆ›å»ºæ•°æ®é›†
            logger.debug(f"   åˆ›å»ºè¯„ä¼°æ•°æ®é›†...")
            dataset = EvaluationDataset(name="runtime_batch")
            for i, record in enumerate(records, 1):
                dataset.add_sample(
                    question=record.question,
                    answer=record.answer,
                    contexts=record.contexts,
                    ground_truth=record.ground_truth,
                    metadata={**record.metadata, "query_id": record.query_id},
                )
                logger.debug(f"      [{i}/{len(records)}] query_id={record.query_id}")

            # æ‰¹é‡è¯„ä¼°
            logger.info(f"   å¼€å§‹ RAGAS æ‰¹é‡è¯„ä¼°...")
            results = await self.evaluator.evaluate(dataset, metrics=self.config.metrics)

            logger.info(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
            logger.info(f"   å¹³å‡åˆ†æ•°:")
            for metric, score in results.scores.items():
                logger.info(f"      {metric}: {score:.3f}")

            # è®°å½•ç»“æœ
            batch_result = {
                "batch_timestamp": datetime.now().isoformat(),
                "batch_size": len(records),
                "avg_scores": results.scores,
                "queries": [r.query_id for r in records],
            }

            self._store_result(batch_result)
            self.stats["evaluated_queries"] += len(records)

            logger.info(f"ğŸ’¾ æ‰¹é‡ç»“æœå·²ä¿å­˜")
            logger.info("=" * 60)

            # è°ƒç”¨å›è°ƒ
            if self.on_evaluation_complete:
                logger.debug(f"ğŸ”” è§¦å‘æ‰¹é‡å›è°ƒ")
                await self.on_evaluation_complete(batch_result)

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
            self.stats["evaluation_errors"] += len(records)

    async def _batch_evaluation_loop(self):
        """æ‰¹é‡è¯„ä¼°å¾ªç¯ï¼ˆåå°ä»»åŠ¡ï¼‰"""
        logger.info(f"ğŸ” æ‰¹é‡è¯„ä¼°å¾ªç¯å·²å¯åŠ¨ï¼Œé—´éš”: {self.config.batch_interval}s")

        while self._is_running:
            try:
                # ç­‰å¾…æŒ‡å®šé—´éš”
                logger.debug(f"â° ç­‰å¾…ä¸‹æ¬¡æ‰¹é‡è¯„ä¼°...")
                await asyncio.sleep(self.config.batch_interval)

                # å¦‚æœæœ‰å¾…è¯„ä¼°çš„æŸ¥è¯¢ï¼Œæ‰§è¡Œè¯„ä¼°
                if self.pending_queries:
                    logger.info(f"â° å®šæ—¶æ‰¹é‡è¯„ä¼°è§¦å‘ (é˜Ÿåˆ—: {len(self.pending_queries)} ä¸ªæŸ¥è¯¢)")
                    await self._evaluate_batch(self.pending_queries)
                    self.pending_queries.clear()
                else:
                    logger.debug(f"â° å®šæ—¶è§¦å‘ï¼Œä½†é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡")

            except asyncio.CancelledError:
                logger.info("ğŸ›‘ æ‰¹é‡è¯„ä¼°å¾ªç¯è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡è¯„ä¼°å¾ªç¯é”™è¯¯: {e}", exc_info=True)

    def _store_result(self, result: Dict):
        """å­˜å‚¨è¯„ä¼°ç»“æœ"""
        # æ·»åŠ åˆ°ç¼“å­˜
        self.recent_results.append(result)
        logger.debug(f"ğŸ’¾ ç»“æœæ·»åŠ åˆ°ç¼“å­˜ (å½“å‰ç¼“å­˜: {len(self.recent_results)})")

        # é™åˆ¶å†…å­˜ä¸­çš„ç»“æœæ•°é‡
        if len(self.recent_results) > 100:
            self.recent_results = self.recent_results[-100:]
            logger.debug(f"   ç¼“å­˜å·²è£å‰ªåˆ° 100 æ¡")

        # ä¿å­˜åˆ°æ–‡ä»¶
        if self.config.save_results:
            try:
                results_path = self.config.storage_dir / self.config.results_file
                with open(results_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(result, ensure_ascii=False) + "\n")
                logger.debug(f"   å·²ä¿å­˜åˆ°æ–‡ä»¶: {results_path}")
            except Exception as e:
                logger.error(f"   âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        # è®¡ç®—è¯„ä¼°ç‡
        if stats["total_queries"] > 0:
            stats["evaluation_rate"] = stats["evaluated_queries"] / stats["total_queries"]
        else:
            stats["evaluation_rate"] = 0.0

        # æ·»åŠ æœ€è¿‘çš„è¯„ä¼°ç»“æœæ‘˜è¦
        if self.recent_results:
            recent_scores = [r.get("scores", {}) for r in self.recent_results if "scores" in r]
            if recent_scores:
                # è®¡ç®—å¹³å‡åˆ†
                avg_scores = {}
                for metric in self.config.metrics:
                    metric_name = metric.value
                    scores = [s.get(metric_name, 0) for s in recent_scores if metric_name in s]
                    if scores:
                        avg_scores[metric_name] = sum(scores) / len(scores)

                stats["recent_avg_scores"] = avg_scores

        return stats

    def get_recent_results(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„è¯„ä¼°ç»“æœ"""
        return self.recent_results[-limit:]


# ========================================
# ä¾¿æ·å‡½æ•°
# ========================================


def create_runtime_evaluator(
    llm, embeddings, mode: str = "sampling", sampling_rate: float = 0.1, metrics: Optional[List[str]] = None
) -> RuntimeEvaluator:
    """
    åˆ›å»ºè¿è¡Œæ—¶è¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°ã€‚

    Args:
        llm: LLM å®ä¾‹
        embeddings: Embeddings å®ä¾‹
        mode: è¯„ä¼°æ¨¡å¼ (disabled/sampling/async_all/batch)
        sampling_rate: é‡‡æ ·ç‡ (0.0-1.0)
        metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨

    Returns:
        RuntimeEvaluator å®ä¾‹
    """
    # è½¬æ¢æŒ‡æ ‡
    metric_enums = []
    if metrics:
        for m in metrics:
            try:
                metric_enums.append(EvaluationMetrics(m))
            except ValueError:
                logger.warning(f"âš ï¸  æœªçŸ¥æŒ‡æ ‡: {m}ï¼Œå·²è·³è¿‡")

    # åˆ›å»ºé…ç½®
    config = RuntimeEvaluationConfig(
        mode=EvaluationMode(mode), sampling_rate=sampling_rate, metrics=metric_enums if metric_enums else None
    )

    # åˆ›å»ºè¯„ä¼°å™¨
    ragas_evaluator = RAGASEvaluator(llm=llm, embeddings=embeddings)

    return RuntimeEvaluator(config=config, evaluator=ragas_evaluator)
