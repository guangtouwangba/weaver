#!/usr/bin/env python3
"""
ä» PDF æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆ

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. è¯»å– PDF æ–‡æ¡£
2. æå–å…³é”®å†…å®¹ç‰‡æ®µ
3. ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡çš„é—®é¢˜å’Œç­”æ¡ˆ
4. ä¿å­˜ä¸ºè¯„ä¼°æ•°æ®é›†

Usage:
    # ä»å•ä¸ª PDF ç”Ÿæˆé—®é¢˜
    python examples/generate_test_questions.py data/uploads/your_paper.pdf
    
    # ä»å¤šä¸ª PDF ç”Ÿæˆé—®é¢˜
    python examples/generate_test_questions.py data/uploads/*.pdf
    
    # æŒ‡å®šç”Ÿæˆæ•°é‡
    python examples/generate_test_questions.py data/uploads/paper.pdf --num-questions 20
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import sys
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Optional
import argparse

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag_core.chains.llm import build_llm
from rag_core.evaluation import EvaluationDataset
from shared_config.settings import AppSettings
from langchain_community.document_loaders import PyPDFLoader


# ========================================
# æç¤ºè¯æ¨¡æ¿
# ========================================

QUESTION_GENERATION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•é—®é¢˜ç”Ÿæˆä¸“å®¶ã€‚åŸºäºç»™å®šçš„æ–‡æ¡£å†…å®¹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆã€‚

è¦æ±‚ï¼š
1. é—®é¢˜åº”è¯¥è¦†ç›–æ–‡æ¡£çš„å…³é”®æ¦‚å¿µå’Œé‡è¦ä¿¡æ¯
2. é—®é¢˜éš¾åº¦åˆ†ä¸º easyï¼ˆåŸºç¡€æ¦‚å¿µï¼‰ã€mediumï¼ˆç†è§£åº”ç”¨ï¼‰ã€hardï¼ˆæ·±å…¥åˆ†æï¼‰
3. æ¯ä¸ªé—®é¢˜éƒ½è¦æœ‰æ˜ç¡®çš„å‚è€ƒç­”æ¡ˆï¼ˆground truthï¼‰
4. é—®é¢˜åº”è¯¥æ˜¯ RAG ç³»ç»Ÿèƒ½å¤Ÿå›ç­”çš„ï¼ˆç­”æ¡ˆåœ¨æ–‡æ¡£ä¸­ï¼‰
5. é¿å…è¿‡äºç®€å•çš„æ˜¯éé¢˜ï¼Œå¤šé—®"å¦‚ä½•"ã€"ä¸ºä»€ä¹ˆ"ã€"æ˜¯ä»€ä¹ˆ"

æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·ç”Ÿæˆ {num_questions} ä¸ªæµ‹è¯•é—®é¢˜ï¼Œè¿”å› JSON æ ¼å¼ï¼š

[
  {{
    "question": "é—®é¢˜å†…å®¹",
    "ground_truth": "è¯¦ç»†çš„å‚è€ƒç­”æ¡ˆ",
    "difficulty": "easy|medium|hard",
    "topic": "é—®é¢˜ä¸»é¢˜",
    "question_type": "factual|conceptual|analytical"
  }},
  ...
]

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""


# ========================================
# æ–‡æ¡£å¤„ç†
# ========================================

def load_pdf_documents(pdf_paths: List[Path]) -> List[Dict]:
    """
    åŠ è½½ PDF æ–‡æ¡£å¹¶æå–å†…å®¹ã€‚
    
    Args:
        pdf_paths: PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    
    Returns:
        æ–‡æ¡£å†…å®¹åˆ—è¡¨
    """
    print(f"ğŸ“„ åŠ è½½ PDF æ–‡æ¡£...")
    print(f"   æ–‡ä»¶æ•°é‡: {len(pdf_paths)}")
    print()
    
    all_docs = []
    for pdf_path in pdf_paths:
        print(f"   å¤„ç†: {pdf_path.name}")
        try:
            # ä½¿ç”¨ PyPDFLoader åŠ è½½ PDF
            loader = PyPDFLoader(str(pdf_path))
            docs = loader.load()
            
            if docs:
                all_docs.append({
                    "source": pdf_path.name,
                    "content": "\n\n".join([doc.page_content for doc in docs]),
                    "num_pages": len(docs)
                })
                print(f"      âœ“ æå–äº† {len(docs)} é¡µå†…å®¹")
            else:
                print(f"      âœ— æ— æ³•æå–å†…å®¹")
                
        except Exception as e:
            print(f"      âœ— é”™è¯¯: {e}")
            continue
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(all_docs)} ä¸ªæ–‡æ¡£\n")
    return all_docs


def extract_key_chunks(content: str, chunk_size: int = 2000, num_chunks: int = 5) -> List[str]:
    """
    ä»æ–‡æ¡£ä¸­æå–å…³é”®ç‰‡æ®µç”¨äºç”Ÿæˆé—®é¢˜ã€‚
    
    Args:
        content: æ–‡æ¡£å†…å®¹
        chunk_size: æ¯ä¸ªç‰‡æ®µçš„å¤§å°
        num_chunks: æå–ç‰‡æ®µæ•°é‡
    
    Returns:
        æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
    """
    # ç®€å•åˆ†å—ç­–ç•¥ï¼šæŒ‰æ®µè½åˆ†å‰²
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
    
    # åˆå¹¶æˆå¤§å—
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) <= chunk_size:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = para + "\n\n"
    
    if current_chunk:
        chunks.append(current_chunk)
    
    # å‡åŒ€é‡‡æ ·
    if len(chunks) > num_chunks:
        step = len(chunks) // num_chunks
        chunks = [chunks[i * step] for i in range(num_chunks)]
    
    return chunks


# ========================================
# é—®é¢˜ç”Ÿæˆ
# ========================================

async def generate_questions_for_chunk(
    chunk: str,
    llm,
    num_questions: int = 3,
    source: str = "unknown"
) -> List[Dict]:
    """
    ä¸ºå•ä¸ªæ–‡æ¡£ç‰‡æ®µç”Ÿæˆé—®é¢˜ã€‚
    
    Args:
        chunk: æ–‡æ¡£ç‰‡æ®µ
        llm: LLM å®ä¾‹
        num_questions: ç”Ÿæˆé—®é¢˜æ•°é‡
        source: æ¥æºæ–‡æ¡£åç§°
    
    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    try:
        # æ„å»º prompt
        prompt = QUESTION_GENERATION_PROMPT.format(
            context=chunk[:3000],  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            num_questions=num_questions
        )
        
        # è°ƒç”¨ LLM
        response = await llm.ainvoke(prompt)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # è§£æ JSON
        # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨å…¶ä»–æ–‡æœ¬ä¸­ï¼‰
        import re
        json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
        if json_match:
            questions = json.loads(json_match.group())
        else:
            questions = json.loads(response_text)
        
        # æ·»åŠ æºä¿¡æ¯
        for q in questions:
            q['source'] = source
        
        return questions
        
    except json.JSONDecodeError as e:
        print(f"      âœ— JSON è§£æå¤±è´¥: {e}")
        print(f"      å“åº”: {response_text[:200]}...")
        return []
    except Exception as e:
        print(f"      âœ— ç”Ÿæˆå¤±è´¥: {e}")
        return []


async def generate_questions_from_documents(
    documents: List[Dict],
    llm,
    questions_per_chunk: int = 3,
    chunks_per_doc: int = 5
) -> List[Dict]:
    """
    ä»æ‰€æœ‰æ–‡æ¡£ç”Ÿæˆé—®é¢˜ã€‚
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        llm: LLM å®ä¾‹
        questions_per_chunk: æ¯ä¸ªç‰‡æ®µç”Ÿæˆçš„é—®é¢˜æ•°
        chunks_per_doc: æ¯ä¸ªæ–‡æ¡£æå–çš„ç‰‡æ®µæ•°
    
    Returns:
        æ‰€æœ‰ç”Ÿæˆçš„é—®é¢˜
    """
    print("ğŸ¤– ä½¿ç”¨ LLM ç”Ÿæˆæµ‹è¯•é—®é¢˜...")
    print(f"   æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µ: {questions_per_chunk} ä¸ªé—®é¢˜")
    print(f"   æ¯ä¸ªæ–‡æ¡£: {chunks_per_doc} ä¸ªç‰‡æ®µ")
    print()
    
    all_questions = []
    
    for doc in documents:
        source = doc['source']
        content = doc['content']
        
        print(f"   å¤„ç†æ–‡æ¡£: {source}")
        
        # æå–å…³é”®ç‰‡æ®µ
        chunks = extract_key_chunks(content, num_chunks=chunks_per_doc)
        print(f"      æå–äº† {len(chunks)} ä¸ªç‰‡æ®µ")
        
        # ä¸ºæ¯ä¸ªç‰‡æ®µç”Ÿæˆé—®é¢˜
        for i, chunk in enumerate(chunks, 1):
            print(f"      ç‰‡æ®µ {i}/{len(chunks)}...", end=" ")
            
            questions = await generate_questions_for_chunk(
                chunk,
                llm,
                num_questions=questions_per_chunk,
                source=source
            )
            
            if questions:
                all_questions.extend(questions)
                print(f"âœ“ ç”Ÿæˆ {len(questions)} ä¸ªé—®é¢˜")
            else:
                print("âœ— å¤±è´¥")
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        
        print()
    
    print(f"âœ… æ€»å…±ç”Ÿæˆ {len(all_questions)} ä¸ªé—®é¢˜\n")
    return all_questions


# ========================================
# æ•°æ®é›†åˆ›å»º
# ========================================

def create_evaluation_dataset(
    questions: List[Dict],
    output_path: Path
) -> EvaluationDataset:
    """
    åˆ›å»ºè¯„ä¼°æ•°æ®é›†å¹¶ä¿å­˜ã€‚
    
    Args:
        questions: é—®é¢˜åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Returns:
        è¯„ä¼°æ•°æ®é›†
    """
    print("ğŸ“Š åˆ›å»ºè¯„ä¼°æ•°æ®é›†...")
    
    dataset = EvaluationDataset(
        name=output_path.stem,
        description=f"Auto-generated test set with {len(questions)} questions"
    )
    
    # æ·»åŠ é—®é¢˜ï¼ˆæ³¨æ„ï¼šè¿™é‡Œåªæœ‰ question å’Œ ground_truthï¼Œæ²¡æœ‰ answer å’Œ contextsï¼‰
    # è¿™äº›å°†åœ¨è¿è¡Œ RAG ç³»ç»Ÿæ—¶å¡«å……
    for q in questions:
        dataset.add_sample(
            question=q['question'],
            answer="",  # å¾… RAG ç³»ç»Ÿå¡«å……
            contexts=[],  # å¾… RAG ç³»ç»Ÿå¡«å……
            ground_truth=q['ground_truth'],
            metadata={
                'source': q.get('source', 'unknown'),
                'difficulty': q.get('difficulty', 'medium'),
                'topic': q.get('topic', 'general'),
                'question_type': q.get('question_type', 'factual')
            }
        )
    
    # ä¿å­˜
    output_path.parent.mkdir(parents=True, exist_ok=True)
    dataset.save(output_path)
    
    print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {output_path}")
    print(f"   é—®é¢˜æ€»æ•°: {len(dataset)}")
    print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    difficulties = {}
    topics = {}
    for sample in dataset.samples:
        diff = sample.metadata.get('difficulty', 'unknown')
        topic = sample.metadata.get('topic', 'unknown')
        difficulties[diff] = difficulties.get(diff, 0) + 1
        topics[topic] = topics.get(topic, 0) + 1
    
    print("   éš¾åº¦åˆ†å¸ƒ:")
    for diff, count in sorted(difficulties.items()):
        print(f"      {diff:10s}: {count:3d} ({count/len(dataset)*100:.1f}%)")
    
    print(f"\n   ä¸»é¢˜åˆ†å¸ƒ (top 5):")
    for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"      {topic:20s}: {count:3d}")
    
    print()
    return dataset


def save_raw_questions(questions: List[Dict], output_path: Path):
    """ä¿å­˜åŸå§‹é—®é¢˜ JSONï¼ˆå¯é€‰ï¼Œç”¨äºæŸ¥çœ‹ï¼‰ã€‚"""
    raw_path = output_path.parent / f"{output_path.stem}_raw.json"
    with open(raw_path, 'w', encoding='utf-8') as f:
        json.dump(questions, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ åŸå§‹é—®é¢˜å·²ä¿å­˜: {raw_path}\n")


# ========================================
# ä¸»å‡½æ•°
# ========================================

async def main():
    parser = argparse.ArgumentParser(
        description="ä» PDF æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆ"
    )
    parser.add_argument(
        'pdf_files',
        nargs='+',
        type=Path,
        help='PDF æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥å¤šä¸ªï¼‰'
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        default=Path('data/evaluation/generated_test_set.json'),
        help='è¾“å‡ºæ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤: data/evaluation/generated_test_set.jsonï¼‰'
    )
    parser.add_argument(
        '--questions-per-chunk', '-q',
        type=int,
        default=3,
        help='æ¯ä¸ªæ–‡æ¡£ç‰‡æ®µç”Ÿæˆçš„é—®é¢˜æ•°ï¼ˆé»˜è®¤: 3ï¼‰'
    )
    parser.add_argument(
        '--chunks-per-doc', '-c',
        type=int,
        default=5,
        help='æ¯ä¸ªæ–‡æ¡£æå–çš„ç‰‡æ®µæ•°ï¼ˆé»˜è®¤: 5ï¼‰'
    )
    parser.add_argument(
        '--save-raw',
        action='store_true',
        help='ä¿å­˜åŸå§‹é—®é¢˜ JSON'
    )
    
    args = parser.parse_args()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆ")
    print("=" * 80)
    print()
    
    # éªŒè¯æ–‡ä»¶
    pdf_files = []
    for path in args.pdf_files:
        if path.is_file() and path.suffix.lower() == '.pdf':
            pdf_files.append(path)
        else:
            print(f"âš ï¸  è·³è¿‡æ— æ•ˆæ–‡ä»¶: {path}")
    
    if not pdf_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ PDF æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶:")
    for f in pdf_files:
        print(f"   - {f.name}")
    print()
    
    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
    try:
        settings = AppSettings()
        llm = build_llm(settings)
        print(f"   LLM: {settings.llm.provider} / {settings.llm.model}")
        print()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Step 1: åŠ è½½ PDF æ–‡æ¡£
    print("=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½ PDF æ–‡æ¡£")
    print("=" * 80)
    print()
    
    documents = load_pdf_documents(pdf_files)
    
    if not documents:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
        return
    
    # Step 2: ç”Ÿæˆé—®é¢˜
    print("=" * 80)
    print("æ­¥éª¤ 2: ç”Ÿæˆæµ‹è¯•é—®é¢˜")
    print("=" * 80)
    print()
    
    questions = await generate_questions_from_documents(
        documents,
        llm,
        questions_per_chunk=args.questions_per_chunk,
        chunks_per_doc=args.chunks_per_doc
    )
    
    if not questions:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é—®é¢˜")
        return
    
    # Step 3: åˆ›å»ºæ•°æ®é›†
    print("=" * 80)
    print("æ­¥éª¤ 3: åˆ›å»ºè¯„ä¼°æ•°æ®é›†")
    print("=" * 80)
    print()
    
    dataset = create_evaluation_dataset(questions, args.output)
    
    # å¯é€‰ï¼šä¿å­˜åŸå§‹ JSON
    if args.save_raw:
        save_raw_questions(questions, args.output)
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print("=" * 80)
    print("ğŸ“ ç¤ºä¾‹é—®é¢˜ï¼ˆå‰ 3 ä¸ªï¼‰")
    print("=" * 80)
    print()
    
    for i, sample in enumerate(dataset.samples[:3], 1):
        print(f"{i}. é—®é¢˜: {sample.question}")
        print(f"   å‚è€ƒç­”æ¡ˆ: {sample.ground_truth[:100]}...")
        print(f"   éš¾åº¦: {sample.metadata.get('difficulty', 'N/A')}")
        print(f"   ä¸»é¢˜: {sample.metadata.get('topic', 'N/A')}")
        print()
    
    # å®Œæˆ
    print("=" * 80)
    print("âœ… æµ‹è¯•é—®é¢˜ç”Ÿæˆå®Œæˆ!")
    print("=" * 80)
    print()
    print("ğŸ“Š ä¸‹ä¸€æ­¥:")
    print(f"   1. æŸ¥çœ‹ç”Ÿæˆçš„é—®é¢˜: cat {args.output}")
    print(f"   2. è¿è¡Œ RAG è¯„ä¼°:")
    print(f"      python examples/evaluate_with_dataset.py {args.output}")
    print()


if __name__ == "__main__":
    asyncio.run(main())

