#!/usr/bin/env python3
"""
ä½¿ç”¨å·²ç”Ÿæˆçš„æµ‹è¯•æ•°æ®é›†è¯„ä¼° RAG ç³»ç»Ÿ

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. åŠ è½½å·²æœ‰çš„æµ‹è¯•æ•°æ®é›†ï¼ˆåªæœ‰ question å’Œ ground_truthï¼‰
2. è¿è¡Œ RAG ç³»ç»Ÿè·å–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
3. ä½¿ç”¨ RAGAS è¯„ä¼°
4. åˆ†æç»“æœå¹¶ä¿å­˜

Usage:
    # è¯„ä¼°æŒ‡å®šæ•°æ®é›†
    python examples/evaluate_with_dataset.py data/evaluation/generated_test_set.json
    
    # è¯„ä¼°å¹¶ä¿å­˜è¯¦ç»†ç»“æœ
    python examples/evaluate_with_dataset.py data/evaluation/generated_test_set.json --save-details
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import sys
import asyncio
import json
from pathlib import Path
from typing import List, Dict
import argparse

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag_core.evaluation import RAGASEvaluator, EvaluationDataset, EvaluationMetrics
from rag_core.retrievers import RetrieverFactory
from rag_core.rerankers import RerankerFactory
from rag_core.chains.llm import build_llm
from rag_core.chains.embeddings import build_embedding_function
from rag_core.chains.vectorstore import load_vector_store
from shared_config.settings import AppSettings


async def fill_rag_responses(
    dataset: EvaluationDataset,
    retriever,
    reranker,
    llm,
    settings: AppSettings,
    show_progress: bool = True
) -> EvaluationDataset:
    """
    ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸ªé—®é¢˜è¿è¡Œ RAG ç³»ç»Ÿï¼Œå¡«å……ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡ã€‚
    
    Args:
        dataset: è¾“å…¥æ•°æ®é›†ï¼ˆåªæœ‰ question å’Œ ground_truthï¼‰
        retriever: æ£€ç´¢å™¨
        reranker: é‡æ’å™¨ï¼ˆå¯é€‰ï¼‰
        llm: LLM
        settings: é…ç½®
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
    
    Returns:
        å¡«å……äº†ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡çš„æ•°æ®é›†
    """
    print(f"ğŸ”„ è¿è¡Œ RAG ç³»ç»Ÿå¡«å……ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡...")
    print(f"   é—®é¢˜æ•°é‡: {len(dataset)}")
    print(f"   æ£€ç´¢å™¨: {retriever.__class__.__name__}")
    if reranker:
        print(f"   é‡æ’å™¨: {reranker.__class__.__name__}")
    print()
    
    # åˆ›å»ºæ–°æ•°æ®é›†
    filled_dataset = EvaluationDataset(
        name=f"{dataset.name}_filled",
        description=f"RAG responses for {dataset.name}"
    )
    
    success_count = 0
    
    for i, sample in enumerate(dataset.samples, 1):
        question = sample.question
        
        if show_progress:
            print(f"   [{i}/{len(dataset)}] {question[:60]}...")
        
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            if hasattr(settings.retriever, 'top_k'):
                top_k = settings.retriever.top_k
            else:
                top_k = 5
            
            retrieved_docs = await retriever.retrieve(question, top_k=top_k * 2)  # å¤šå–ä¸€äº›
            
            # 2. å¯é€‰ï¼šé‡æ’åº
            if reranker and len(retrieved_docs) > 0:
                retrieved_docs = await reranker.rerank(
                    query=question,
                    documents=retrieved_docs,
                    top_n=top_k
                )
            else:
                retrieved_docs = retrieved_docs[:top_k]
            
            # 3. å‡†å¤‡ä¸Šä¸‹æ–‡
            contexts = [doc.page_content for doc in retrieved_docs]
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            if contexts:
                context_text = "\n\n".join(contexts)
                prompt = f"""Based on the following context, answer the question.

Context:
{context_text}

Question: {question}

Answer:"""
                
                response = await llm.ainvoke(prompt)
                answer = response.content if hasattr(response, 'content') else str(response)
            else:
                answer = "I don't have enough information to answer this question."
            
            # 5. æ·»åŠ åˆ°æ–°æ•°æ®é›†
            filled_dataset.add_sample(
                question=question,
                answer=answer,
                contexts=contexts,
                ground_truth=sample.ground_truth,
                metadata=sample.metadata
            )
            
            if show_progress:
                print(f"      âœ“ æ£€ç´¢: {len(contexts)} ä¸ªæ–‡æ¡£, ç­”æ¡ˆ: {len(answer)} å­—ç¬¦")
            
            success_count += 1
            
        except Exception as e:
            print(f"      âœ— é”™è¯¯: {e}")
            # æ·»åŠ ç©ºå“åº”
            filled_dataset.add_sample(
                question=question,
                answer="Error: Failed to generate response",
                contexts=[],
                ground_truth=sample.ground_truth,
                metadata=sample.metadata
            )
            continue
    
    print(f"\nâœ… å®Œæˆ! æˆåŠŸå¤„ç† {success_count}/{len(dataset)} ä¸ªé—®é¢˜\n")
    return filled_dataset


async def evaluate_dataset(
    dataset: EvaluationDataset,
    llm,
    embeddings,
    metrics: List[EvaluationMetrics]
) -> Dict:
    """
    ä½¿ç”¨ RAGAS è¯„ä¼°æ•°æ®é›†ã€‚
    
    Args:
        dataset: è¯„ä¼°æ•°æ®é›†
        llm: LLM ç”¨äºè¯„ä¼°
        embeddings: Embeddings ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
        metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
    
    Returns:
        è¯„ä¼°ç»“æœ
    """
    print("ğŸ¯ å¼€å§‹ RAGAS è¯„ä¼°...")
    print(f"   æ•°æ®é›†: {dataset.name}")
    print(f"   æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   æŒ‡æ ‡: {[m.value for m in metrics]}")
    print()
    
    evaluator = RAGASEvaluator(llm=llm, embeddings=embeddings)
    
    try:
        results = await evaluator.evaluate(dataset, metrics=metrics)
        
        print("âœ… è¯„ä¼°å®Œæˆ!\n")
        print(results.summary())
        print()
        
        return {
            "scores": results.scores,
            "metadata": results.metadata
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def analyze_results(results: Dict, dataset: EvaluationDataset):
    """åˆ†æè¯„ä¼°ç»“æœï¼Œæä¾›ä¼˜åŒ–å»ºè®®ã€‚"""
    print("=" * 80)
    print("ğŸ“ˆ ç»“æœåˆ†æå’Œä¼˜åŒ–å»ºè®®")
    print("=" * 80)
    print()
    
    if not results or "scores" not in results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°ç»“æœ")
        return
    
    scores = results["scores"]
    
    # åˆ†æå„é¡¹æŒ‡æ ‡
    print("1ï¸âƒ£  æŒ‡æ ‡åˆ†æ:")
    print()
    
    for metric, score in scores.items():
        print(f"   ğŸ“Œ {metric}: {score:.3f}")
        
        if score < 0.7:
            print(f"      âš ï¸  åˆ†æ•°åä½!")
        elif score < 0.85:
            print(f"      â„¹ï¸  åˆ†æ•°ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
        else:
            print(f"      âœ… åˆ†æ•°è‰¯å¥½!")
        print()
    
    # æ€»ä½“å»ºè®®
    print("2ï¸âƒ£  æ€»ä½“è¯„åˆ†:")
    print()
    
    avg_score = sum(scores.values()) / len(scores)
    print(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    
    if avg_score >= 0.85:
        grade = "A (ä¼˜ç§€)"
        emoji = "ğŸŸ¢"
    elif avg_score >= 0.75:
        grade = "B (è‰¯å¥½)"
        emoji = "ğŸŸ¡"
    elif avg_score >= 0.65:
        grade = "C (ä¸­ç­‰)"
        emoji = "ğŸŸ "
    else:
        grade = "D (éœ€æ”¹è¿›)"
        emoji = "ğŸ”´"
    
    print(f"   {emoji} è¯„çº§: {grade}")
    print()


def save_results(
    original_dataset: EvaluationDataset,
    filled_dataset: EvaluationDataset,
    results: Dict,
    output_dir: Path,
    save_details: bool = False
):
    """ä¿å­˜è¯„ä¼°ç»“æœã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å¡«å……åçš„æ•°æ®é›†
    filled_path = output_dir / f"{filled_dataset.name}.json"
    filled_dataset.save(filled_path)
    print(f"ğŸ’¾ å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {filled_path}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    if results:
        results_path = output_dir / f"{original_dataset.name}_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")
    
    # å¯é€‰ï¼šä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if save_details and results:
        report_path = output_dir / f"{original_dataset.name}_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# è¯„ä¼°æŠ¥å‘Š: {original_dataset.name}\n\n")
            f.write(f"## æ¦‚è§ˆ\n\n")
            f.write(f"- æ•°æ®é›†: {original_dataset.name}\n")
            f.write(f"- é—®é¢˜æ•°é‡: {len(original_dataset)}\n")
            f.write(f"- è¯„ä¼°æ—¶é—´: {results.get('metadata', {}).get('timestamp', 'N/A')}\n\n")
            
            f.write(f"## è¯„åˆ†\n\n")
            scores = results["scores"]
            avg_score = sum(scores.values()) / len(scores)
            f.write(f"**å¹³å‡åˆ†æ•°**: {avg_score:.3f}\n\n")
            f.write("| æŒ‡æ ‡ | åˆ†æ•° | è¯„ä»· |\n")
            f.write("|------|------|------|\n")
            for metric, score in scores.items():
                status = "âœ… ä¼˜ç§€" if score >= 0.85 else "ğŸŸ¡ è‰¯å¥½" if score >= 0.75 else "ğŸ”´ éœ€æ”¹è¿›"
                f.write(f"| {metric} | {score:.3f} | {status} |\n")
            
            f.write(f"\n## æ ·æœ¬ç¤ºä¾‹\n\n")
            for i, sample in enumerate(filled_dataset.samples[:3], 1):
                f.write(f"### ç¤ºä¾‹ {i}\n\n")
                f.write(f"**é—®é¢˜**: {sample.question}\n\n")
                f.write(f"**RAG ç­”æ¡ˆ**: {sample.answer[:200]}...\n\n")
                f.write(f"**å‚è€ƒç­”æ¡ˆ**: {sample.ground_truth[:200]}...\n\n")
                f.write(f"**æ£€ç´¢æ–‡æ¡£æ•°**: {len(sample.contexts)}\n\n")
        
        print(f"ğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    print()


async def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨å·²ç”Ÿæˆçš„æµ‹è¯•æ•°æ®é›†è¯„ä¼° RAG ç³»ç»Ÿ"
    )
    parser.add_argument(
        'dataset',
        type=Path,
        help='æµ‹è¯•æ•°æ®é›†è·¯å¾„ï¼ˆJSON æ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--output-dir', '-o',
        type=Path,
        default=Path('data/evaluation'),
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/evaluationï¼‰'
    )
    parser.add_argument(
        '--metrics', '-m',
        nargs='+',
        choices=['faithfulness', 'answer_relevancy', 'context_precision', 
                 'answer_similarity', 'answer_correctness', 'context_recall'],
        default=['faithfulness', 'answer_relevancy', 'context_precision'],
        help='è¯„ä¼°æŒ‡æ ‡ï¼ˆé»˜è®¤: faithfulness answer_relevancy context_precisionï¼‰'
    )
    parser.add_argument(
        '--save-details',
        action='store_true',
        help='ä¿å­˜è¯¦ç»†æŠ¥å‘Š'
    )
    
    args = parser.parse_args()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ RAG ç³»ç»Ÿè¯„ä¼° - ä½¿ç”¨å·²ç”Ÿæˆçš„æ•°æ®é›†")
    print("=" * 80)
    print()
    
    # éªŒè¯æ–‡ä»¶
    if not args.dataset.exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset}")
        return
    
    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
    try:
        settings = AppSettings()
        llm = build_llm(settings)
        embeddings = build_embedding_function(settings)
        vector_store = load_vector_store()  # ä¸éœ€è¦ä¼ é€’å‚æ•°
        
        if not vector_store:
            print("âŒ å‘é‡åº“ä¸ºç©º! è¯·å…ˆå¯¼å…¥æ–‡æ¡£")
            return
        
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = RetrieverFactory.create_from_settings(
            settings=settings,
            vector_store=vector_store
        )
        
        # åˆ›å»ºé‡æ’å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        reranker = None
        if settings.reranker.enabled:
            reranker = RerankerFactory.create_from_settings(settings)
        
        print(f"   LLM: {settings.llm.provider} / {settings.llm.model}")
        print(f"   æ£€ç´¢å™¨: {retriever.__class__.__name__}")
        if reranker:
            print(f"   é‡æ’å™¨: {reranker.__class__.__name__}")
        print()
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Step 1: åŠ è½½æ•°æ®é›†
    print("=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½æµ‹è¯•æ•°æ®é›†")
    print("=" * 80)
    print()
    
    try:
        dataset = EvaluationDataset.load(args.dataset)
        print(f"âœ… åŠ è½½æ•°æ®é›†: {dataset.name}")
        print(f"   é—®é¢˜æ•°é‡: {len(dataset)}")
        print()
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return
    
    # Step 2: è¿è¡Œ RAG ç³»ç»Ÿ
    print("=" * 80)
    print("æ­¥éª¤ 2: è¿è¡Œ RAG ç³»ç»Ÿ")
    print("=" * 80)
    print()
    
    filled_dataset = await fill_rag_responses(
        dataset, retriever, reranker, llm, settings
    )
    
    # Step 3: RAGAS è¯„ä¼°
    print("=" * 80)
    print("æ­¥éª¤ 3: RAGAS è¯„ä¼°")
    print("=" * 80)
    print()
    
    # è½¬æ¢æŒ‡æ ‡åç§°
    metrics = [EvaluationMetrics(m) for m in args.metrics]
    
    results = await evaluate_dataset(
        filled_dataset, llm, embeddings, metrics
    )
    
    # Step 4: åˆ†æç»“æœ
    if results:
        analyze_results(results, dataset)
    
    # Step 5: ä¿å­˜ç»“æœ
    print("=" * 80)
    print("æ­¥éª¤ 5: ä¿å­˜ç»“æœ")
    print("=" * 80)
    print()
    
    save_results(
        dataset, filled_dataset, results, 
        args.output_dir, args.save_details
    )
    
    # å®Œæˆ
    print("=" * 80)
    print("âœ… è¯„ä¼°å®Œæˆ!")
    print("=" * 80)
    print()


if __name__ == "__main__":
    asyncio.run(main())

