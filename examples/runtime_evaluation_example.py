#!/usr/bin/env python3
"""
è¿è¡Œæ—¶è¯„ä¼°ç³»ç»Ÿç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•åœ¨ RAG ç³»ç»Ÿä¸­é›†æˆè¿è¡Œæ—¶è¯„ä¼°
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import asyncio
import logging
import sys
import uuid
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ========================================
# é…ç½®æ—¥å¿—
# ========================================
logging.basicConfig(
    level=logging.INFO,  # ä½¿ç”¨ INFO çº§åˆ«æŸ¥çœ‹è¯„ä¼°è¿‡ç¨‹
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('runtime_evaluation_example.log')  # ä¿å­˜åˆ°æ–‡ä»¶
    ]
)

# å¦‚æœæƒ³çœ‹æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¯ä»¥è®¾ç½®ä¸º DEBUG
# logging.getLogger('rag_core.evaluation').setLevel(logging.DEBUG)

from rag_core.evaluation import (
    create_runtime_evaluator,
    EvaluationMode,
)
from rag_core.chains.llm import build_llm
from rag_core.chains.embeddings import build_embedding_function
from rag_core.chains.vectorstore import load_vector_store
from rag_core.retrievers import RetrieverFactory
from shared_config.settings import AppSettings


async def simulate_rag_query(
    question: str,
    retriever,
    llm,
    runtime_evaluator
):
    """
    æ¨¡æ‹Ÿä¸€æ¬¡ RAG æŸ¥è¯¢ï¼Œå¹¶è¿›è¡Œè¿è¡Œæ—¶è¯„ä¼°ã€‚
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        retriever: æ£€ç´¢å™¨
        llm: LLM
        runtime_evaluator: è¿è¡Œæ—¶è¯„ä¼°å™¨
    """
    query_id = str(uuid.uuid4())
    
    print(f"\n{'='*60}")
    print(f"ğŸ“ æŸ¥è¯¢: {question}")
    print(f"   Query ID: {query_id}")
    
    try:
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print(f"ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        retrieved_docs = await retriever.retrieve(question, top_k=5)
        contexts = [doc.page_content for doc in retrieved_docs]
        print(f"   âœ“ æ£€ç´¢åˆ° {len(contexts)} ä¸ªæ–‡æ¡£")
        
        # 2. ç”Ÿæˆç­”æ¡ˆ
        print(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ...")
        if contexts:
            context_text = "\n\n".join(contexts)
            prompt = f"""Based on the following context, answer the question.

Context:
{context_text}

Question: {question}

Answer:"""
            
            response = await llm.ainvoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
        else:
            answer = "I don't have enough information to answer this question."
        
        print(f"   âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
        print(f"   ç­”æ¡ˆ: {answer[:100]}...")
        
        # 3. è®°å½•åˆ°è¿è¡Œæ—¶è¯„ä¼°ç³»ç»Ÿ
        print(f"ğŸ“Š è®°å½•åˆ°è¯„ä¼°ç³»ç»Ÿ...")
        await runtime_evaluator.record_query(
            query_id=query_id,
            question=question,
            answer=answer,
            contexts=contexts,
            metadata={
                "source": "example",
                "num_contexts": len(contexts)
            }
        )
        
        # åˆ¤æ–­æ˜¯å¦ä¼šè¢«è¯„ä¼°
        if runtime_evaluator.should_evaluate():
            print(f"   âœ… è¯¥æŸ¥è¯¢å°†è¢«è¯„ä¼°")
        else:
            print(f"   â­ï¸  è¯¥æŸ¥è¯¢å·²è·³è¿‡è¯„ä¼°ï¼ˆé‡‡æ ·ï¼‰")
        
        return {
            "query_id": query_id,
            "question": question,
            "answer": answer,
            "contexts": contexts
        }
    
    except Exception as e:
        print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


async def evaluation_callback(result: dict):
    """è¯„ä¼°å®Œæˆåçš„å›è°ƒå‡½æ•°"""
    print(f"\nğŸ¯ è¯„ä¼°å®Œæˆå›è°ƒ:")
    print(f"   Query ID: {result.get('query_id', 'N/A')}")
    print(f"   Scores: {result.get('scores', {})}")


async def main():
    print("\n" + "="*60)
    print("ğŸ¯ è¿è¡Œæ—¶è¯„ä¼°ç³»ç»Ÿç¤ºä¾‹")
    print("="*60)
    print()
    
    # ========================================
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    # ========================================
    print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
    try:
        settings = AppSettings()
        llm = build_llm(settings)
        embeddings = build_embedding_function(settings)
        vector_store = load_vector_store()  # ä¸éœ€è¦ä¼ é€’å‚æ•°
        
        if not vector_store:
            print("âŒ å‘é‡åº“ä¸ºç©º! è¯·å…ˆå¯¼å…¥æ–‡æ¡£")
            return
        
        retriever = RetrieverFactory.create_from_settings(
            settings=settings,
            vector_store=vector_store
        )
        
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print()
    
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # ========================================
    # 2. åˆ›å»ºè¿è¡Œæ—¶è¯„ä¼°å™¨
    # ========================================
    print("ğŸ“Š åˆ›å»ºè¿è¡Œæ—¶è¯„ä¼°å™¨...")
    print()
    
    # æ–¹å¼ 1: é‡‡æ ·è¯„ä¼°æ¨¡å¼ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰
    runtime_evaluator = create_runtime_evaluator(
        llm=llm,
        embeddings=embeddings,
        mode="sampling",          # é‡‡æ ·æ¨¡å¼
        sampling_rate=0.5,        # è¯„ä¼° 50% çš„æŸ¥è¯¢ï¼ˆç¤ºä¾‹ç”¨ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®® 0.1ï¼‰
        metrics=["faithfulness", "answer_relevancy"]
    )
    
    # æ–¹å¼ 2: å¼‚æ­¥è¯„ä¼°æ‰€æœ‰æŸ¥è¯¢ï¼ˆé€‚åˆæµ‹è¯•ç¯å¢ƒï¼‰
    # runtime_evaluator = create_runtime_evaluator(
    #     llm=llm,
    #     embeddings=embeddings,
    #     mode="async_all",
    #     metrics=["faithfulness", "answer_relevancy"]
    # )
    
    # æ–¹å¼ 3: æ‰¹é‡è¯„ä¼°æ¨¡å¼ï¼ˆé€‚åˆç¦»çº¿åˆ†æï¼‰
    # runtime_evaluator = create_runtime_evaluator(
    #     llm=llm,
    #     embeddings=embeddings,
    #     mode="batch",
    #     metrics=["faithfulness", "answer_relevancy"]
    # )
    
    # è®¾ç½®å›è°ƒ
    runtime_evaluator.on_evaluation_complete = evaluation_callback
    
    # å¯åŠ¨è¯„ä¼°å™¨ï¼ˆå¦‚æœæ˜¯æ‰¹é‡æ¨¡å¼ï¼‰
    await runtime_evaluator.start()
    
    print()
    
    # ========================================
    # 3. æ¨¡æ‹Ÿå¤šæ¬¡ RAG æŸ¥è¯¢
    # ========================================
    print("="*60)
    print("æ¨¡æ‹Ÿ RAG æŸ¥è¯¢")
    print("="*60)
    
    test_questions = [
        "What is machine learning?",
        "How do neural networks work?",
        "Explain the concept of deep learning",
        "What is the difference between AI and ML?",
        "How does backpropagation work?",
    ]
    
    results = []
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢ {i}/{len(test_questions)}")
        print(f"{'='*60}")
        
        result = await simulate_rag_query(
            question=question,
            retriever=retriever,
            llm=llm,
            runtime_evaluator=runtime_evaluator
        )
        
        if result:
            results.append(result)
        
        # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å… API é™æµ
        await asyncio.sleep(2)
    
    # ========================================
    # 4. ç­‰å¾…å¼‚æ­¥è¯„ä¼°å®Œæˆ
    # ========================================
    print(f"\n{'='*60}")
    print("â³ ç­‰å¾…å¼‚æ­¥è¯„ä¼°å®Œæˆ...")
    print(f"{'='*60}")
    await asyncio.sleep(5)  # ç­‰å¾…å¼‚æ­¥è¯„ä¼°å®Œæˆ
    
    # ========================================
    # 5. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    # ========================================
    print(f"\n{'='*60}")
    print("ğŸ“ˆ è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯")
    print(f"{'='*60}")
    
    stats = runtime_evaluator.get_stats()
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
    print(f"   å·²è¯„ä¼°: {stats['evaluated_queries']}")
    print(f"   å·²è·³è¿‡: {stats['skipped_queries']}")
    print(f"   è¯„ä¼°ç‡: {stats['evaluation_rate']*100:.1f}%")
    print(f"   é”™è¯¯æ•°: {stats['evaluation_errors']}")
    
    if "recent_avg_scores" in stats:
        print(f"\næœ€è¿‘çš„å¹³å‡åˆ†æ•°:")
        for metric, score in stats["recent_avg_scores"].items():
            print(f"   {metric}: {score:.3f}")
    
    # ========================================
    # 6. æŸ¥çœ‹æœ€è¿‘çš„è¯„ä¼°ç»“æœ
    # ========================================
    recent_results = runtime_evaluator.get_recent_results(limit=3)
    
    if recent_results:
        print(f"\næœ€è¿‘çš„è¯„ä¼°ç»“æœ (æœ€å¤š 3 ä¸ª):")
        for i, result in enumerate(recent_results, 1):
            print(f"\n   ç»“æœ {i}:")
            print(f"      Query ID: {result.get('query_id', 'N/A')}")
            print(f"      Question: {result.get('question', 'N/A')}")
            if 'scores' in result:
                print(f"      Scores:")
                for metric, score in result['scores'].items():
                    print(f"         {metric}: {score:.3f}")
    
    # ========================================
    # 7. åœæ­¢è¯„ä¼°å™¨
    # ========================================
    print(f"\n{'='*60}")
    print("ğŸ›‘ åœæ­¢è¯„ä¼°å™¨...")
    await runtime_evaluator.stop()
    
    print(f"\n{'='*60}")
    print("âœ… ç¤ºä¾‹å®Œæˆï¼")
    print(f"{'='*60}")
    print()
    print(f"ğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: data/evaluation/runtime/")
    print(f"   æŸ¥çœ‹ç»“æœ: cat data/evaluation/runtime/runtime_evaluation_results.jsonl")
    print()


if __name__ == "__main__":
    asyncio.run(main())

