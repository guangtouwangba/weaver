#!/usr/bin/env python3
"""
å®é™…è¯„ä¼° RAG ç³»ç»Ÿçš„å®Œæ•´ç¤ºä¾‹è„šæœ¬

ä½¿ç”¨æ­¥éª¤:
1. å‡†å¤‡æµ‹è¯•é—®é¢˜åˆ—è¡¨
2. è¿è¡Œ RAG ç³»ç»Ÿè·å–ç­”æ¡ˆ
3. æ”¶é›†è¯„ä¼°æ•°æ®
4. ä½¿ç”¨ RAGAS è¯„ä¼°
5. åˆ†æå’Œä¼˜åŒ–

Usage:
    python examples/evaluate_rag_system.py
"""

# Fix OpenMP conflict on macOS
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import asyncio
import sys
from pathlib import Path
from typing import List, Dict
import json

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag_core.evaluation import RAGASEvaluator, EvaluationDataset, EvaluationMetrics
from rag_core.retrievers import HybridRetriever, VectorRetriever, RetrieverFactory
from rag_core.chains.llm import build_llm
from rag_core.chains.embeddings import build_embedding_function
from rag_core.chains.vectorstore import load_vector_store
from shared_config.settings import AppSettings


# ========================================
# æ­¥éª¤ 1: å‡†å¤‡æµ‹è¯•é—®é¢˜
# ========================================

def load_test_questions() -> List[Dict[str, str]]:
    """
    åŠ è½½æµ‹è¯•é—®é¢˜åˆ—è¡¨ã€‚
    
    åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä½ å¯ä»¥ä»æ–‡ä»¶åŠ è½½æˆ–ç›´æ¥å®šä¹‰ã€‚
    """
    test_questions = [
        {
            "question": "What is machine learning?",
            "ground_truth": "Machine learning is a method of data analysis that automates analytical model building.",
            "topic": "ml_basics"
        },
        {
            "question": "How do neural networks learn?",
            "ground_truth": "Neural networks learn through backpropagation and gradient descent.",
            "topic": "neural_networks"
        },
        {
            "question": "What is the difference between supervised and unsupervised learning?",
            "ground_truth": "Supervised learning uses labeled data, while unsupervised learning finds patterns in unlabeled data.",
            "topic": "ml_types"
        },
        # æ·»åŠ æ›´å¤šé—®é¢˜...
    ]
    
    return test_questions


# ========================================
# æ­¥éª¤ 2: è¿è¡Œ RAG ç³»ç»Ÿæ”¶é›†æ•°æ®
# ========================================

async def collect_rag_responses(
    test_questions: List[Dict],
    retriever,
    llm,
    settings: AppSettings
) -> EvaluationDataset:
    """
    è¿è¡Œ RAG ç³»ç»Ÿï¼Œæ”¶é›†è¯„ä¼°æ•°æ®ã€‚
    
    Args:
        test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
        retriever: æ£€ç´¢å™¨å®ä¾‹
        llm: LLM å®ä¾‹
        settings: åº”ç”¨é…ç½®
    
    Returns:
        åŒ…å« RAG å“åº”çš„è¯„ä¼°æ•°æ®é›†
    """
    print(f"ğŸ“Š å¼€å§‹æ”¶é›† RAG ç³»ç»Ÿå“åº”...")
    print(f"   é—®é¢˜æ•°é‡: {len(test_questions)}")
    print()
    
    dataset = EvaluationDataset(
        name="rag_system_evaluation",
        description=f"RAG system evaluation with {len(test_questions)} questions"
    )
    
    for i, item in enumerate(test_questions, 1):
        question = item["question"]
        ground_truth = item.get("ground_truth")
        
        print(f"   [{i}/{len(test_questions)}] å¤„ç†: {question[:50]}...")
        
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = await retriever.retrieve(question, top_k=5)
            
            # 2. å‡†å¤‡ä¸Šä¸‹æ–‡
            contexts = [doc.page_content for doc in retrieved_docs]
            
            # 3. ç”Ÿæˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä½¿ç”¨å®Œæ•´çš„ QA chainï¼‰
            if contexts:
                context_text = "\n\n".join(contexts)
                prompt = f"""Based on the following context, answer the question.

Context:
{context_text}

Question: {question}

Answer:"""
                
                answer = await llm.ainvoke(prompt)
                if hasattr(answer, 'content'):
                    answer_text = answer.content
                else:
                    answer_text = str(answer)
            else:
                answer_text = "I don't have enough information to answer this question."
            
            # 4. æ·»åŠ åˆ°è¯„ä¼°æ•°æ®é›†
            dataset.add_sample(
                question=question,
                answer=answer_text,
                contexts=contexts,
                ground_truth=ground_truth,
                metadata={
                    "topic": item.get("topic", "unknown"),
                    "num_contexts": len(contexts)
                }
            )
            
            print(f"      âœ“ æ£€ç´¢åˆ° {len(contexts)} ä¸ªæ–‡æ¡£")
            
        except Exception as e:
            print(f"      âœ— é”™è¯¯: {e}")
            continue
    
    print(f"\nâœ… æ”¶é›†å®Œæˆ! æˆåŠŸå¤„ç† {len(dataset)} ä¸ªé—®é¢˜\n")
    return dataset


# ========================================
# æ­¥éª¤ 3: è¿è¡Œ RAGAS è¯„ä¼°
# ========================================

async def evaluate_with_ragas(
    dataset: EvaluationDataset,
    llm,
    embeddings
) -> Dict:
    """
    ä½¿ç”¨ RAGAS è¯„ä¼°æ•°æ®é›†ã€‚
    
    Args:
        dataset: è¯„ä¼°æ•°æ®é›†
        llm: LLM ç”¨äºè¯„ä¼°
        embeddings: Embeddings ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print("ğŸ¯ å¼€å§‹ RAGAS è¯„ä¼°...")
    print(f"   æ•°æ®é›†: {dataset.name}")
    print(f"   æ ·æœ¬æ•°: {len(dataset)}")
    print()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGASEvaluator(
        llm=llm,
        embeddings=embeddings
    )
    
    # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
    metrics = [
        EvaluationMetrics.FAITHFULNESS,        # æ£€æµ‹å¹»è§‰
        EvaluationMetrics.ANSWER_RELEVANCY,    # ç­”æ¡ˆç›¸å…³æ€§
        EvaluationMetrics.CONTEXT_PRECISION,   # æ£€ç´¢ç²¾ç¡®åº¦
    ]
    
    # å¦‚æœæœ‰ ground truthï¼Œæ·»åŠ æ›´å¤šæŒ‡æ ‡
    has_ground_truth = any(s.ground_truth for s in dataset.samples)
    if has_ground_truth:
        metrics.append(EvaluationMetrics.ANSWER_SIMILARITY)
        print("   â„¹ï¸  æ£€æµ‹åˆ° ground truthï¼Œå°†è¯„ä¼°ç­”æ¡ˆç›¸ä¼¼åº¦")
    
    print(f"   è¯„ä¼°æŒ‡æ ‡: {[m.value for m in metrics]}")
    print()
    
    try:
        # è¿è¡Œè¯„ä¼°
        results = await evaluator.evaluate(
            dataset,
            metrics=metrics
        )
        
        print("âœ… è¯„ä¼°å®Œæˆ!\n")
        print(results.summary())
        print()
        
        return {
            "scores": results.scores,
            "metadata": results.metadata
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


# ========================================
# æ­¥éª¤ 4: åˆ†æç»“æœ
# ========================================

def analyze_results(results: Dict, dataset: EvaluationDataset):
    """åˆ†æè¯„ä¼°ç»“æœï¼Œæä¾›ä¼˜åŒ–å»ºè®®ã€‚"""
    print("=" * 80)
    print("ğŸ“ˆ ç»“æœåˆ†æå’Œä¼˜åŒ–å»ºè®®")
    print("=" * 80)
    print()
    
    if not results or "scores" not in results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°ç»“æœ")
        return
    
    scores = results["scores"]
    
    # åˆ†æå„é¡¹æŒ‡æ ‡
    print("1ï¸âƒ£  æŒ‡æ ‡åˆ†æ:")
    print()
    
    # Faithfulness
    if "faithfulness" in scores:
        faith_score = scores["faithfulness"]
        print(f"   ğŸ“Œ Faithfulness (å¿ å®åº¦): {faith_score:.3f}")
        if faith_score < 0.7:
            print("      âš ï¸  åˆ†æ•°åä½! å¯èƒ½å­˜åœ¨å¹»è§‰é—®é¢˜")
            print("      ğŸ’¡ å»ºè®®:")
            print("         - æ”¹è¿› promptï¼Œå¼ºè°ƒ'ä»…åŸºäºä¸Šä¸‹æ–‡å›ç­”'")
            print("         - æå‡æ£€ç´¢è´¨é‡ï¼Œç¡®ä¿ç›¸å…³ä¸Šä¸‹æ–‡")
            print("         - è€ƒè™‘ä½¿ç”¨æ›´å¯æ§çš„ LLM")
        elif faith_score < 0.85:
            print("      â„¹ï¸  åˆ†æ•°ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
            print("      ğŸ’¡ å»ºè®®: æ£€æŸ¥ä½åˆ†æ ·æœ¬ï¼Œé’ˆå¯¹æ€§ä¼˜åŒ–")
        else:
            print("      âœ… åˆ†æ•°è‰¯å¥½! å¹»è§‰é—®é¢˜è¾ƒå°‘")
        print()
    
    # Answer Relevancy
    if "answer_relevancy" in scores:
        rel_score = scores["answer_relevancy"]
        print(f"   ğŸ“Œ Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§): {rel_score:.3f}")
        if rel_score < 0.7:
            print("      âš ï¸  ç­”æ¡ˆç»å¸¸è·‘é¢˜æˆ–è¿‡äºé€šç”¨")
            print("      ğŸ’¡ å»ºè®®:")
            print("         - ä¼˜åŒ– promptï¼Œä½¿ç­”æ¡ˆæ›´èšç„¦")
            print("         - è°ƒæ•´ç”Ÿæˆå‚æ•° (temperature, max_tokens)")
            print("         - æ”¹è¿›é—®é¢˜ç†è§£å’Œä¸Šä¸‹æ–‡é€‰æ‹©")
        elif rel_score < 0.85:
            print("      â„¹ï¸  ç›¸å…³æ€§å°šå¯ï¼Œå¯ä»¥ç»§ç»­ä¼˜åŒ–")
        else:
            print("      âœ… ç­”æ¡ˆç›¸å…³æ€§å¾ˆå¥½!")
        print()
    
    # Context Precision
    if "context_precision" in scores:
        prec_score = scores["context_precision"]
        print(f"   ğŸ“Œ Context Precision (æ£€ç´¢ç²¾ç¡®åº¦): {prec_score:.3f}")
        if prec_score < 0.7:
            print("      âš ï¸  æ£€ç´¢åˆ°çš„æ–‡æ¡£ç›¸å…³æ€§ä¸è¶³")
            print("      ğŸ’¡ å»ºè®®:")
            print("         - ä½¿ç”¨ HybridRetriever (BM25 + Vector)")
            print("         - æ·»åŠ  Reranker ç²¾æ’")
            print("         - è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼")
            print("         - å¢åŠ  top_k å†ç”¨ reranker ç­›é€‰")
        elif prec_score < 0.85:
            print("      â„¹ï¸  æ£€ç´¢è´¨é‡ä¸­ç­‰")
            print("      ğŸ’¡ å»ºè®®: è€ƒè™‘æ·»åŠ  reranker æå‡ç²¾åº¦")
        else:
            print("      âœ… æ£€ç´¢è´¨é‡å¾ˆå¥½!")
        print()
    
    # æ€»ä½“å»ºè®®
    print("2ï¸âƒ£  æ€»ä½“å»ºè®®:")
    print()
    
    avg_score = sum(scores.values()) / len(scores)
    print(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    
    if avg_score < 0.7:
        print("   ğŸ”´ ç³»ç»Ÿè´¨é‡éœ€è¦æ˜¾è‘—æ”¹è¿›")
        print("   ä¼˜å…ˆçº§:")
        print("      1. æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆå‘é‡åº“æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£ï¼‰")
        print("      2. ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ï¼ˆhybrid + rerankerï¼‰")
        print("      3. æ”¹è¿› prompt engineering")
    elif avg_score < 0.85:
        print("   ğŸŸ¡ ç³»ç»Ÿè´¨é‡ä¸­ç­‰ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´")
        print("   å»ºè®®:")
        print("      1. æ‰¾å‡ºä½åˆ†æ ·æœ¬ï¼Œåˆ†æåŸå› ")
        print("      2. é’ˆå¯¹æ€§ä¼˜åŒ–è–„å¼±ç¯èŠ‚")
        print("      3. A/B æµ‹è¯•ä¸åŒé…ç½®")
    else:
        print("   ğŸŸ¢ ç³»ç»Ÿè´¨é‡è‰¯å¥½!")
        print("   ç»§ç»­:")
        print("      1. åœ¨æ›´å¤šæ ·æœ¬ä¸Šæµ‹è¯•")
        print("      2. ç›‘æ§ç”Ÿäº§ç¯å¢ƒè¡¨ç°")
        print("      3. æŒç»­æ”¶é›†ç”¨æˆ·åé¦ˆ")
    
    print()


# ========================================
# æ­¥éª¤ 5: ä¿å­˜ç»“æœ
# ========================================

def save_evaluation_results(
    dataset: EvaluationDataset,
    results: Dict,
    output_dir: Path
):
    """ä¿å­˜è¯„ä¼°æ•°æ®å’Œç»“æœã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ•°æ®é›†
    dataset_path = output_dir / f"{dataset.name}.json"
    dataset.save(dataset_path)
    print(f"ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {dataset_path}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    if results:
        results_path = output_dir / f"{dataset.name}_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")
    
    print()


# ========================================
# ä¸»å‡½æ•°
# ========================================

async def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„ RAG è¯„ä¼°æµç¨‹ã€‚"""
    print("\n" + "=" * 80)
    print("ğŸ¯ RAG ç³»ç»Ÿè¯„ä¼° - å®Œæ•´æµç¨‹")
    print("=" * 80)
    print()
    
    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿ...")
    try:
        settings = AppSettings()
        print(f"   LLM Provider: {settings.llm.provider}")
        print(f"   LLM Model: {settings.llm.model}")
        print(f"   Embedding Provider: {settings.embedding.provider}")
        print()
        
        # åˆå§‹åŒ–ç»„ä»¶
        llm = build_llm(settings)
        embeddings = build_embedding_function(settings)
        vector_store = load_vector_store()
        
        if not vector_store:
            print("âŒ å‘é‡åº“ä¸ºç©º! è¯·å…ˆå¯¼å…¥æ–‡æ¡£:")
            print("   python examples/ingest_documents.py")
            return
        
        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®ï¼‰
        retriever = RetrieverFactory.create_from_settings(
            settings=settings,
            vector_store=vector_store
        )
        
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   æ£€ç´¢å™¨: {retriever.__class__.__name__}")
        print()
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ­¥éª¤ 1: åŠ è½½æµ‹è¯•é—®é¢˜
    print("=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½æµ‹è¯•é—®é¢˜")
    print("=" * 80)
    print()
    
    test_questions = load_test_questions()
    print(f"âœ… åŠ è½½äº† {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜")
    print()
    
    # æ­¥éª¤ 2: æ”¶é›† RAG å“åº”
    print("=" * 80)
    print("æ­¥éª¤ 2: è¿è¡Œ RAG ç³»ç»Ÿæ”¶é›†æ•°æ®")
    print("=" * 80)
    print()
    
    dataset = await collect_rag_responses(
        test_questions,
        retriever,
        llm,
        settings
    )
    
    if len(dataset) == 0:
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
        return
    
    # æ­¥éª¤ 3: RAGAS è¯„ä¼°
    print("=" * 80)
    print("æ­¥éª¤ 3: RAGAS è¯„ä¼°")
    print("=" * 80)
    print()
    
    results = await evaluate_with_ragas(dataset, llm, embeddings)
    
    # æ­¥éª¤ 4: åˆ†æç»“æœ
    if results:
        analyze_results(results, dataset)
    
    # æ­¥éª¤ 5: ä¿å­˜ç»“æœ
    print("=" * 80)
    print("æ­¥éª¤ 5: ä¿å­˜ç»“æœ")
    print("=" * 80)
    print()
    
    output_dir = project_root / "data" / "evaluation"
    save_evaluation_results(dataset, results, output_dir)
    
    # å®Œæˆ
    print("=" * 80)
    print("âœ… è¯„ä¼°æµç¨‹å®Œæˆ!")
    print("=" * 80)
    print()
    print("ğŸ“Š ä¸‹ä¸€æ­¥:")
    print("   1. æŸ¥çœ‹ä¿å­˜çš„è¯„ä¼°ç»“æœ")
    print("   2. æ ¹æ®å»ºè®®ä¼˜åŒ–ç³»ç»Ÿ")
    print("   3. é‡æ–°è¯„ä¼°éªŒè¯æ”¹è¿›")
    print("   4. å»ºç«‹æŒç»­è¯„ä¼°æµç¨‹")
    print()


if __name__ == "__main__":
    asyncio.run(main())

