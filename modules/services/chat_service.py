"""
Chat Service

æä¾›èŠå¤©åŠŸèƒ½çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼Œé›†æˆRAGæ£€ç´¢å’ŒAIç”Ÿæˆã€‚
"""

import asyncio
import json
import uuid
from datetime import datetime, timezone
from typing import AsyncGenerator, Dict, List, Optional, Any, Tuple

try:
    import openai
except ImportError:
    openai = None

from sqlalchemy.ext.asyncio import AsyncSession

from config.settings import get_config
from logging_system import get_logger
from modules.services.base_service import BaseService
from modules.services.elasticsearch_service import elasticsearch_chat_service
from modules.schemas.chat import (
    ChatRequest,
    ChatResponse,
    ChatMessage,
    RetrievedContext,
    AIMetadata,
    MessageRole,
    ConversationSummary,
    ChatSearchResult,
    SearchType
)
from modules.vector_store.weaviate_service import WeaviateVectorStore
from modules.embedding.openai_service import OpenAIEmbeddingService

logger = get_logger(__name__)


class ChatService(BaseService):
    """èŠå¤©æœåŠ¡"""
    
    def __init__(self, session: AsyncSession = None, ai_client=None):
        if session:
            super().__init__(session)
        else:
            self.session = None
            self.logger = logger
        
        self.config = get_config()
        self.ai_client = ai_client
        self.es_service = elasticsearch_chat_service
        
        # RAGç»„ä»¶
        self._vector_store: Optional[WeaviateVectorStore] = None
        self._embedding_service: Optional[OpenAIEmbeddingService] = None
        
        # åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡ç»„ä»¶"""
        if self._initialized:
            return
            
        try:
            # åˆå§‹åŒ–Elasticsearch
            await self.es_service.initialize()
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            weaviate_url = getattr(self.config, 'weaviate_url', 'http://localhost:8080')
            self._vector_store = WeaviateVectorStore(url=weaviate_url)
            await self._vector_store.initialize()
            
            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if hasattr(self.config, 'ai') and hasattr(self.config.ai, 'embedding'):
                api_key = getattr(self.config.ai.embedding.openai, 'api_key', None)
                if api_key:
                    self._embedding_service = OpenAIEmbeddingService(api_key=api_key)
                    await self._embedding_service.initialize()
                    
            # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
            if not self.ai_client and openai and hasattr(self.config, 'ai'):
                # æ­£ç¡®çš„é…ç½®è·¯å¾„ï¼šconfig.ai.chat.openai.api_key
                api_key = getattr(self.config.ai.chat.openai, 'api_key', None)
                if api_key:
                    self.ai_client = openai.AsyncOpenAI(
                        api_key=api_key,
                        organization=getattr(self.config.ai.chat.openai, 'organization', None),
                        base_url=getattr(self.config.ai.chat.openai, 'api_base', None),
                        timeout=getattr(self.config.ai.chat.openai, 'timeout', 60),
                        max_retries=getattr(self.config.ai.chat.openai, 'max_retries', 3)
                    )
                    logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                else:
                    raise Exception("æœªé…ç½®OpenAI APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥ AI__CHAT__OPENAI__API_KEY ç¯å¢ƒå˜é‡")
            
            self._initialized = True
            logger.info("âœ… ChatServiceåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ChatServiceåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """å¤„ç†èŠå¤©è¯·æ±‚ï¼ˆåŒæ­¥æ–¹å¼ï¼‰"""
        await self.initialize()
        
        try:
            # ç”Ÿæˆå¯¹è¯ID
            conversation_id = request.conversation_id or str(uuid.uuid4())
            message_id = str(uuid.uuid4())
            
            # 1. RAGæ£€ç´¢
            start_time = datetime.now(timezone.utc)
            retrieved_contexts, search_time_ms = await self._retrieve_contexts(
                query=request.message,
                topic_id=request.topic_id,
                search_type=request.search_type,
                max_results=request.max_results,
                score_threshold=request.score_threshold
            )
            
            # 2. è·å–å¯¹è¯å†å²
            conversation_history = []
            if request.context_window > 0:
                conversation_history = await self.get_conversation_messages(
                    conversation_id, 
                    limit=request.context_window * 2
                )
            
            # 3. æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(
                user_message=request.message,
                retrieved_contexts=retrieved_contexts if request.include_context else [],
                conversation_history=conversation_history
            )
            
            # 4. ç”ŸæˆAIå›ç­”
            generation_start = datetime.now(timezone.utc)
            ai_response, tokens_used = await self._generate_ai_response(
                prompt=prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            generation_time_ms = int((datetime.now(timezone.utc) - generation_start).total_seconds() * 1000)
            
            # 5. ä¿å­˜å¯¹è¯
            await self.es_service.save_conversation(
                conversation_id=conversation_id,
                user_message=request.message,
                assistant_message=ai_response,
                topic_id=request.topic_id,
                retrieved_contexts=retrieved_contexts,
                ai_metadata={
                    "model": "gpt-3.5-turbo",
                    "tokens_used": tokens_used,
                    "generation_time_ms": generation_time_ms,
                    "search_time_ms": search_time_ms,
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens
                }
            )
            
            # 6. æ„å»ºå“åº”
            return ChatResponse(
                message_id=message_id,
                conversation_id=conversation_id,
                content=ai_response,
                retrieved_contexts=retrieved_contexts,
                ai_metadata=AIMetadata(
                    model="gpt-3.5-turbo",
                    tokens_used=tokens_used,
                    generation_time_ms=generation_time_ms,
                    search_time_ms=search_time_ms,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ),
                timestamp=datetime.now(timezone.utc)
            )
            
        except Exception as e:
            logger.error(f"âŒ èŠå¤©å¤„ç†å¤±è´¥: {e}")
            raise
    
    async def chat_with_summary(self, request: Dict[str, Any]) -> ChatResponse:
        """åŸºäºæ‘˜è¦ç´¢å¼•çš„èŠå¤©åŠŸèƒ½"""
        try:
            query = request.get("query", "")
            topic_id = request.get("topic_id")
            max_results = request.get("max_results", 5)
            score_threshold = request.get("score_threshold", 0.75)
            enhanced_query = request.get("enhanced_query", query)
            
            logger.info(f"ğŸ” å¼€å§‹æ‘˜è¦èŠå¤© - æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}', topic_id: {topic_id}")
            
            # æ£€ç´¢æ‘˜è¦ä¸Šä¸‹æ–‡
            summary_contexts, context_count = await self._retrieve_summary_contexts(
                query=query,
                topic_id=topic_id,
                max_results=max_results,
                score_threshold=score_threshold
            )
            
            # å¦‚æœæ‘˜è¦ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢
            if context_count < 2:
                logger.info("ğŸ”„ æ‘˜è¦ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢")
                contexts, context_count = await self._retrieve_contexts(
                    query=query,
                    topic_id=topic_id,
                    max_results=max_results,
                    score_threshold=score_threshold
                )
            else:
                contexts = summary_contexts
            
            # æ„å»ºæ‘˜è¦é£æ ¼çš„æç¤º
            prompt = self._build_summary_prompt(
                query=enhanced_query or query,
                contexts=contexts,
                style=request.get("response_style", "summary")
            )
            
            # ç”ŸæˆAIå“åº”
            if request.get("stream", False):
                response_content = await self._generate_ai_response_stream(prompt)
            else:
                response_content = await self._generate_ai_response(prompt)
            
            # æ„å»ºå“åº”
            response = ChatResponse(
                content=response_content,
                retrieved_context=contexts,
                ai_metadata=AIMetadata(
                    model="gpt-3.5-turbo",
                    search_type="summary",
                    context_count=context_count,
                    processing_time=0.0
                ),
                timestamp=datetime.now(timezone.utc)
            )
            
            logger.info(f"âœ… æ‘˜è¦èŠå¤©å®Œæˆ - ä¸Šä¸‹æ–‡: {context_count}ä¸ª")
            return response
            
        except Exception as e:
            logger.error(f"âŒ æ‘˜è¦èŠå¤©å¤„ç†å¤±è´¥: {e}")
            raise
    
    async def _retrieve_summary_contexts(
        self,
        query: str,
        topic_id: Optional[str] = None,
        max_results: int = 5,
        score_threshold: float = 0.75
    ) -> tuple[List[RetrievedContext], int]:
        """æ£€ç´¢æ‘˜è¦ä¸Šä¸‹æ–‡"""
        logger.info(f"ğŸ” å¼€å§‹æ‘˜è¦æ£€ç´¢ - æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}', "
                   f"topic_id: {topic_id}, max_results: {max_results}, "
                   f"score_threshold: {score_threshold}")
        
        if not self._vector_store or not self._embedding_service:
            logger.warning("âš ï¸ å‘é‡å­˜å‚¨æˆ–åµŒå…¥æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ‘˜è¦æ£€ç´¢")
            return [], 0
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            logger.debug("ğŸ§® ç”ŸæˆæŸ¥è¯¢å‘é‡åµŒå…¥...")
            query_embedding = await self._embedding_service.generate_embedding(query)
            logger.debug(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filters = None
            if topic_id:
                from modules.vector_store.base import SearchFilter
                filters = SearchFilter(metadata_filters={"topic_id": topic_id})
                logger.info(f"ğŸ¯ åº”ç”¨è¿‡æ»¤æ¡ä»¶: topic_id={topic_id}")
            else:
                logger.info("ğŸŒ æ— è¿‡æ»¤æ¡ä»¶ï¼Œæœç´¢æ‰€æœ‰æ‘˜è¦æ–‡æ¡£")
            
            # æ‘˜è¦å‘é‡æœç´¢
            logger.debug(f"ğŸ” æ‰§è¡Œæ‘˜è¦å‘é‡ç›¸ä¼¼åº¦æœç´¢...")
            search_results = await self._vector_store.search_summaries(
                query_vector=query_embedding,
                top_k=max_results,
                score_threshold=score_threshold,
                filters=filters
            )
            
            # è®°å½•åŸå§‹æœç´¢ç»“æœ
            logger.info(f"ğŸ“Š æ‘˜è¦æœç´¢è¿”å› {len(search_results)} ä¸ªåŸå§‹ç»“æœ")
            
            # è½¬æ¢ä¸ºRetrievedContext
            contexts = []
            empty_content_filtered = 0
            
            for i, result in enumerate(search_results, 1):
                doc = result.document
                doc_metadata = doc.metadata or {}
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ‘˜è¦æ–‡æ¡£
                if not doc_metadata.get('summary_document', False):
                    continue
                
                # è¿‡æ»¤ç©ºå†…å®¹æ‘˜è¦
                if not doc.content or len(doc.content.strip()) < 20:
                    empty_content_filtered += 1
                    logger.debug(f"âŒ æ‘˜è¦{i} è¢«è¿‡æ»¤ï¼šå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­")
                    continue
                
                # åˆ›å»ºæ‘˜è¦ä¸Šä¸‹æ–‡
                context = RetrievedContext(
                    content=doc.content,
                    source="summary",
                    score=result.score,
                    document_id=doc_metadata.get('document_ids', [None])[0] if doc_metadata.get('document_ids') else doc.id,
                    chunk_index=0,  # æ‘˜è¦æ²¡æœ‰chunkæ¦‚å¿µ
                    metadata={
                        "type": "summary",
                        "scope_level": doc_metadata.get('scope_level', 'document'),
                        "key_topics": doc_metadata.get('key_topics', []),
                        "source_documents": doc_metadata.get('document_ids', []),
                        "original_score": result.score,
                        "rank": result.rank
                    }
                )
                contexts.append(context)
                
                logger.debug(f"âœ… æ‘˜è¦{i}: score={result.score:.3f}, "
                           f"scope={doc_metadata.get('scope_level')}, "
                           f"topics={len(doc_metadata.get('key_topics', []))}, "
                           f"content_len={len(doc.content)}")
            
            # è®°å½•è¿‡æ»¤ç»Ÿè®¡
            if empty_content_filtered > 0:
                logger.info(f"ğŸš® è¿‡æ»¤ç©ºæ‘˜è¦: {empty_content_filtered}ä¸ª")
            
            final_count = len(contexts)
            logger.info(f"ğŸ“‹ æœ€ç»ˆæ‘˜è¦ä¸Šä¸‹æ–‡: {final_count}ä¸ª")
            
            return contexts, final_count
            
        except Exception as e:
            logger.error(f"âŒ æ‘˜è¦æ£€ç´¢å¤±è´¥: {e}")
            return [], 0
    
    def _build_summary_prompt(
        self,
        query: str,
        contexts: List[RetrievedContext],
        style: str = "summary"
    ) -> str:
        """æ„å»ºæ‘˜è¦é£æ ¼çš„æç¤º"""
        if not contexts:
            return f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{query}
            
æ³¨æ„ï¼šå½“å‰æ²¡æœ‰ç›¸å…³çš„æ–‡æ¡£æ‘˜è¦å¯ä¾›å‚è€ƒï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"""
        
        # æ„å»ºä¸Šä¸‹æ–‡éƒ¨åˆ†
        context_text = ""
        for i, context in enumerate(contexts, 1):
            metadata = context.metadata or {}
            scope_level = metadata.get('scope_level', 'document')
            key_topics = metadata.get('key_topics', [])
            
            topics_text = f" (å…³é”®ä¸»é¢˜: {', '.join(key_topics)})" if key_topics else ""
            
            context_text += f"""
=== æ‘˜è¦ {i} ({scope_level} çº§åˆ«{topics_text}) ===
{context.content}
ç›¸å…³æ€§å¾—åˆ†: {context.score:.3f}
"""
        
        # æ ¹æ®é£æ ¼è°ƒæ•´æç¤º
        if style == "summary":
            style_instruction = """è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£æ‘˜è¦ï¼Œä»é«˜å±‚æ¬¡è§’åº¦å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. ä¸»è¦æ¦‚å¿µå’Œæ ¸å¿ƒè§‚ç‚¹
2. æ•´ä½“è¶‹åŠ¿å’Œæ¨¡å¼
3. å…³é”®è¦ç‚¹çš„ç»¼åˆåˆ†æ
4. é¿å…è¿‡å¤šå…·ä½“ç»†èŠ‚

è¯·ç”¨æ¸…æ™°ã€ç»“æ„åŒ–çš„æ–¹å¼ç»„ç»‡å›ç­”ã€‚"""
        else:
            style_instruction = "è¯·åŸºäºä»¥ä¸Šæ‘˜è¦ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦åŸºäºæä¾›çš„æ–‡æ¡£æ‘˜è¦å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

=== ç›¸å…³æ–‡æ¡£æ‘˜è¦ ===
{context_text}

=== ç”¨æˆ·é—®é¢˜ ===
{query}

=== å›ç­”æŒ‡å¯¼ ===
{style_instruction}"""
    
    async def _retrieve_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        search_type: SearchType = SearchType.SEMANTIC,
        max_results: int = 5,
        score_threshold: float = 0.0
    ) -> Tuple[List[RetrievedContext], int]:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        
        start_time = datetime.now(timezone.utc)
        
        # è®°å½•æ£€ç´¢å‚æ•°
        logger.info(f"ğŸ” å¼€å§‹RAGæ£€ç´¢ - æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}', "
                   f"topic_id: {topic_id}, max_results: {max_results}, "
                   f"score_threshold: {score_threshold}, search_type: {search_type}")
        
        if not self._vector_store or not self._embedding_service:
            logger.warning("âš ï¸ å‘é‡å­˜å‚¨æˆ–åµŒå…¥æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ£€ç´¢")
            return [], 0
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            logger.debug("ğŸ§® ç”ŸæˆæŸ¥è¯¢å‘é‡åµŒå…¥...")
            query_embedding = await self._embedding_service.generate_embedding(query)
            logger.debug(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filters = None
            if topic_id:
                from modules.vector_store.base import SearchFilter
                filters = SearchFilter(metadata_filters={"topic_id": topic_id})
                logger.info(f"ğŸ¯ åº”ç”¨è¿‡æ»¤æ¡ä»¶: topic_id={topic_id}")
            else:
                logger.info("ğŸŒ æ— è¿‡æ»¤æ¡ä»¶ï¼Œæœç´¢æ‰€æœ‰æ–‡æ¡£")
            
            # å‘é‡æœç´¢
            logger.debug(f"ğŸ” æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢...")
            search_results = await self._vector_store.search_similar(
                query_vector=query_embedding,
                limit=max_results,
                score_threshold=score_threshold,
                filters=filters
            )
            
            # è®°å½•åŸå§‹æœç´¢ç»“æœ
            logger.info(f"ğŸ“Š å‘é‡æœç´¢è¿”å› {len(search_results)} ä¸ªåŸå§‹ç»“æœ")
            
            # è½¬æ¢ä¸ºRetrievedContextå¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
            contexts = []
            empty_content_filtered = 0
            
            for i, result in enumerate(search_results, 1):
                # ä»SearchResult.documentä¸­è·å–ä¿¡æ¯
                doc = result.document
                doc_metadata = doc.metadata or {}
                result_metadata = result.metadata or {}
                
                # è¿‡æ»¤ç©ºå†…å®¹æ–‡æ¡£
                if not doc.content or len(doc.content.strip()) < 10:
                    empty_content_filtered += 1
                    logger.debug(f"âŒ æ–‡æ¡£{i} è¢«è¿‡æ»¤ï¼šå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ (é•¿åº¦={len(doc.content) if doc.content else 0})")
                    continue
                
                # è·å–æ–‡æ¡£ä¿¡æ¯ç”¨äºæ—¥å¿—
                doc_title = result_metadata.get("document_title", "") or doc_metadata.get("title", "")
                file_id = result_metadata.get("file_id", "") or doc_metadata.get("file_id", "")
                chunk_index = doc_metadata.get("chunk_index", 0)
                content_preview = doc.content[:100] + "..." if len(doc.content) > 100 else doc.content
                
                # è®°å½•æ¯ä¸ªå¬å›æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
                logger.info(f"ğŸ“„ æ–‡æ¡£{i}: ID={doc.id}, æ ‡é¢˜='{doc_title}', "
                           f"æ–‡ä»¶ID={file_id}, å—ç´¢å¼•={chunk_index}, "
                           f"ç›¸ä¼¼åº¦={result.score:.4f}, å†…å®¹é¢„è§ˆ='{content_preview}'")
                
                contexts.append(RetrievedContext(
                    content=doc.content,
                    document_id=doc.id or "",
                    chunk_index=chunk_index,
                    similarity_score=result.score,
                    document_title=doc_title,
                    file_id=file_id,
                    metadata={**doc_metadata, **result_metadata}
                ))
            
            # è®°å½•ç©ºå†…å®¹è¿‡æ»¤ç»Ÿè®¡
            if empty_content_filtered > 0:
                logger.info(f"ğŸš« ç©ºå†…å®¹è¿‡æ»¤ç§»é™¤äº† {empty_content_filtered} ä¸ªæ–‡æ¡£")
            
            search_time_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
            
            # è¯¦ç»†çš„ç»“æœæ‘˜è¦
            if contexts:
                avg_score = sum(ctx.similarity_score for ctx in contexts) / len(contexts)
                best_score = max(ctx.similarity_score for ctx in contexts)
                logger.info(f"âœ… RAGæ£€ç´¢å®Œæˆ: å¬å›{len(contexts)}ä¸ªæ–‡æ¡£, è€—æ—¶{search_time_ms}ms, "
                           f"å¹³å‡ç›¸ä¼¼åº¦={avg_score:.4f}, æœ€é«˜ç›¸ä¼¼åº¦={best_score:.4f}")
            else:
                logger.warning(f"âš ï¸ RAGæ£€ç´¢å®Œæˆä½†æ— ç»“æœ: è€—æ—¶{search_time_ms}ms")
                # åˆ†æå¯èƒ½çš„åŸå› 
                if topic_id:
                    logger.warning(f"ğŸ’¡ å¯èƒ½åŸå› : 1) topic_id={topic_id}æ²¡æœ‰å¯¹åº”æ–‡æ¡£ "
                                 f"2) ç›¸ä¼¼åº¦é˜ˆå€¼{score_threshold}è¿‡é«˜ 3) æŸ¥è¯¢ä¸æ–‡æ¡£å†…å®¹å·®å¼‚è¾ƒå¤§")
                else:
                    logger.warning(f"ğŸ’¡ å¯èƒ½åŸå› : 1) å‘é‡æ•°æ®åº“ä¸ºç©º "
                                 f"2) ç›¸ä¼¼åº¦é˜ˆå€¼{score_threshold}è¿‡é«˜ 3) æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£å†…å®¹å·®å¼‚è¾ƒå¤§")
            
            return contexts, search_time_ms
            
        except Exception as e:
            logger.error(f"âŒ RAGæ£€ç´¢å¤±è´¥: {e}")
            return [], 0
    
    def _build_prompt(
        self,
        user_message: str,
        retrieved_contexts: List[RetrievedContext],
        conversation_history: List[ChatMessage] = None
    ) -> str:
        """æ„å»ºAIæç¤ºè¯"""
        
        prompt_parts = []
        
        # æ ¹æ®æ˜¯å¦æœ‰æ£€ç´¢ç»“æœè°ƒæ•´ç³»ç»Ÿæç¤º
        if retrieved_contexts:
            # æœ‰æ£€ç´¢ç»“æœæ—¶çš„ç³»ç»Ÿæç¤º
            prompt_parts.append("""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. ä¼˜å…ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¡¥å……
3. æ˜ç¡®æŒ‡å‡ºå“ªäº›ä¿¡æ¯æ¥è‡ªæ–‡æ¡£ï¼Œå“ªäº›æ˜¯ä½ çš„è¡¥å……
4. ä¿æŒå›ç­”å‡†ç¡®ã€æœ‰ç”¨ã€å‹å¥½""")
            
            # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            prompt_parts.append("\n\n=== ç›¸å…³æ–‡æ¡£å†…å®¹ ===")
            for i, context in enumerate(retrieved_contexts, 1):
                doc_title = context.document_title or f"æ–‡æ¡£{i}"
                prompt_parts.append(f"\nã€æ–‡æ¡£{i}: {doc_title}ã€‘")
                prompt_parts.append(f"å†…å®¹: {context.content}")
                prompt_parts.append(f"ç›¸ä¼¼åº¦: {context.similarity_score:.3f}")
        else:
            # æ— æ£€ç´¢ç»“æœæ—¶çš„ç³»ç»Ÿæç¤º
            prompt_parts.append("""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è™½ç„¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ï¼Œä½†è¯·åŸºäºä½ çš„çŸ¥è¯†ä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„å›ç­”ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. æ ¹æ®é—®é¢˜çš„ç±»å‹å’Œé¢†åŸŸï¼Œæä¾›å‡†ç¡®ã€å®ç”¨çš„ä¿¡æ¯
2. æ‰¿è®¤æ²¡æœ‰ç‰¹å®šçš„æ–‡æ¡£æ”¯æŒï¼Œä½†ä»ç„¶å°½åŠ›å¸®åŠ©ç”¨æˆ·
3. å¦‚æœé—®é¢˜æ¶‰åŠä¸“ä¸šé¢†åŸŸï¼Œå»ºè®®ç”¨æˆ·æŸ¥æ‰¾æ›´æƒå¨çš„èµ„æ–™
4. ä¿æŒå›ç­”å‹å¥½ã€æœ‰å»ºè®¾æ€§ï¼Œé¿å…ç®€å•åœ°è¯´"æˆ‘ä¸çŸ¥é“"
5. å¯ä»¥æä¾›ç›¸å…³çš„æ¦‚å¿µã€æ–¹æ³•ã€å»ºè®®æˆ–å­¦ä¹ æ–¹å‘""")
            
            prompt_parts.append(f"\n\nğŸ’¡ æç¤ºï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ï¼Œä»¥ä¸‹å›ç­”åŸºäºAIçš„ä¸€èˆ¬çŸ¥è¯†ã€‚")
        
        # å¯¹è¯å†å²
        if conversation_history:
            prompt_parts.append("\n\n=== å¯¹è¯å†å² ===")
            for msg in conversation_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
                role_name = "ç”¨æˆ·" if msg.role == MessageRole.USER else "åŠ©æ‰‹"
                prompt_parts.append(f"\n{role_name}: {msg.content}")
        
        # å½“å‰ç”¨æˆ·é—®é¢˜
        prompt_parts.append(f"\n\n=== å½“å‰é—®é¢˜ ===\nç”¨æˆ·: {user_message}")
        
        if retrieved_contexts:
            prompt_parts.append("\n\nè¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜:")
        else:
            prompt_parts.append("\n\nè¯·åŸºäºä½ çš„çŸ¥è¯†ä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„å›ç­”:")
        
        return "".join(prompt_parts)
    
    async def _generate_ai_response(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Tuple[str, int]:
        """ç”ŸæˆAIå›ç­”"""
        
        if not self.ai_client:
            raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
        
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹å’Œå‚æ•°
            chat_model = getattr(self.config.ai.chat.openai, 'chat_model', 'gpt-3.5-turbo')
            
            response = await self.ai_client.chat.completions.create(
                model=chat_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            return content, tokens_used
            
        except Exception as e:
            logger.error(f"âŒ AIç”Ÿæˆå¤±è´¥: {e}")
            error_response = f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"
            return error_response, 0
    
    async def _generate_ai_response_stream(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”ŸæˆAIå›ç­”"""
        
        if not self.ai_client:
            yield {
                "content": "âŒ OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®",
                "tokens": 0
            }
            return
        
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹å’Œå‚æ•°
            chat_model = getattr(self.config.ai.chat.openai, 'chat_model', 'gpt-3.5-turbo')
            
            stream = await self.ai_client.chat.completions.create(
                model=chat_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield {
                        "content": chunk.choices[0].delta.content,
                        "tokens": 1  # ç®€åŒ–tokenè®¡ç®—
                    }
                    
        except Exception as e:
            logger.error(f"âŒ æµå¼AIç”Ÿæˆå¤±è´¥: {e}")
            yield {
                "content": f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}",
                "tokens": 0
            }
    
    async def get_conversation_messages(
        self,
        conversation_id: str,
        limit: int = 50,
        before: Optional[str] = None
    ) -> List[ChatMessage]:
        """è·å–å¯¹è¯æ¶ˆæ¯å†å²"""
        return await self.es_service.get_conversation_messages(
            conversation_id, limit, before
        )
    
    async def search_chat_content(
        self,
        query: str,
        topic_id: Optional[int] = None,
        limit: int = 20
    ) -> List[ChatSearchResult]:
        """æœç´¢èŠå¤©å†…å®¹"""
        return await self.es_service.search_chat_content(
            query, topic_id, limit
        )
    
    async def get_conversations_list(
        self,
        topic_id: Optional[int] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[ConversationSummary]:
        """è·å–å¯¹è¯åˆ—è¡¨"""
        # è¿™ä¸ªæ–¹æ³•éœ€è¦åŸºäºESèšåˆæŸ¥è¯¢å®ç°
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œåç»­å®Œå–„
        logger.warning("get_conversations_listæ–¹æ³•å¾…å®ç°")
        return []
    
    async def delete_conversation(self, conversation_id: str) -> bool:
        """åˆ é™¤å¯¹è¯"""
        # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨ESä¸­åˆ é™¤å¯¹åº”çš„æ–‡æ¡£
        # æš‚æ—¶è¿”å›Trueï¼Œåç»­å®Œå–„
        logger.warning("delete_conversationæ–¹æ³•å¾…å®ç°")
        return True
    
    async def get_chat_statistics(
        self,
        topic_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """è·å–èŠå¤©ç»Ÿè®¡"""
        return await self.es_service.get_conversation_statistics(topic_id)
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        await self.es_service.close()
        if self._vector_store:
            await self._vector_store.cleanup()
        if self._embedding_service:
            await self._embedding_service.cleanup()


# ä¾¿æ·å‡½æ•°
def get_chat_service() -> ChatService:
    """è·å–ChatServiceå®ä¾‹"""
    return ChatService()
