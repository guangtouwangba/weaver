"""
Chat Service

æä¾›èŠå¤©åŠŸèƒ½çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼Œé›†æˆRAGæ£€ç´¢å’ŒAIç”Ÿæˆã€‚
"""

import asyncio
import json
import uuid
from datetime import datetime, timezone
from typing import AsyncGenerator, Dict, List, Optional, Any, Tuple

try:
    import openai
except ImportError:
    openai = None

from sqlalchemy.ext.asyncio import AsyncSession

from config.settings import get_config
from logging_system import get_logger
from modules.services.base_service import BaseService
from modules.services.elasticsearch_service import elasticsearch_chat_service
from modules.schemas.chat import (
    ChatRequest,
    ChatResponse,
    ChatMessage,
    RetrievedContext,
    AIMetadata,
    MessageRole,
    ConversationSummary,
    ChatSearchResult,
    SearchType
)
from modules.vector_store.weaviate_service import WeaviateVectorStore
from modules.embedding.openai_service import OpenAIEmbeddingService

logger = get_logger(__name__)


class ChatService(BaseService):
    """èŠå¤©æœåŠ¡"""
    
    def __init__(self, session: AsyncSession = None, ai_client=None):
        if session:
            super().__init__(session)
        else:
            self.session = None
            self.logger = logger
        
        self.config = get_config()
        self.ai_client = ai_client
        self.es_service = elasticsearch_chat_service
        
        # RAGç»„ä»¶
        self._vector_store: Optional[WeaviateVectorStore] = None
        self._embedding_service: Optional[OpenAIEmbeddingService] = None
        
        # åˆå§‹åŒ–æ ‡å¿—
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡ç»„ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if self._initialized:
            return
            
        # ä½¿ç”¨é”ç¡®ä¿åˆå§‹åŒ–åªæ‰§è¡Œä¸€æ¬¡
        if not hasattr(self, '_init_lock'):
            self._init_lock = asyncio.Lock()
            
        async with self._init_lock:
            if self._initialized:
                return
                
            try:
                # åˆå§‹åŒ–Elasticsearch
                await self.es_service.initialize()
                
                # åˆå§‹åŒ–å‘é‡å­˜å‚¨
                weaviate_url = getattr(self.config, 'weaviate_url', 'http://localhost:8080')
                self._vector_store = WeaviateVectorStore(url=weaviate_url)
                await self._vector_store.initialize()
                
                # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
                if hasattr(self.config, 'ai') and hasattr(self.config.ai, 'embedding'):
                    api_key = getattr(self.config.ai.embedding.openai, 'api_key', None)
                    if api_key:
                        self._embedding_service = OpenAIEmbeddingService(api_key=api_key)
                        await self._embedding_service.initialize()
                        
                # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
                if not self.ai_client and openai and hasattr(self.config, 'ai'):
                    # ä½¿ç”¨æ–°çš„é…ç½®ç®¡ç†
                    chat_config = ChatConfiguration.from_config(self.config)
                    chat_config.validate()
                    
                    self.ai_client = openai.AsyncOpenAI(
                        api_key=chat_config.api_key,
                        organization=chat_config.organization,
                        base_url=chat_config.base_url,
                        timeout=chat_config.timeout,
                        max_retries=chat_config.max_retries
                    )
                    logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                
                self._initialized = True
                logger.info("âœ… ChatServiceåˆå§‹åŒ–å®Œæˆ")
                
            except ValueError as e:
                logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
                raise Exception(f"ChatServiceé…ç½®é”™è¯¯: {str(e)}") from e
            except Exception as e:
                logger.error(f"âŒ ChatServiceåˆå§‹åŒ–å¤±è´¥: {e}")
                # æ¸…ç†å·²åˆå§‹åŒ–çš„èµ„æº
                await self._cleanup_partial_initialization()
                raise
    
    async def _cleanup_partial_initialization(self):
        """æ¸…ç†éƒ¨åˆ†åˆå§‹åŒ–çš„èµ„æº"""
        try:
            if hasattr(self, '_vector_store') and self._vector_store:
                await self._vector_store.cleanup()
            if hasattr(self, '_embedding_service') and self._embedding_service:
                await self._embedding_service.cleanup()
        except Exception as e:
            logger.error(f"âš ï¸ æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def _get_services(self):
        """è·å–æœåŠ¡å®ä¾‹"""
        # åˆ›å»ºé…ç½®
        chat_config = ChatConfiguration.from_config(self.config)
        
        # åˆ›å»ºæ£€ç´¢æœåŠ¡
        retrieval_service = WeaviateContextRetrievalService(
            self._vector_store, 
            self._embedding_service
        )
        
        # åˆ›å»ºæç¤ºæ„å»ºå™¨
        prompt_builder = PromptBuilder()
        
        # åˆ›å»ºAIç”ŸæˆæœåŠ¡
        ai_service = AIGenerationService(self.ai_client, chat_config)
        
        # åˆ›å»ºå¯¹è¯ç®¡ç†å™¨
        conversation_manager = ConversationManager(self.es_service)
        
        return retrieval_service, prompt_builder, ai_service, conversation_manager
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """å¤„ç†èŠå¤©è¯·æ±‚ - é‡æ„åçš„èŒè´£åˆ†ç¦»ç‰ˆæœ¬"""
        await self.initialize()
        
        try:
            # è·å–æœåŠ¡å®ä¾‹
            retrieval_service, prompt_builder, ai_service, conversation_manager = self._get_services()
            
            # 1. ç”Ÿæˆå¯¹è¯ID
            conversation_id = request.conversation_id or str(uuid.uuid4())
            message_id = str(uuid.uuid4())
            
            # 2. æ£€ç´¢ä¸Šä¸‹æ–‡
            retrieved_contexts, search_time_ms = await retrieval_service.retrieve_contexts(
                query=request.message,
                topic_id=request.topic_id,
                search_type=request.search_type,
                max_results=request.max_results,
                score_threshold=request.score_threshold
            )
            
            # 3. è·å–å¯¹è¯å†å²
            conversation_history = []
            if request.context_window > 0:
                conversation_history = await conversation_manager.get_conversation_history(
                    conversation_id, limit=request.context_window * 2
                )
            
            # 4. æ„å»ºæç¤ºè¯
            prompt = prompt_builder.build_chat_prompt(
                user_message=request.message,
                retrieved_contexts=retrieved_contexts if request.include_context else [],
                conversation_history=conversation_history
            )
            
            # 5. ç”ŸæˆAIå›ç­”
            generation_start = datetime.now(timezone.utc)
            ai_response, tokens_used = await ai_service.generate_response(
                prompt=prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            generation_time_ms = int((datetime.now(timezone.utc) - generation_start).total_seconds() * 1000)
            
            # 6. ä¿å­˜å¯¹è¯
            ai_metadata = {
                "model": ai_service.config.chat_model,
                "tokens_used": tokens_used,
                "generation_time_ms": generation_time_ms,
                "search_time_ms": search_time_ms,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens
            }
            
            await conversation_manager.save_conversation(
                conversation_id=conversation_id,
                user_message=request.message,
                assistant_message=ai_response,
                topic_id=request.topic_id,
                retrieved_contexts=retrieved_contexts,
                ai_metadata=ai_metadata
            )
            
            # 7. æ„å»ºå“åº”
            return ChatResponse(
                message_id=message_id,
                conversation_id=conversation_id,
                content=ai_response,
                retrieved_contexts=retrieved_contexts,
                ai_metadata=AIMetadata(
                    model=ai_service.config.chat_model,
                    tokens_used=tokens_used,
                    generation_time_ms=generation_time_ms,
                    search_time_ms=search_time_ms,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ),
                timestamp=datetime.now(timezone.utc)
            )
            
        except ValueError as e:
            logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
            raise Exception(f"èŠå¤©æœåŠ¡é…ç½®é”™è¯¯: {str(e)}") from e
        except Exception as e:
            logger.error(f"âŒ èŠå¤©å¤„ç†å¤±è´¥: {e}")
            raise Exception(f"èŠå¤©å¤„ç†å¤±è´¥: {str(e)}") from e
    
    async def chat_with_summary(self, request: Dict[str, Any]) -> ChatResponse:
        """åŸºäºæ‘˜è¦ç´¢å¼•çš„èŠå¤©åŠŸèƒ½ - é‡æ„ç‰ˆæœ¬"""
        await self.initialize()
        
        try:
            # è·å–æœåŠ¡å®ä¾‹
            retrieval_service, prompt_builder, ai_service, conversation_manager = self._get_services()
            
            query = request.get("query", "")
            topic_id = request.get("topic_id")
            max_results = request.get("max_results", 5)
            score_threshold = request.get("score_threshold", 0.75)
            enhanced_query = request.get("enhanced_query", query)
            
            logger.info(f"ğŸ” å¼€å§‹æ‘˜è¦èŠå¤© - æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}', topic_id: {topic_id}")
            
            # æ£€ç´¢æ‘˜è¦ä¸Šä¸‹æ–‡
            summary_contexts, context_count = await retrieval_service.retrieve_summary_contexts(
                query=query,
                topic_id=topic_id,
                max_results=max_results,
                score_threshold=score_threshold
            )
            
            # å¦‚æœæ‘˜è¦ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢
            if context_count < 2:
                logger.info("ğŸ”„ æ‘˜è¦ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢")
                contexts, context_count = await retrieval_service.retrieve_contexts(
                    query=query,
                    topic_id=topic_id,
                    max_results=max_results,
                    score_threshold=score_threshold
                )
            else:
                contexts = summary_contexts
            
            # æ„å»ºæ‘˜è¦é£æ ¼çš„æç¤º
            prompt = prompt_builder.build_summary_prompt(
                query=enhanced_query or query,
                contexts=contexts,
                style=request.get("response_style", "summary")
            )
            
            # ç”ŸæˆAIå“åº”
            if request.get("stream", False):
                response_content = await ai_service.generate_response_stream(prompt)
                tokens_used = 0  # æµå¼æ¨¡å¼æ— æ³•å‡†ç¡®è®¡ç®—
            else:
                response_content, tokens_used = await ai_service.generate_response(prompt)
            
            # æ„å»ºå“åº”
            response = ChatResponse(
                content=response_content,
                retrieved_context=contexts,
                ai_metadata=AIMetadata(
                    model=ai_service.config.chat_model,
                    search_type="summary",
                    context_count=context_count,
                    processing_time=0.0,
                    tokens_used=tokens_used
                ),
                timestamp=datetime.now(timezone.utc)
            )
            
            logger.info(f"âœ… æ‘˜è¦èŠå¤©å®Œæˆ - ä¸Šä¸‹æ–‡: {context_count}ä¸ª")
            return response
            
        except Exception as e:
            logger.error(f"âŒ æ‘˜è¦èŠå¤©å¤„ç†å¤±è´¥: {e}")
            raise
    

    

    
    async def retrieve_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        search_type: SearchType = SearchType.SEMANTIC,
        max_results: int = 5,
        score_threshold: float = 0.0,
        collection_name: str = "documents"
    ) -> List[Dict[str, Any]]:
        """
        å…¬å…±æ£€ç´¢ä¸Šä¸‹æ–‡æ–¹æ³•ï¼Œä¾›Handlerä½¿ç”¨ - é‡æ„ç‰ˆæœ¬
        
        Returns:
            List[Dict[str, Any]]: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆç®€åŒ–æ ¼å¼ï¼‰
        """
        await self.initialize()
        
        # è·å–æ£€ç´¢æœåŠ¡
        retrieval_service = WeaviateContextRetrievalService(
            self._vector_store, 
            self._embedding_service
        )
        
        contexts, search_time = await retrieval_service.retrieve_contexts(
            query=query,
            topic_id=topic_id,
            search_type=search_type,
            max_results=max_results,
            score_threshold=score_threshold,
            collection_name=collection_name
        )
        
        # è½¬æ¢ä¸ºç®€åŒ–çš„å­—å…¸æ ¼å¼ä¾›Handlerä½¿ç”¨
        result = []
        for ctx in contexts:
            result.append({
                "content": ctx.content,
                "document_id": getattr(ctx, 'document_id', ''),
                "chunk_index": getattr(ctx, 'chunk_index', 0),
                "similarity_score": getattr(ctx, 'similarity_score', getattr(ctx, 'score', 0)),
                "document_title": getattr(ctx, 'document_title', ''),
                "file_id": getattr(ctx, 'file_id', ''),
                "metadata": getattr(ctx, 'metadata', {})
            })
        
        return result


    

    

    
    async def get_conversation_messages(
        self,
        conversation_id: str,
        limit: int = 50,
        before: Optional[str] = None
    ) -> List[ChatMessage]:
        """è·å–å¯¹è¯æ¶ˆæ¯å†å² - é‡æ„ç‰ˆæœ¬"""
        await self.initialize()
        conversation_manager = ConversationManager(self.es_service)
        return await conversation_manager.get_conversation_history(conversation_id, limit)
    
    async def search_chat_content(
        self,
        query: str,
        topic_id: Optional[int] = None,
        limit: int = 20
    ) -> List[ChatSearchResult]:
        """æœç´¢èŠå¤©å†…å®¹"""
        return await self.es_service.search_chat_content(
            query, topic_id, limit
        )
    
    async def get_conversations_list(
        self,
        topic_id: Optional[int] = None,
        limit: int = 20,
        offset: int = 0
    ) -> List[ConversationSummary]:
        """è·å–å¯¹è¯åˆ—è¡¨"""
        # è¿™ä¸ªæ–¹æ³•éœ€è¦åŸºäºESèšåˆæŸ¥è¯¢å®ç°
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œåç»­å®Œå–„
        logger.warning("get_conversations_listæ–¹æ³•å¾…å®ç°")
        return []
    
    async def delete_conversation(self, conversation_id: str) -> bool:
        """åˆ é™¤å¯¹è¯"""
        # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨ESä¸­åˆ é™¤å¯¹åº”çš„æ–‡æ¡£
        # æš‚æ—¶è¿”å›Trueï¼Œåç»­å®Œå–„
        logger.warning("delete_conversationæ–¹æ³•å¾…å®ç°")
        return True
    
    async def get_chat_statistics(
        self,
        topic_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """è·å–èŠå¤©ç»Ÿè®¡"""
        return await self.es_service.get_conversation_statistics(topic_id)
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        await self.es_service.close()
        if self._vector_store:
            await self._vector_store.cleanup()
        if self._embedding_service:
            await self._embedding_service.cleanup()
    


from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class ChatConfiguration:
    """èŠå¤©æœåŠ¡é…ç½®ç®¡ç†"""
    chat_model: str
    api_key: str
    organization: Optional[str] = None
    base_url: Optional[str] = None
    timeout: int = 60
    max_retries: int = 3
    
    @classmethod
    def from_config(cls, config) -> 'ChatConfiguration':
        """ä»å…¨å±€é…ç½®åˆ›å»ºèŠå¤©é…ç½®"""
        try:
            ai_config = config.ai.chat.openai
            api_key = getattr(ai_config, 'api_key', None)
            if not api_key:
                raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥ AI__CHAT__OPENAI__API_KEY ç¯å¢ƒå˜é‡")
            
            return cls(
                chat_model=getattr(ai_config, 'chat_model', 'gpt-3.5-turbo'),
                api_key=api_key,
                organization=getattr(ai_config, 'organization', None),
                base_url=getattr(ai_config, 'api_base', None),
                timeout=getattr(ai_config, 'timeout', 60),
                max_retries=getattr(ai_config, 'max_retries', 3)
            )
        except AttributeError as e:
            raise ValueError(f"é…ç½®ç»“æ„é”™è¯¯: {e}ï¼Œè¯·æ£€æŸ¥AIé…ç½®") from e
    
    def validate(self) -> None:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        if not self.api_key:
            raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        if not self.chat_model:
            raise ValueError("èŠå¤©æ¨¡å‹ä¸èƒ½ä¸ºç©º")
        if self.timeout <= 0:
            raise ValueError("è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äº0")
        if self.max_retries < 0:
            raise ValueError("é‡è¯•æ¬¡æ•°ä¸èƒ½å°äº0")


class ContextRetrievalService(ABC):
    """ä¸Šä¸‹æ–‡æ£€ç´¢æœåŠ¡æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    async def retrieve_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        search_type: SearchType = SearchType.SEMANTIC,
        max_results: int = 5,
        score_threshold: float = 0.0,
        collection_name: str = "documents"
    ) -> Tuple[List[RetrievedContext], int]:
        """æ£€ç´¢æ–‡æ¡£ä¸Šä¸‹æ–‡"""
        pass
    
    @abstractmethod
    async def retrieve_summary_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        max_results: int = 5,
        score_threshold: float = 0.75
    ) -> Tuple[List[RetrievedContext], int]:
        """æ£€ç´¢æ‘˜è¦ä¸Šä¸‹æ–‡"""
        pass


class WeaviateContextRetrievalService(ContextRetrievalService):
    """åŸºäºWeaviateçš„ä¸Šä¸‹æ–‡æ£€ç´¢æœåŠ¡å®ç°"""
    
    def __init__(self, vector_store: WeaviateVectorStore, embedding_service: OpenAIEmbeddingService):
        self.vector_store = vector_store
        self.embedding_service = embedding_service
        self.logger = get_logger(f"{__name__}.{self.__class__.__name__}")
    
    async def retrieve_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        search_type: SearchType = SearchType.SEMANTIC,
        max_results: int = 5,
        score_threshold: float = 0.0,
        collection_name: str = "documents"
    ) -> Tuple[List[RetrievedContext], int]:
        """æ£€ç´¢æ–‡æ¡£ä¸Šä¸‹æ–‡"""
        return await self._retrieve_contexts_generic(
            query=query,
            topic_id=topic_id,
            max_results=max_results,
            score_threshold=score_threshold,
            collection_name=collection_name,
            search_type="documents"
        )
    
    async def retrieve_summary_contexts(
        self,
        query: str,
        topic_id: Optional[int] = None,
        max_results: int = 5,
        score_threshold: float = 0.75
    ) -> Tuple[List[RetrievedContext], int]:
        """æ£€ç´¢æ‘˜è¦ä¸Šä¸‹æ–‡"""
        return await self._retrieve_contexts_generic(
            query=query,
            topic_id=topic_id,
            max_results=max_results,
            score_threshold=score_threshold,
            collection_name="documents",  # ç»Ÿä¸€ä½¿ç”¨documentsé›†åˆ
            search_type="summaries"
        )
    
    async def _retrieve_contexts_generic(
        self,
        query: str,
        topic_id: Optional[int] = None,
        max_results: int = 5,
        score_threshold: float = 0.0,
        collection_name: str = "documents",
        search_type: str = "documents"
    ) -> Tuple[List[RetrievedContext], int]:
        """é€šç”¨æ£€ç´¢æ–¹æ³•ï¼Œç»Ÿä¸€å¤„ç†æ–‡æ¡£å’Œæ‘˜è¦æ£€ç´¢"""
        
        start_time = datetime.now(timezone.utc)
        
        # è®°å½•æ£€ç´¢å‚æ•°
        self.logger.info(f"ğŸ” å¼€å§‹{search_type}æ£€ç´¢ - æŸ¥è¯¢: '{query[:50]}{'...' if len(query) > 50 else ''}', "
                        f"topic_id: {topic_id}, max_results: {max_results}, "
                        f"score_threshold: {score_threshold}")
        
        if not self.vector_store or not self.embedding_service:
            self.logger.warning("âš ï¸ å‘é‡å­˜å‚¨æˆ–åµŒå…¥æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ£€ç´¢")
            return [], 0
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            self.logger.debug("ğŸ§® ç”ŸæˆæŸ¥è¯¢å‘é‡åµŒå…¥...")
            query_embedding = await self.embedding_service.generate_embedding(query)
            self.logger.debug(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filters = None
            if topic_id:
                from modules.vector_store.base import SearchFilter
                filters = SearchFilter(metadata_filters={"topic_id": topic_id})
                self.logger.info(f"ğŸ¯ åº”ç”¨è¿‡æ»¤æ¡ä»¶: topic_id={topic_id}")
            else:
                self.logger.info("ğŸŒ æ— è¿‡æ»¤æ¡ä»¶ï¼Œæœç´¢æ‰€æœ‰æ–‡æ¡£")
            
            # æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©ä¸åŒçš„æœç´¢æ–¹æ³•
            if search_type == "summaries":
                self.logger.debug(f"ğŸ” æ‰§è¡Œæ‘˜è¦å‘é‡ç›¸ä¼¼åº¦æœç´¢...")
                search_results = await self.vector_store.search_summaries(
                    query_vector=query_embedding,
                    top_k=max_results,
                    score_threshold=score_threshold,
                    filters=filters
                )
            else:
                self.logger.debug(f"ğŸ” æ‰§è¡Œæ–‡æ¡£å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œé›†åˆ: {collection_name}")
                search_results = await self.vector_store.search_similar(
                    query_vector=query_embedding,
                    limit=max_results,
                    score_threshold=score_threshold,
                    filters=filters,
                    collection_name=collection_name
                )
            
            # è®°å½•åŸå§‹æœç´¢ç»“æœ
            self.logger.info(f"ğŸ“Š å‘é‡æœç´¢è¿”å› {len(search_results)} ä¸ªåŸå§‹ç»“æœ")
            
            # è½¬æ¢ä¸ºRetrievedContextå¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
            contexts = []
            empty_content_filtered = 0
            
            for i, result in enumerate(search_results, 1):
                doc = result.document
                doc_metadata = doc.metadata or {}
                result_metadata = result.metadata or {}
                
                # å¯¹äºæ‘˜è¦æœç´¢ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯æ‘˜è¦æ–‡æ¡£
                if search_type == "summaries" and not doc_metadata.get('summary_document', False):
                    continue
                
                # è¿‡æ»¤ç©ºå†…å®¹æ–‡æ¡£
                min_content_length = 20 if search_type == "summaries" else 10
                if not doc.content or len(doc.content.strip()) < min_content_length:
                    empty_content_filtered += 1
                    self.logger.debug(f"âŒ æ–‡æ¡£{i} è¢«è¿‡æ»¤ï¼šå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ (é•¿åº¦={len(doc.content) if doc.content else 0})")
                    continue
                
                # è·å–æ–‡æ¡£ä¿¡æ¯ç”¨äºæ—¥å¿—
                doc_title = result_metadata.get("document_title", "") or doc_metadata.get("title", "")
                file_id = result_metadata.get("file_id", "") or doc_metadata.get("file_id", "")
                chunk_index = doc_metadata.get("chunk_index", 0)
                content_preview = doc.content[:100] + "..." if len(doc.content) > 100 else doc.content
                
                # è®°å½•æ¯ä¸ªå¬å›æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
                self.logger.info(f"ğŸ“„ æ–‡æ¡£{i}: ID={doc.id}, æ ‡é¢˜='{doc_title}', "
                               f"æ–‡ä»¶ID={file_id}, å—ç´¢å¼•={chunk_index}, "
                               f"ç›¸ä¼¼åº¦={result.score:.4f}, å†…å®¹é¢„è§ˆ='{content_preview}'")
                
                # ä¸ºæ‘˜è¦ä¸Šä¸‹æ–‡åˆ›å»ºç‰¹æ®Šçš„context
                if search_type == "summaries":
                    context = RetrievedContext(
                        content=doc.content,
                        source="summary",
                        score=result.score,
                        document_id=doc_metadata.get('document_ids', [None])[0] if doc_metadata.get('document_ids') else doc.id,
                        chunk_index=0,  # æ‘˜è¦æ²¡æœ‰chunkæ¦‚å¿µ
                        metadata={
                            "type": "summary",
                            "scope_level": doc_metadata.get('scope_level', 'document'),
                            "key_topics": doc_metadata.get('key_topics', []),
                            "source_documents": doc_metadata.get('document_ids', []),
                            "original_score": result.score,
                            "rank": result.rank if hasattr(result, 'rank') else i
                        }
                    )
                else:
                    context = RetrievedContext(
                        content=doc.content,
                        document_id=doc.id or "",
                        chunk_index=chunk_index,
                        similarity_score=result.score,
                        document_title=doc_title,
                        file_id=file_id,
                        metadata={**doc_metadata, **result_metadata}
                    )
                
                contexts.append(context)
            
            # è®°å½•ç©ºå†…å®¹è¿‡æ»¤ç»Ÿè®¡
            if empty_content_filtered > 0:
                self.logger.info(f"ğŸš« ç©ºå†…å®¹è¿‡æ»¤ç§»é™¤äº† {empty_content_filtered} ä¸ªæ–‡æ¡£")
            
            search_time_ms = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
            
            # è¯¦ç»†çš„ç»“æœæ‘˜è¦
            if contexts:
                avg_score = sum(getattr(ctx, 'similarity_score', getattr(ctx, 'score', 0)) for ctx in contexts) / len(contexts)
                best_score = max(getattr(ctx, 'similarity_score', getattr(ctx, 'score', 0)) for ctx in contexts)
                self.logger.info(f"âœ… {search_type}æ£€ç´¢å®Œæˆ: å¬å›{len(contexts)}ä¸ªæ–‡æ¡£, è€—æ—¶{search_time_ms}ms, "
                               f"å¹³å‡ç›¸ä¼¼åº¦={avg_score:.4f}, æœ€é«˜ç›¸ä¼¼åº¦={best_score:.4f}")
            else:
                self.logger.warning(f"âš ï¸ {search_type}æ£€ç´¢å®Œæˆä½†æ— ç»“æœ: è€—æ—¶{search_time_ms}ms")
                # åˆ†æå¯èƒ½çš„åŸå› 
                if topic_id:
                    self.logger.warning(f"ğŸ’¡ å¯èƒ½åŸå› : 1) topic_id={topic_id}æ²¡æœ‰å¯¹åº”æ–‡æ¡£ "
                                      f"2) ç›¸ä¼¼åº¦é˜ˆå€¼{score_threshold}è¿‡é«˜ 3) æŸ¥è¯¢ä¸æ–‡æ¡£å†…å®¹å·®å¼‚è¾ƒå¤§")
                else:
                    self.logger.warning(f"ğŸ’¡ å¯èƒ½åŸå› : 1) å‘é‡æ•°æ®åº“ä¸ºç©º "
                                      f"2) ç›¸ä¼¼åº¦é˜ˆå€¼{score_threshold}è¿‡é«˜ 3) æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£å†…å®¹å·®å¼‚è¾ƒå¤§")
            
            return contexts, search_time_ms
            
        except Exception as e:
            self.logger.error(f"âŒ {search_type}æ£€ç´¢å¤±è´¥: {e}")
            return [], 0


class PromptBuilder:
    """æç¤ºè¯æ„å»ºæœåŠ¡"""
    
    def build_chat_prompt(
        self,
        user_message: str,
        retrieved_contexts: List[RetrievedContext],
        conversation_history: List[ChatMessage] = None
    ) -> str:
        """æ„å»ºèŠå¤©æç¤ºè¯"""
        
        prompt_parts = []
        
        # æ ¹æ®æ˜¯å¦æœ‰æ£€ç´¢ç»“æœè°ƒæ•´ç³»ç»Ÿæç¤º
        if retrieved_contexts:
            # æœ‰æ£€ç´¢ç»“æœæ—¶çš„ç³»ç»Ÿæç¤º
            prompt_parts.append("""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. ä¼˜å…ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†è¡¥å……
3. æ˜ç¡®æŒ‡å‡ºå“ªäº›ä¿¡æ¯æ¥è‡ªæ–‡æ¡£ï¼Œå“ªäº›æ˜¯ä½ çš„è¡¥å……
4. ä¿æŒå›ç­”å‡†ç¡®ã€æœ‰ç”¨ã€å‹å¥½""")
            
            # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            prompt_parts.append("\n\n=== ç›¸å…³æ–‡æ¡£å†…å®¹ ===")
            for i, context in enumerate(retrieved_contexts, 1):
                doc_title = getattr(context, 'document_title', None) or f"æ–‡æ¡£{i}"
                prompt_parts.append(f"\nã€æ–‡æ¡£{i}: {doc_title}ã€‘")
                prompt_parts.append(f"å†…å®¹: {context.content}")
                
                # è·å–ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå…¼å®¹ä¸åŒçš„å­—æ®µåï¼‰
                score = getattr(context, 'similarity_score', getattr(context, 'score', 0))
                prompt_parts.append(f"ç›¸ä¼¼åº¦: {score:.3f}")
        else:
            # æ— æ£€ç´¢ç»“æœæ—¶çš„ç³»ç»Ÿæç¤º
            prompt_parts.append("""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è™½ç„¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ï¼Œä½†è¯·åŸºäºä½ çš„çŸ¥è¯†ä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„å›ç­”ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. æ ¹æ®é—®é¢˜çš„ç±»å‹å’Œé¢†åŸŸï¼Œæä¾›å‡†ç¡®ã€å®ç”¨çš„ä¿¡æ¯
2. æ‰¿è®¤æ²¡æœ‰ç‰¹å®šçš„æ–‡æ¡£æ”¯æŒï¼Œä½†ä»ç„¶å°½åŠ›å¸®åŠ©ç”¨æˆ·
3. å¦‚æœé—®é¢˜æ¶‰åŠä¸“ä¸šé¢†åŸŸï¼Œå»ºè®®ç”¨æˆ·æŸ¥æ‰¾æ›´æƒå¨çš„èµ„æ–™
4. ä¿æŒå›ç­”å‹å¥½ã€æœ‰å»ºè®¾æ€§ï¼Œé¿å…ç®€å•åœ°è¯´"æˆ‘ä¸çŸ¥é“"
5. å¯ä»¥æä¾›ç›¸å…³çš„æ¦‚å¿µã€æ–¹æ³•ã€å»ºè®®æˆ–å­¦ä¹ æ–¹å‘""")
            
            prompt_parts.append(f"\n\nğŸ’¡ æç¤ºï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ï¼Œä»¥ä¸‹å›ç­”åŸºäºAIçš„ä¸€èˆ¬çŸ¥è¯†ã€‚")
        
        # å¯¹è¯å†å²
        if conversation_history:
            prompt_parts.append("\n\n=== å¯¹è¯å†å² ===")
            for msg in conversation_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
                role_name = "ç”¨æˆ·" if msg.role == MessageRole.USER else "åŠ©æ‰‹"
                prompt_parts.append(f"\n{role_name}: {msg.content}")
        
        # å½“å‰ç”¨æˆ·é—®é¢˜
        prompt_parts.append(f"\n\n=== å½“å‰é—®é¢˜ ===\nç”¨æˆ·: {user_message}")
        
        if retrieved_contexts:
            prompt_parts.append("\n\nè¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜:")
        else:
            prompt_parts.append("\n\nè¯·åŸºäºä½ çš„çŸ¥è¯†ä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„å›ç­”:")
        
        return "".join(prompt_parts)
    
    def build_summary_prompt(
        self,
        query: str,
        contexts: List[RetrievedContext],
        style: str = "summary"
    ) -> str:
        """æ„å»ºæ‘˜è¦é£æ ¼çš„æç¤º"""
        if not contexts:
            return f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{query}
            
æ³¨æ„ï¼šå½“å‰æ²¡æœ‰ç›¸å…³çš„æ–‡æ¡£æ‘˜è¦å¯ä¾›å‚è€ƒï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"""
        
        # æ„å»ºä¸Šä¸‹æ–‡éƒ¨åˆ†
        context_text = ""
        for i, context in enumerate(contexts, 1):
            metadata = context.metadata or {}
            scope_level = metadata.get('scope_level', 'document')
            key_topics = metadata.get('key_topics', [])
            
            topics_text = f" (å…³é”®ä¸»é¢˜: {', '.join(key_topics)})" if key_topics else ""
            
            # è·å–ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå…¼å®¹ä¸åŒçš„å­—æ®µåï¼‰
            score = getattr(context, 'similarity_score', getattr(context, 'score', 0))
            
            context_text += f"""
=== æ‘˜è¦ {i} ({scope_level} çº§åˆ«{topics_text}) ===
{context.content}
ç›¸å…³æ€§å¾—åˆ†: {score:.3f}
"""
        
        # æ ¹æ®é£æ ¼è°ƒæ•´æç¤º
        if style == "summary":
            style_instruction = """è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£æ‘˜è¦ï¼Œä»é«˜å±‚æ¬¡è§’åº¦å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. ä¸»è¦æ¦‚å¿µå’Œæ ¸å¿ƒè§‚ç‚¹
2. æ•´ä½“è¶‹åŠ¿å’Œæ¨¡å¼
3. å…³é”®è¦ç‚¹çš„ç»¼åˆåˆ†æ
4. é¿å…è¿‡å¤šå…·ä½“ç»†èŠ‚

è¯·ç”¨æ¸…æ™°ã€ç»“æ„åŒ–çš„æ–¹å¼ç»„ç»‡å›ç­”ã€‚"""
        else:
            style_instruction = "è¯·åŸºäºä»¥ä¸Šæ‘˜è¦ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
        
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦åŸºäºæä¾›çš„æ–‡æ¡£æ‘˜è¦å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

=== ç›¸å…³æ–‡æ¡£æ‘˜è¦ ===
{context_text}

=== ç”¨æˆ·é—®é¢˜ ===
{query}

=== å›ç­”æŒ‡å¯¼ ===
{style_instruction}"""


class AIGenerationService:
    """AIç”ŸæˆæœåŠ¡"""
    
    def __init__(self, ai_client, config: ChatConfiguration):
        self.ai_client = ai_client
        self.config = config
        self.logger = get_logger(f"{__name__}.{self.__class__.__name__}")
    
    async def generate_response(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> Tuple[str, int]:
        """ç”ŸæˆAIå›ç­”"""
        
        if not self.ai_client:
            raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®")
        
        try:
            self.logger.debug(f"ğŸ¤– å¼€å§‹AIç”Ÿæˆ - æ¨¡å‹: {self.config.chat_model}, "
                            f"max_tokens: {max_tokens}, temperature: {temperature}")
            
            response = await self.ai_client.chat.completions.create(
                model=self.config.chat_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            self.logger.info(f"âœ… AIç”ŸæˆæˆåŠŸ - tokens: {tokens_used}, "
                           f"å†…å®¹é•¿åº¦: {len(content) if content else 0}")
            
            return content, tokens_used
            
        except Exception as e:
            self.logger.error(f"âŒ AIç”Ÿæˆå¤±è´¥: {e}")
            error_response = f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"
            return error_response, 0
    
    async def generate_response_stream(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”ŸæˆAIå›ç­”"""
        
        if not self.ai_client:
            yield {
                "content": "âŒ OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®",
                "tokens": 0
            }
            return
        
        try:
            self.logger.debug(f"ğŸŒŠ å¼€å§‹AIæµå¼ç”Ÿæˆ - æ¨¡å‹: {self.config.chat_model}")
            
            stream = await self.ai_client.chat.completions.create(
                model=self.config.chat_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True
            )
            
            total_tokens = 0
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    total_tokens += 1  # ç®€åŒ–tokenè®¡ç®—
                    yield {
                        "content": chunk.choices[0].delta.content,
                        "tokens": 1
                    }
            
            self.logger.info(f"âœ… AIæµå¼ç”Ÿæˆå®Œæˆ - ä¼°è®¡tokens: {total_tokens}")
                    
        except Exception as e:
            self.logger.error(f"âŒ æµå¼AIç”Ÿæˆå¤±è´¥: {e}")
            yield {
                "content": f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}",
                "tokens": 0
            }


class ConversationManager:
    """å¯¹è¯ç®¡ç†æœåŠ¡"""
    
    def __init__(self, es_service):
        self.es_service = es_service
        self.logger = get_logger(f"{__name__}.{self.__class__.__name__}")
    
    async def save_conversation(
        self,
        conversation_id: str,
        user_message: str,
        assistant_message: str,
        topic_id: Optional[int] = None,
        retrieved_contexts: List[RetrievedContext] = None,
        ai_metadata: Dict[str, Any] = None
    ):
        """ä¿å­˜å¯¹è¯"""
        try:
            self.logger.debug(f"ğŸ’¾ ä¿å­˜å¯¹è¯ - conversation_id: {conversation_id}, "
                            f"topic_id: {topic_id}")
            
            await self.es_service.save_conversation(
                conversation_id=conversation_id,
                user_message=user_message,
                assistant_message=assistant_message,
                topic_id=topic_id,
                retrieved_contexts=retrieved_contexts or [],
                ai_metadata=ai_metadata or {}
            )
            
            self.logger.info(f"âœ… å¯¹è¯ä¿å­˜æˆåŠŸ - conversation_id: {conversation_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ å¯¹è¯ä¿å­˜å¤±è´¥: {e}")
            # ä¸é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
    
    async def get_conversation_history(
        self,
        conversation_id: str,
        limit: int = 50
    ) -> List[ChatMessage]:
        """è·å–å¯¹è¯å†å²"""
        try:
            self.logger.debug(f"ğŸ“œ è·å–å¯¹è¯å†å² - conversation_id: {conversation_id}, limit: {limit}")
            
            messages = await self.es_service.get_conversation_messages(
                conversation_id, limit
            )
            
            self.logger.info(f"âœ… è·å–å¯¹è¯å†å²æˆåŠŸ - {len(messages)}æ¡æ¶ˆæ¯")
            return messages
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–å¯¹è¯å†å²å¤±è´¥: {e}")
            return []


# ä¾¿æ·å‡½æ•°
def get_chat_service() -> ChatService:
    """è·å–ChatServiceå®ä¾‹"""
    return ChatService()