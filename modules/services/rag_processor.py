"""
RAGå¤„ç†æœåŠ¡å®ç°

æä¾›å®Œæ•´çš„RAGæ–‡æ¡£å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬åˆ†å—ã€åµŒå…¥ç”Ÿæˆå’Œå‘é‡å­˜å‚¨ã€‚
"""

import asyncio
import logging
import time
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from modules.schemas import Document, DocumentChunk
from modules.schemas.enums import ContentType, ProcessingStatus
from modules.chunking.strategy_factory import get_global_factory
from modules.chunking.base import ChunkingContext
from modules.chunking.config import RuntimeChunkingConfig, ChunkingMode
from modules.chunking.integration import initialize_chunking_integration
from modules.embedding.base import IEmbeddingService, EmbeddingProvider
from modules.vector_store.base import IVectorStore, VectorDocument, VectorStoreProvider, SummaryDocument
from modules.models import DocumentChunk as DocumentChunkModel

logger = logging.getLogger(__name__)


class RAGProcessingError(Exception):
    """RAGå¤„ç†é”™è¯¯"""
    
    def __init__(self, message: str, stage: str = None, error_code: str = None):
        self.stage = stage
        self.error_code = error_code
        super().__init__(message)


class RAGProcessorConfig:
    """RAGå¤„ç†å™¨é…ç½®"""
    
    def __init__(
        self,
        embedding_provider: str = "openai",
        vector_store_provider: str = "weaviate",
        collection_name: str = "documents",
        batch_size: int = 100,
        max_concurrent_embeddings: int = 5,
        enable_quality_scoring: bool = True,
        enable_summary_generation: bool = True,  # æ–°å¢ï¼šå¯ç”¨æ‘˜è¦ç”Ÿæˆ
        summary_min_content_length: int = 500,  # æ–°å¢ï¼šæ‘˜è¦ç”Ÿæˆæœ€å°å†…å®¹é•¿åº¦
        retry_attempts: int = 3,
        timeout_seconds: int = 300,
    ):
        self.embedding_provider = embedding_provider
        self.vector_store_provider = vector_store_provider
        self.collection_name = collection_name
        self.batch_size = batch_size
        self.max_concurrent_embeddings = max_concurrent_embeddings
        self.enable_quality_scoring = enable_quality_scoring
        self.enable_summary_generation = enable_summary_generation
        self.summary_min_content_length = summary_min_content_length
        self.retry_attempts = retry_attempts
        self.timeout_seconds = timeout_seconds


class RAGProcessingResult:
    """RAGå¤„ç†ç»“æœ"""
    
    def __init__(
        self,
        document_id: str,
        status: ProcessingStatus,
        chunks_created: int = 0,
        embeddings_generated: int = 0,
        vectors_stored: int = 0,
        processing_time_ms: float = 0,
        strategy_used: str = None,
        quality_score: float = 0.0,
        error_message: str = None,
        stage_details: Dict[str, Any] = None,
        # æ–°å¢æ‘˜è¦ç›¸å…³å­—æ®µ
        summary_generated: bool = False,
        summary_id: Optional[str] = None,
        summary_content: Optional[str] = None,
        summary_key_topics: Optional[List[str]] = None,
    ):
        self.document_id = document_id
        self.status = status
        self.chunks_created = chunks_created
        self.embeddings_generated = embeddings_generated
        self.vectors_stored = vectors_stored
        self.processing_time_ms = processing_time_ms
        self.strategy_used = strategy_used
        self.quality_score = quality_score
        self.error_message = error_message
        self.stage_details = stage_details or {}
        # æ‘˜è¦ç›¸å…³å­—æ®µ
        self.summary_generated = summary_generated
        self.summary_id = summary_id
        self.summary_content = summary_content
        self.summary_key_topics = summary_key_topics or []


class RAGProcessor:
    """RAGå¤„ç†å™¨ - æä¾›å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
    
    def __init__(
        self,
        config: Optional[RAGProcessorConfig] = None,
        embedding_service: Optional[IEmbeddingService] = None,
        vector_store: Optional[IVectorStore] = None,
    ):
        self.config = config or RAGProcessorConfig()
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        
        # ä½¿ç”¨å…¨å±€åˆ†å—å·¥å‚
        self.chunking_factory = None  # å°†åœ¨ initialize() ä¸­è®¾ç½®
        
        # å¤„ç†çŠ¶æ€
        self._initialized = False
        self._processing_semaphore = asyncio.Semaphore(
            self.config.max_concurrent_embeddings
        )
        
        logger.info(f"RAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.config.embedding_provider} + {self.config.vector_store_provider}")
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–RAGå¤„ç†å™¨"""
        if self._initialized:
            return
        
        try:
            # åˆå§‹åŒ–chunkingé›†æˆï¼ˆè¿™ä¼šè‡ªåŠ¨æ³¨å†Œæ‰€æœ‰ç­–ç•¥ï¼‰
            await initialize_chunking_integration()
            self.chunking_factory = get_global_factory()
            logger.info("åˆ†å—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if self.embedding_service:
                await self.embedding_service.initialize()
                logger.info(f"åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ: {self.embedding_service.service_name}")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            if self.vector_store:
                await self.vector_store.initialize()
                logger.info(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ: {self.config.vector_store_provider}")
            
            self._initialized = True
            logger.info("RAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"RAGå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise RAGProcessingError(f"åˆå§‹åŒ–å¤±è´¥: {e}", stage="initialization")
    
    async def process_document(
        self,
        document: Document,
        chunking_config: Optional[Dict[str, Any]] = None,
        topic_id: Optional[str] = None,
        file_id: Optional[str] = None,
    ) -> RAGProcessingResult:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´RAGæµç¨‹
        
        Args:
            document: è¦å¤„ç†çš„æ–‡æ¡£
            chunking_config: åˆ†å—é…ç½®
            topic_id: ä¸»é¢˜ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            RAGProcessingResult: å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        # ç¡®ä¿åˆå§‹åŒ–
        await self.initialize()
        
        try:
            # é˜¶æ®µ1: æ™ºèƒ½åˆ†å—
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ {document.id}: æ™ºèƒ½åˆ†å—é˜¶æ®µ")
            chunks, chunking_details = await self._process_chunking(
                document, chunking_config
            )
            
            if not chunks:
                raise RAGProcessingError("åˆ†å—å¤„ç†å¤±è´¥: æœªç”Ÿæˆä»»ä½•æ–‡æ¡£å—", stage="chunking")
            
            logger.info(f"åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—ï¼Œç­–ç•¥: {chunking_details.get('strategy_used')}")
            
            # é˜¶æ®µ2: ç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"å¼€å§‹åµŒå…¥ç”Ÿæˆé˜¶æ®µ: {len(chunks)} ä¸ªå—")
            embeddings = await self._generate_embeddings(chunks)
            
            if len(embeddings) != len(chunks):
                raise RAGProcessingError(
                    f"åµŒå…¥æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(chunks)}, å®é™… {len(embeddings)}",
                    stage="embedding"
                )
            
            logger.info(f"åµŒå…¥ç”Ÿæˆå®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
            
            # é˜¶æ®µ3: å­˜å‚¨å‘é‡
            logger.info(f"å¼€å§‹å‘é‡å­˜å‚¨é˜¶æ®µ")
            stored_count = await self._store_vectors(
                chunks, embeddings, document, topic_id, file_id
            )
            
            logger.info(f"å‘é‡å­˜å‚¨å®Œæˆ: {stored_count} ä¸ªå‘é‡")
            
            # é˜¶æ®µ4: ç”Ÿæˆæ‘˜è¦ç´¢å¼• (æ–°å¢)
            summary_result = None
            if (self.config.enable_summary_generation and 
                len(document.content) >= self.config.summary_min_content_length):
                logger.info(f"å¼€å§‹æ‘˜è¦ç”Ÿæˆé˜¶æ®µ")
                summary_result = await self._generate_document_summary(
                    document, topic_id, file_id
                )
                if summary_result:
                    logger.info(f"æ‘˜è¦ç”Ÿæˆå®Œæˆ: {summary_result.id}")
                else:
                    logger.warning("æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½†ä¸å½±å“æ•´ä½“å¤„ç†")
            else:
                logger.info(f"è·³è¿‡æ‘˜è¦ç”Ÿæˆ: å†…å®¹é•¿åº¦ {len(document.content)} < {self.config.summary_min_content_length}")
            
            # è®¡ç®—å¤„ç†æ—¶é—´å’Œè´¨é‡åˆ†æ•°
            processing_time = (time.time() - start_time) * 1000
            quality_score = self._calculate_quality_score(chunks, chunking_details)
            
            result = RAGProcessingResult(
                document_id=document.id,
                status=ProcessingStatus.COMPLETED,
                chunks_created=len(chunks),
                embeddings_generated=len(embeddings),
                vectors_stored=stored_count,
                processing_time_ms=processing_time,
                strategy_used=chunking_details.get('strategy_used'),
                quality_score=quality_score,
                # æ‘˜è¦ç›¸å…³å­—æ®µ
                summary_generated=summary_result is not None,
                summary_id=summary_result.id if summary_result else None,
                summary_content=summary_result.summary[:200] + "..." if summary_result and len(summary_result.summary) > 200 else (summary_result.summary if summary_result else None),
                summary_key_topics=summary_result.key_topics if summary_result else None,
                stage_details={
                    'chunking': chunking_details,
                    'embedding_dimension': len(embeddings[0]) if embeddings else 0,
                    'vector_store_collection': self.config.collection_name,
                    'summary_generated': summary_result is not None,
                    'summary_length': len(summary_result.summary) if summary_result else 0,
                }
            )
            
            logger.info(
                f"æ–‡æ¡£ {document.id} RAGå¤„ç†å®Œæˆ: "
                f"{result.chunks_created} å—, "
                f"{result.embeddings_generated} åµŒå…¥, "
                f"{result.vectors_stored} å­˜å‚¨, "
                f"è€—æ—¶ {result.processing_time_ms:.1f}ms"
            )
            
            return result
            
        except RAGProcessingError:
            raise
        except Exception as e:
            logger.error(f"æ–‡æ¡£ {document.id} RAGå¤„ç†å¤±è´¥: {e}")
            return RAGProcessingResult(
                document_id=document.id,
                status=ProcessingStatus.FAILED,
                processing_time_ms=(time.time() - start_time) * 1000,
                error_message=str(e)
            )
        finally:
            # ç¡®ä¿æ¸…ç†å‘é‡å­˜å‚¨è¿æ¥
            try:
                if hasattr(self.vector_store, 'cleanup'):
                    await self.vector_store.cleanup()
            except Exception as cleanup_error:
                logger.warning(f"å‘é‡å­˜å‚¨æ¸…ç†å¤±è´¥: {cleanup_error}")
    
    async def _process_chunking(
        self, document: Document, chunking_config: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[DocumentChunk], Dict[str, Any]]:
        """å¤„ç†æ–‡æ¡£åˆ†å—"""
        try:
            # è§£æåˆ†å—é…ç½®
            config = self._parse_chunking_config(chunking_config)
            
            # åˆ†ææ–‡æ¡£ç‰¹å¾
            content = document.content
            words = content.split()
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            sentences = [s.strip() for s in content.split('.') if s.strip()]
            
            # è½¬æ¢å†…å®¹ç±»å‹ä¸ºMIMEç±»å‹æ ¼å¼
            content_type_mapping = {
                ContentType.TEXT: "text/plain",
                ContentType.TXT: "text/plain", 
                ContentType.MD: "text/markdown",
                ContentType.HTML: "text/html",
                ContentType.PDF: "application/pdf",
                ContentType.JSON: "application/json",
                ContentType.CSV: "text/csv",
                ContentType.XML: "text/xml"
            }
            mime_content_type = content_type_mapping.get(document.content_type, "text/plain")
            
            # åˆ›å»ºåˆ†å—ä¸Šä¸‹æ–‡
            context = ChunkingContext(
                document=document,
                document_length=len(content),
                word_count=len(words),
                paragraph_count=len(paragraphs),
                sentence_count=len(sentences),
                avg_paragraph_length=sum(len(p) for p in paragraphs) / max(len(paragraphs), 1),
                avg_sentence_length=sum(len(s) for s in sentences) / max(len(sentences), 1),
                has_structure_markers=any(marker in content for marker in ['#', '##', '###', '-', '*', '1.', '2.']),
                content_type=mime_content_type,  # ä½¿ç”¨è½¬æ¢åçš„MIMEç±»å‹
                target_chunk_size=getattr(config, 'target_chunk_size', 1000),
                overlap_size=getattr(config, 'overlap_size', 200),
                metadata=chunking_config or {}
            )
            
            # æ‰§è¡Œåˆ†å—
            if config.mode == ChunkingMode.AUTO:
                # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
                result = await self.chunking_factory.chunk_document(context)
            else:
                # ä½¿ç”¨æŒ‡å®šç­–ç•¥
                strategy_name = self._convert_mode_to_strategy_name(config.mode)
                result = await self.chunking_factory.chunk_document(context, strategy_name)
            
            # è½¬æ¢ä¸ºDocumentChunkå¯¹è±¡
            document_chunks = []
            for i, chunk in enumerate(result.chunks):
                doc_chunk = DocumentChunk(
                    id=str(uuid4()),
                    document_id=document.id,
                    chunk_index=i,
                    content=chunk.content,
                    metadata={
                        'start_char': chunk.start_char,
                        'end_char': chunk.end_char,
                        'token_count': getattr(chunk, 'token_count', len(chunk.content.split())),
                        'quality_score': getattr(chunk, 'quality_score', 0.0),
                        'strategy': result.strategy_used,
                    }
                )
                document_chunks.append(doc_chunk)
            
            details = {
                'strategy_used': result.strategy_used,
                'quality_score': result.quality_score,
                'processing_time_ms': result.processing_time_ms,
                'chunk_count': result.chunk_count,
                'average_chunk_size': sum(len(c.content) for c in document_chunks) / len(document_chunks) if document_chunks else 0,
            }
            
            return document_chunks, details
            
        except Exception as e:
            logger.error(f"åˆ†å—å¤„ç†å¤±è´¥: {e}")
            raise RAGProcessingError(f"åˆ†å—å¤„ç†å¤±è´¥: {e}", stage="chunking")
    
    async def _generate_embeddings(
        self, chunks: List[DocumentChunk]
    ) -> List[List[float]]:
        """ç”ŸæˆåµŒå…¥å‘é‡"""
        if not self.embedding_service:
            raise RAGProcessingError("åµŒå…¥æœåŠ¡æœªé…ç½®", stage="embedding")
        
        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = [chunk.content for chunk in chunks]
            
            # æ‰¹é‡ç”ŸæˆåµŒå…¥
            embeddings = []
            batch_size = self.config.batch_size
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                async with self._processing_semaphore:
                    # ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„åµŒå…¥
                    batch_result = await self.embedding_service.generate_embeddings(batch_texts)
                    embeddings.extend(batch_result.vectors)
                
                logger.debug(f"åµŒå…¥ç”Ÿæˆè¿›åº¦: {len(embeddings)}/{len(texts)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            raise RAGProcessingError(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}", stage="embedding")
    
    async def _store_vectors(
        self,
        chunks: List[DocumentChunk],
        embeddings: List[List[float]],
        document: Document,
        topic_id: Optional[str] = None,
        file_id: Optional[str] = None,
    ) -> int:
        """å­˜å‚¨å‘é‡åˆ°å‘é‡æ•°æ®åº“"""
        if not self.vector_store:
            raise RAGProcessingError("å‘é‡å­˜å‚¨æœåŠ¡æœªé…ç½®", stage="vector_storage")
        
        try:
            # åˆ›å»ºå‘é‡æ–‡æ¡£
            vector_documents = []
            for chunk, embedding in zip(chunks, embeddings):
                vector_doc = VectorDocument(
                    id=chunk.id,
                    vector=embedding,
                    content=chunk.content,
                    metadata={
                        'document_id': document.id,
                        'chunk_index': chunk.chunk_index,
                        'content_type': document.content_type.value if hasattr(document.content_type, 'value') else str(document.content_type),
                        'topic_id': topic_id,
                        'file_id': file_id,
                        'title': document.title,
                        'chunk_metadata': chunk.metadata,
                        'created_at': document.created_at.isoformat() if hasattr(document, 'created_at') else None,
                        'collection_name': self.config.collection_name,  # ç§»åˆ°metadataä¸­
                    },
                    document_id=document.id,
                    chunk_index=chunk.chunk_index,
                )
                vector_documents.append(vector_doc)
            
            # æ‰¹é‡ä¸Šä¼ å‘é‡
            result = await self.vector_store.upsert_vectors(vector_documents)
            
            if result.failed_count > 0:
                logger.warning(f"å‘é‡å­˜å‚¨éƒ¨åˆ†å¤±è´¥: {result.failed_count}/{len(vector_documents)}")
            
            return result.success_count
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise RAGProcessingError(f"å‘é‡å­˜å‚¨å¤±è´¥: {e}", stage="vector_storage")
    
    def _parse_chunking_config(
        self, chunking_config: Optional[Dict[str, Any]] = None
    ) -> RuntimeChunkingConfig:
        """è§£æåˆ†å—é…ç½®"""
        if not chunking_config:
            return RuntimeChunkingConfig(mode=ChunkingMode.AUTO)
        
        # è§£ææ¨èç­–ç•¥
        recommended_strategy = chunking_config.get('recommended_strategy', 'auto')
        
        # è½¬æ¢ä¸ºChunkingMode
        mode_mapping = {
            'fixed_size': ChunkingMode.FIXED_SIZE,
            'semantic': ChunkingMode.SEMANTIC,
            'paragraph': ChunkingMode.PARAGRAPH,
            'sentence': ChunkingMode.SENTENCE,
            'adaptive': ChunkingMode.ADAPTIVE,
            'auto': ChunkingMode.AUTO,
        }
        
        mode = mode_mapping.get(recommended_strategy, ChunkingMode.AUTO)
        
        return RuntimeChunkingConfig(
            mode=mode,
            target_chunk_size=chunking_config.get('chunk_size', 1000),
            overlap_size=chunking_config.get('overlap', 200),
        )
    
    def _convert_mode_to_strategy_name(self, mode: ChunkingMode) -> str:
        """è½¬æ¢æ¨¡å¼ä¸ºç­–ç•¥åç§°"""
        mapping = {
            ChunkingMode.FIXED_SIZE: 'fixed_size',
            ChunkingMode.SEMANTIC: 'semantic', 
            ChunkingMode.PARAGRAPH: 'paragraph',
            ChunkingMode.SENTENCE: 'sentence',
            ChunkingMode.ADAPTIVE: 'adaptive',
        }
        return mapping.get(mode, 'fixed_size')
    
    async def _generate_document_summary(
        self, 
        document: Document, 
        topic_id: Optional[str], 
        file_id: Optional[str]
    ) -> Optional[SummaryDocument]:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å¹¶å­˜å‚¨åˆ°æ‘˜è¦ç´¢å¼•"""
        try:
            # å¯¼å…¥æ‘˜è¦æœåŠ¡å’Œæ•°æ®åº“è¿æ¥
            from modules.services.summary_service import SummaryGenerationService
            from modules.database.connection import get_session
            
            # ä½¿ç”¨æ•°æ®åº“sessionåˆ›å»ºæ‘˜è¦æœåŠ¡å®ä¾‹
            async with get_session() as session:
                summary_service = SummaryGenerationService(
                    session=session,
                    vector_store=None  # è®©æœåŠ¡è‡ªå·±åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹
                )
                
                # ç¡®ä¿æœåŠ¡å®Œå…¨åˆå§‹åŒ–
                logger.info("ğŸ”§ åˆå§‹åŒ–æ‘˜è¦ç”ŸæˆæœåŠ¡...")
                await summary_service.initialize()
                
                # è¯¦ç»†æ£€æŸ¥æœåŠ¡çŠ¶æ€
                logger.info("ğŸ” å¼€å§‹éªŒè¯æ‘˜è¦æœåŠ¡ç»„ä»¶...")
                
                # æ£€æŸ¥AIå®¢æˆ·ç«¯
                if not hasattr(summary_service, 'ai_client') or summary_service.ai_client is None:
                    logger.error("âŒ AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                    raise ValueError("AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
                logger.info("âœ… AIå®¢æˆ·ç«¯çŠ¶æ€æ­£å¸¸")
                
                # æ£€æŸ¥åµŒå…¥æœåŠ¡åŠå…¶åˆå§‹åŒ–çŠ¶æ€
                if not hasattr(summary_service, 'embedding_service') or summary_service.embedding_service is None:
                    logger.error("âŒ åµŒå…¥æœåŠ¡æœªåˆ›å»º")
                    raise ValueError("åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥")
                
                # æ£€æŸ¥åµŒå…¥æœåŠ¡çš„_initializedçŠ¶æ€
                if hasattr(summary_service.embedding_service, '_initialized') and not summary_service.embedding_service._initialized:
                    logger.error("âŒ åµŒå…¥æœåŠ¡æœªå®Œæˆåˆå§‹åŒ–")
                    raise ValueError("åµŒå…¥æœåŠ¡æœªå®Œæˆåˆå§‹åŒ–")
                logger.info("âœ… åµŒå…¥æœåŠ¡çŠ¶æ€æ­£å¸¸")
                
                # æ£€æŸ¥å‘é‡å­˜å‚¨åŠå…¶åˆå§‹åŒ–çŠ¶æ€
                if not hasattr(summary_service, 'vector_store') or summary_service.vector_store is None:
                    logger.error("âŒ å‘é‡å­˜å‚¨æœªåˆ›å»º")
                    raise ValueError("å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥")
                
                # æ£€æŸ¥å‘é‡å­˜å‚¨çš„_initializedçŠ¶æ€
                if hasattr(summary_service.vector_store, '_initialized') and not summary_service.vector_store._initialized:
                    logger.error("âŒ å‘é‡å­˜å‚¨æœªå®Œæˆåˆå§‹åŒ–")
                    raise ValueError("å‘é‡å­˜å‚¨æœªå®Œæˆåˆå§‹åŒ–")
                logger.info("âœ… å‘é‡å­˜å‚¨çŠ¶æ€æ­£å¸¸")
                
                logger.info("âœ… æ‘˜è¦æœåŠ¡æ‰€æœ‰ç»„ä»¶éªŒè¯å®Œæˆ")
                
                # ç”Ÿæˆæ‘˜è¦
                logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ–‡æ¡£æ‘˜è¦: {document.id}")
                summary_doc = await summary_service.generate_document_summary(
                    document_id=document.id,
                    content=document.content,
                    topic_id=topic_id
                )
                
                # æ‰‹åŠ¨æäº¤æ•°æ®åº“äº‹åŠ¡
                await session.commit()
                logger.info(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ: {document.id}")
                
                return summary_doc
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¤±è´¥ {document.id}: {e}")
            # æ‘˜è¦ç”Ÿæˆå¤±è´¥ä¸åº”è¯¥å½±å“æ•´ä¸ªRAGæµç¨‹
            return None
    
    def _calculate_quality_score(
        self, chunks: List[DocumentChunk], chunking_details: Dict[str, Any]
    ) -> float:
        """è®¡ç®—å¤„ç†è´¨é‡åˆ†æ•°"""
        if not chunks:
            return 0.0
        
        # åŸºç¡€è´¨é‡åˆ†æ•°ï¼ˆæ¥è‡ªåˆ†å—è¿‡ç¨‹ï¼‰
        base_score = chunking_details.get('quality_score', 0.5)
        
        # å—æ•°é‡è¯„åˆ†
        chunk_count_score = min(1.0, len(chunks) / 50)  # å‡è®¾50ä¸ªå—æ˜¯ç†æƒ³æ•°é‡
        
        # å—å¤§å°ä¸€è‡´æ€§è¯„åˆ†
        chunk_sizes = [len(chunk.content) for chunk in chunks]
        avg_size = sum(chunk_sizes) / len(chunk_sizes)
        size_variance = sum((size - avg_size) ** 2 for size in chunk_sizes) / len(chunk_sizes)
        size_consistency_score = max(0, 1 - (size_variance / (avg_size ** 2)))
        
        # ç»¼åˆè¯„åˆ†
        final_score = (base_score * 0.6 + chunk_count_score * 0.2 + size_consistency_score * 0.2)
        
        return min(1.0, max(0.0, final_score))
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        status = {
            'rag_processor': 'healthy',
            'initialized': self._initialized,
            'config': {
                'embedding_provider': self.config.embedding_provider,
                'vector_store_provider': self.config.vector_store_provider,
                'collection_name': self.config.collection_name,
            }
        }
        
        # æ£€æŸ¥åµŒå…¥æœåŠ¡
        if self.embedding_service:
            try:
                embedding_health = await self.embedding_service.health_check()
                status['embedding_service'] = embedding_health
            except Exception as e:
                status['embedding_service'] = {'status': 'unhealthy', 'error': str(e)}
        else:
            status['embedding_service'] = {'status': 'not_configured'}
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨
        if self.vector_store:
            try:
                vector_store_health = await self.vector_store.health_check()
                status['vector_store'] = vector_store_health
            except Exception as e:
                status['vector_store'] = {'status': 'unhealthy', 'error': str(e)}
        else:
            status['vector_store'] = {'status': 'not_configured'}
        
        return status
