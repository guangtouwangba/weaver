"""
Summary Generation Service

æ‘˜è¦ç”ŸæˆæœåŠ¡ï¼Œè´Ÿè´£ä»æ–‡æ¡£ç”Ÿæˆæ‘˜è¦å¹¶å­˜å‚¨åˆ°æ‘˜è¦ç´¢å¼•ä¸­ã€‚
"""

import logging
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional

try:
    import openai
except ImportError:
    openai = None

from sqlalchemy.ext.asyncio import AsyncSession

from config.settings import get_config
from logging_system import get_logger
from modules.services.base_service import BaseService
from modules.vector_store import SummaryDocument, VectorStoreError
from modules.vector_store.weaviate_service import WeaviateVectorStore
from modules.embedding.openai_service import OpenAIEmbeddingService
from modules.repository import DocumentRepository
from modules.schemas import Document

logger = get_logger(__name__)


class SummaryGenerationService(BaseService):
    """æ‘˜è¦ç”ŸæˆæœåŠ¡"""
    
    def __init__(self, session: AsyncSession, vector_store: WeaviateVectorStore = None):
        super().__init__(session)
        self.config = get_config()
        self.vector_store = vector_store
        self.embedding_service: Optional[OpenAIEmbeddingService] = None
        self.ai_client = None
        
        # æ‘˜è¦ç”Ÿæˆé…ç½®
        self.max_chunk_length = 4000  # å•ä¸ªæ‘˜è¦å¤„ç†çš„æœ€å¤§æ–‡æœ¬é•¿åº¦
        self.summary_overlap = 200    # æ‘˜è¦é‡å é•¿åº¦
        self.min_content_length = 500 # æœ€å°å†…å®¹é•¿åº¦æ‰ç”Ÿæˆæ‘˜è¦
        
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            if hasattr(self.config, 'ai') and hasattr(self.config.ai, 'embedding'):
                openai_config = self.config.ai.embedding.openai
                api_key = getattr(openai_config, 'api_key', None)
                if api_key:
                    # æ„å»ºåµŒå…¥æœåŠ¡é…ç½®ï¼ˆåŒ…å«ä»£ç†ï¼‰
                    service_kwargs = {'api_key': api_key}
                    
                    # æ·»åŠ ä»£ç†é…ç½®
                    http_proxy = getattr(openai_config, 'http_proxy', None)
                    https_proxy = getattr(openai_config, 'https_proxy', None)
                    api_base = getattr(openai_config, 'api_base', None)
                    
                    if http_proxy:
                        service_kwargs['http_proxy'] = http_proxy
                    if https_proxy:
                        service_kwargs['https_proxy'] = https_proxy
                    if api_base:
                        service_kwargs['api_base'] = api_base
                    
                    self.embedding_service = OpenAIEmbeddingService(**service_kwargs)
                    await self.embedding_service.initialize()
                    logger.info("âœ… åµŒå…¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–AIå®¢æˆ·ç«¯ç”¨äºç”Ÿæˆæ‘˜è¦
            if hasattr(self.config, 'ai') and hasattr(self.config.ai, 'chat'):
                openai_chat_config = self.config.ai.chat.openai
                chat_api_key = getattr(openai_chat_config, 'api_key', None)
                if chat_api_key and openai:
                    # æ„å»ºAIå®¢æˆ·ç«¯é…ç½®ï¼ˆåŒ…å«ä»£ç†ï¼‰
                    client_kwargs = {'api_key': chat_api_key}
                    
                    # æ·»åŠ API base URLé…ç½®
                    api_base = getattr(openai_chat_config, 'api_base', None)
                    if api_base:
                        client_kwargs['base_url'] = api_base
                    
                    # æ·»åŠ ä»£ç†é…ç½®
                    http_proxy = getattr(openai_chat_config, 'http_proxy', None)
                    https_proxy = getattr(openai_chat_config, 'https_proxy', None)
                    
                    if http_proxy or https_proxy:
                        import httpx
                        # ä½¿ç”¨HTTPSä»£ç†ï¼ˆé€šå¸¸HTTPå’ŒHTTPSä»£ç†åœ°å€ç›¸åŒï¼‰
                        proxy_url = https_proxy or http_proxy
                        
                        # åˆ›å»ºhttpxå®¢æˆ·ç«¯with proxy
                        http_client = httpx.AsyncClient(proxy=proxy_url)
                        client_kwargs["http_client"] = http_client
                        logger.info(f"AIå®¢æˆ·ç«¯é…ç½®ä»£ç†: {proxy_url}")
                    
                    self.ai_client = openai.AsyncOpenAI(**client_kwargs)
                    logger.info("âœ… AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            if not self.vector_store:
                weaviate_url = getattr(self.config, 'weaviate_url', 'http://localhost:8080')
                self.vector_store = WeaviateVectorStore(url=weaviate_url)
                await self.vector_store.initialize()
                logger.info("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ‘˜è¦æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def generate_document_summary(
        self,
        document_id: str,
        content: str,
        topic_id: Optional[str] = None,
        force_regenerate: bool = False
    ) -> SummaryDocument:
        """ä¸ºå•ä¸ªæ–‡æ¡£ç”Ÿæˆæ‘˜è¦"""
        
        if len(content) < self.min_content_length:
            raise ValueError(f"æ–‡æ¡£å†…å®¹å¤ªçŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ (æœ€å°é•¿åº¦: {self.min_content_length})")
        
        logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ–‡æ¡£æ‘˜è¦: document_id={document_id}, content_length={len(content)}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ‘˜è¦ï¼ˆå¦‚æœä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼‰
            if not force_regenerate:
                existing_summary = await self._check_existing_summary(document_id)
                if existing_summary:
                    logger.info(f"ğŸ“‹ å‘ç°å·²å­˜åœ¨çš„æ‘˜è¦: {document_id}")
                    return existing_summary
            
            # å°†é•¿æ–‡æ¡£åˆ†å—å¤„ç†
            chunks = self._split_content_for_summary(content)
            
            # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå­æ‘˜è¦
            chunk_summaries = []
            for i, chunk in enumerate(chunks):
                logger.debug(f"ğŸ§® å¤„ç†ç¬¬ {i+1}/{len(chunks)} ä¸ªæ–‡æ¡£å—...")
                chunk_summary = await self._generate_chunk_summary(chunk, i+1, len(chunks))
                if chunk_summary:
                    chunk_summaries.append(chunk_summary)
            
            # å¦‚æœæœ‰å¤šä¸ªå—ï¼Œéœ€è¦ç”Ÿæˆæ€»ä½“æ‘˜è¦
            if len(chunk_summaries) > 1:
                final_summary = await self._generate_consolidated_summary(chunk_summaries)
            else:
                final_summary = chunk_summaries[0] if chunk_summaries else "æ— æ³•ç”Ÿæˆæ‘˜è¦"
            
            # æå–å…³é”®ä¸»é¢˜
            key_topics = await self._extract_key_topics(final_summary, content[:2000])
            
            # ç”Ÿæˆæ‘˜è¦å‘é‡
            summary_embedding = await self.embedding_service.generate_embedding(final_summary)
            
            # åˆ›å»ºæ‘˜è¦æ–‡æ¡£
            summary_doc = SummaryDocument(
                id=str(uuid.uuid4()),
                vector=summary_embedding,
                summary=final_summary,
                key_topics=key_topics,
                document_ids=[document_id],
                metadata={
                    "topic_id": topic_id,
                    "generated_at": datetime.now(timezone.utc).isoformat(),
                    "source_length": len(content),
                    "chunk_count": len(chunks),
                    "processing_method": "consolidated" if len(chunk_summaries) > 1 else "direct"
                },
                scope_level="document",
                created_at=datetime.now(timezone.utc)
            )
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            result = await self.vector_store.upsert_summary_documents([summary_doc])
            
            if result.success_count > 0:
                logger.info(f"âœ… æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¹¶å­˜å‚¨æˆåŠŸ: {document_id}")
            else:
                logger.error(f"âŒ æ‘˜è¦å­˜å‚¨å¤±è´¥: {result.errors}")
                raise VectorStoreError("æ‘˜è¦å­˜å‚¨å¤±è´¥")
            
            return summary_doc
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¤±è´¥ {document_id}: {e}")
            raise
    
    async def generate_topic_summary(
        self,
        topic_id: str,
        document_ids: List[str],
        force_regenerate: bool = False
    ) -> SummaryDocument:
        """ä¸ºä¸»é¢˜ç”Ÿæˆç»¼åˆæ‘˜è¦"""
        
        logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆä¸»é¢˜æ‘˜è¦: topic_id={topic_id}, documents={len(document_ids)}")
        
        try:
            # è·å–ä¸»é¢˜ä¸‹æ‰€æœ‰æ–‡æ¡£çš„æ‘˜è¦
            document_summaries = []
            async with self.session.begin():
                doc_repo = DocumentRepository(self.session)
                
                for doc_id in document_ids:
                    document = await doc_repo.get_document_by_id(doc_id)
                    if document and document.content:
                        # å°è¯•è·å–å·²æœ‰æ‘˜è¦ï¼Œå¦åˆ™ç”Ÿæˆæ–°æ‘˜è¦
                        try:
                            doc_summary = await self.generate_document_summary(
                                doc_id,
                                document.content,
                                topic_id,
                                force_regenerate=False
                            )
                            document_summaries.append(doc_summary.summary)
                        except Exception as e:
                            logger.warning(f"âš ï¸ è·³è¿‡æ–‡æ¡£ {doc_id} çš„æ‘˜è¦ç”Ÿæˆ: {e}")
                            continue
            
            if not document_summaries:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£æ‘˜è¦æ¥ç”Ÿæˆä¸»é¢˜æ‘˜è¦")
            
            # ç”Ÿæˆä¸»é¢˜ç»¼åˆæ‘˜è¦
            topic_summary = await self._generate_topic_consolidated_summary(
                document_summaries,
                topic_id
            )
            
            # æå–ä¸»é¢˜å…³é”®è¯
            key_topics = await self._extract_topic_key_topics(topic_summary, document_summaries)
            
            # ç”Ÿæˆæ‘˜è¦å‘é‡
            summary_embedding = await self.embedding_service.generate_embedding(topic_summary)
            
            # åˆ›å»ºä¸»é¢˜æ‘˜è¦æ–‡æ¡£
            topic_summary_doc = SummaryDocument(
                id=str(uuid.uuid4()),
                vector=summary_embedding,
                summary=topic_summary,
                key_topics=key_topics,
                document_ids=document_ids,
                metadata={
                    "topic_id": topic_id,
                    "generated_at": datetime.now(timezone.utc).isoformat(),
                    "source_documents_count": len(document_ids),
                    "processing_method": "topic_consolidation"
                },
                scope_level="topic",
                created_at=datetime.now(timezone.utc)
            )
            
            # å­˜å‚¨ä¸»é¢˜æ‘˜è¦
            result = await self.vector_store.upsert_summary_documents([topic_summary_doc])
            
            if result.success_count > 0:
                logger.info(f"âœ… ä¸»é¢˜æ‘˜è¦ç”Ÿæˆå¹¶å­˜å‚¨æˆåŠŸ: {topic_id}")
            else:
                logger.error(f"âŒ ä¸»é¢˜æ‘˜è¦å­˜å‚¨å¤±è´¥: {result.errors}")
                raise VectorStoreError("ä¸»é¢˜æ‘˜è¦å­˜å‚¨å¤±è´¥")
            
            return topic_summary_doc
            
        except Exception as e:
            logger.error(f"âŒ ä¸»é¢˜æ‘˜è¦ç”Ÿæˆå¤±è´¥ {topic_id}: {e}")
            raise
    
    def _split_content_for_summary(self, content: str) -> List[str]:
        """å°†é•¿å†…å®¹åˆ†å—ä»¥ä¾¿ç”Ÿæˆæ‘˜è¦"""
        if len(content) <= self.max_chunk_length:
            return [content]
        
        chunks = []
        start = 0
        
        while start < len(content):
            end = start + self.max_chunk_length
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·å¤„åˆ†å‰²
            if end < len(content):
                # å‘åæŸ¥æ‰¾å¥å·
                last_period = content.rfind('.', start, end)
                if last_period > start + self.max_chunk_length // 2:
                    end = last_period + 1
            
            chunk = content[start:end]
            chunks.append(chunk)
            
            # è®¾ç½®ä¸‹ä¸€å—çš„å¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            if end < len(content):
                start = end - self.summary_overlap
            else:
                break
        
        return chunks
    
    async def _generate_chunk_summary(self, chunk: str, chunk_num: int, total_chunks: int) -> str:
        """ç”Ÿæˆå•ä¸ªå—çš„æ‘˜è¦"""
        if not self.ai_client:
            raise ValueError("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        context_info = f"ï¼ˆè¿™æ˜¯ç¬¬{chunk_num}éƒ¨åˆ†ï¼Œå…±{total_chunks}éƒ¨åˆ†ï¼‰" if total_chunks > 1 else ""
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦{context_info}ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{chunk}

è¦æ±‚ï¼š
1. æå–ä¸»è¦è§‚ç‚¹å’Œæ ¸å¿ƒä¿¡æ¯
2. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œå†…å®¹å‡†ç¡®
3. é•¿åº¦æ§åˆ¶åœ¨200-300å­—
4. ä½¿ç”¨ä¸­æ–‡å›ç­”"""
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=400
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå—æ‘˜è¦å¤±è´¥: {e}")
            return f"ç¬¬{chunk_num}éƒ¨åˆ†æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    async def _generate_consolidated_summary(self, chunk_summaries: List[str]) -> str:
        """ç”Ÿæˆç»¼åˆæ‘˜è¦ - ä½¿ç”¨é€’å½’å¼ç­–ç•¥å¤„ç†é•¿æ–‡æ¡£"""
        if not self.ai_client:
            raise ValueError("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # å¦‚æœæ‘˜è¦å¾ˆå°‘ï¼Œç›´æ¥å¤„ç†
        if len(chunk_summaries) <= 3:
            return await self._merge_summaries_direct(chunk_summaries)
        
        # é€’å½’å¼å¤„ç†ï¼šåˆ†æ‰¹åˆå¹¶æ‘˜è¦
        logger.info(f"ğŸ“Š å¼€å§‹é€’å½’å¼æ‘˜è¦å¤„ç†: {len(chunk_summaries)} ä¸ªå­æ‘˜è¦")
        
        current_summaries = chunk_summaries.copy()
        batch_size = 4  # æ¯æ‰¹å¤„ç†4ä¸ªæ‘˜è¦
        
        while len(current_summaries) > 1:
            next_level_summaries = []
            
            # åˆ†æ‰¹å¤„ç†å½“å‰å±‚çº§çš„æ‘˜è¦
            for i in range(0, len(current_summaries), batch_size):
                batch = current_summaries[i:i + batch_size]
                logger.debug(f"ğŸ”„ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ‘˜è¦: {len(batch)} ä¸ª")
                
                merged_summary = await self._merge_summaries_direct(batch)
                next_level_summaries.append(merged_summary)
            
            current_summaries = next_level_summaries
            logger.info(f"ğŸ“ˆ å®Œæˆä¸€è½®åˆå¹¶ï¼Œå‰©ä½™æ‘˜è¦æ•°: {len(current_summaries)}")
        
        final_summary = current_summaries[0]
        logger.info("âœ… é€’å½’å¼æ‘˜è¦å¤„ç†å®Œæˆ")
        return final_summary
    
    async def _merge_summaries_direct(self, summaries: List[str]) -> str:
        """ç›´æ¥åˆå¹¶å°‘é‡æ‘˜è¦"""
        if not summaries:
            return "æ— å†…å®¹"
        
        if len(summaries) == 1:
            return summaries[0]
        
        summaries_text = "\n\n".join([
            f"éƒ¨åˆ†{i+1}æ‘˜è¦ï¼š{summary}" 
            for i, summary in enumerate(summaries)
        ])
        
        # ä¼°ç®—tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼š4ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtokenï¼‰
        estimated_tokens = len(summaries_text) // 4
        if estimated_tokens > 12000:  # ç•™å‡ºå®‰å…¨è¾¹ç•Œ
            logger.warning(f"âš ï¸ æ‘˜è¦å†…å®¹è¿‡é•¿ ({estimated_tokens} tokens)ï¼Œè¿›ä¸€æ­¥åˆ†å‰²")
            # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œç»§ç»­åˆ†å‰²
            mid = len(summaries) // 2
            left_part = await self._merge_summaries_direct(summaries[:mid])
            right_part = await self._merge_summaries_direct(summaries[mid:])
            return await self._merge_summaries_direct([left_part, right_part])
        
        prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ–‡æ¡£å„éƒ¨åˆ†çš„æ‘˜è¦ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªç»Ÿä¸€çš„ç»¼åˆæ‘˜è¦ï¼š

{summaries_text}

è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰éƒ¨åˆ†çš„æ ¸å¿ƒä¿¡æ¯
2. æ¶ˆé™¤é‡å¤ï¼Œä¿æŒé€»è¾‘è¿è´¯
3. çªå‡ºä¸»è¦è§‚ç‚¹å’Œç»“è®º
4. é•¿åº¦æ§åˆ¶åœ¨300-500å­—
5. ä½¿ç”¨ä¸­æ–‡å›ç­”"""
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=600
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç»¼åˆæ‘˜è¦å¤±è´¥: {e}")
            # å¦‚æœæ˜¯tokené™åˆ¶é”™è¯¯ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²
            if "context_length_exceeded" in str(e).lower():
                logger.warning("ğŸ”„ æ£€æµ‹åˆ°tokenè¶…é™ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²")
                if len(summaries) > 1:
                    mid = len(summaries) // 2
                    left_part = await self._merge_summaries_direct(summaries[:mid])
                    right_part = await self._merge_summaries_direct(summaries[mid:])
                    return await self._merge_summaries_direct([left_part, right_part])
            return "ç»¼åˆæ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    async def _generate_topic_consolidated_summary(
        self,
        document_summaries: List[str],
        topic_id: str
    ) -> str:
        """ç”Ÿæˆä¸»é¢˜ç»¼åˆæ‘˜è¦ - ä½¿ç”¨é€’å½’ç­–ç•¥å¤„ç†å¤šæ–‡æ¡£"""
        if not self.ai_client:
            raise ValueError("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # å¦‚æœæ–‡æ¡£æ‘˜è¦å¾ˆå°‘ï¼Œç›´æ¥å¤„ç†
        if len(document_summaries) <= 3:
            return await self._merge_topic_summaries_direct(document_summaries, topic_id)
        
        # é€’å½’å¼å¤„ç†ï¼šåˆ†æ‰¹åˆå¹¶æ–‡æ¡£æ‘˜è¦
        logger.info(f"ğŸ“Š å¼€å§‹ä¸»é¢˜é€’å½’å¼æ‘˜è¦å¤„ç†: {len(document_summaries)} ä¸ªæ–‡æ¡£æ‘˜è¦")
        
        current_summaries = document_summaries.copy()
        batch_size = 4  # æ¯æ‰¹å¤„ç†4ä¸ªæ–‡æ¡£æ‘˜è¦
        
        while len(current_summaries) > 1:
            next_level_summaries = []
            
            # åˆ†æ‰¹å¤„ç†å½“å‰å±‚çº§çš„æ‘˜è¦
            for i in range(0, len(current_summaries), batch_size):
                batch = current_summaries[i:i + batch_size]
                logger.debug(f"ğŸ”„ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹æ–‡æ¡£æ‘˜è¦: {len(batch)} ä¸ª")
                
                merged_summary = await self._merge_topic_summaries_direct(batch, topic_id)
                next_level_summaries.append(merged_summary)
            
            current_summaries = next_level_summaries
            logger.info(f"ğŸ“ˆ å®Œæˆä¸€è½®ä¸»é¢˜åˆå¹¶ï¼Œå‰©ä½™æ‘˜è¦æ•°: {len(current_summaries)}")
        
        final_summary = current_summaries[0]
        logger.info("âœ… ä¸»é¢˜é€’å½’å¼æ‘˜è¦å¤„ç†å®Œæˆ")
        return final_summary
    
    async def _merge_topic_summaries_direct(self, summaries: List[str], topic_id: str) -> str:
        """ç›´æ¥åˆå¹¶å°‘é‡ä¸»é¢˜æ‘˜è¦"""
        if not summaries:
            return "æ— å†…å®¹"
        
        if len(summaries) == 1:
            return summaries[0]
        
        summaries_text = "\n\n".join([
            f"æ–‡æ¡£{i+1}æ‘˜è¦ï¼š{summary}" 
            for i, summary in enumerate(summaries)
        ])
        
        # ä¼°ç®—tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼š4ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtokenï¼‰
        estimated_tokens = len(summaries_text) // 4
        if estimated_tokens > 12000:  # ç•™å‡ºå®‰å…¨è¾¹ç•Œ
            logger.warning(f"âš ï¸ ä¸»é¢˜æ‘˜è¦å†…å®¹è¿‡é•¿ ({estimated_tokens} tokens)ï¼Œè¿›ä¸€æ­¥åˆ†å‰²")
            # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œç»§ç»­åˆ†å‰²
            mid = len(summaries) // 2
            left_part = await self._merge_topic_summaries_direct(summaries[:mid], topic_id)
            right_part = await self._merge_topic_summaries_direct(summaries[mid:], topic_id)
            return await self._merge_topic_summaries_direct([left_part, right_part], topic_id)
        
        prompt = f"""ä»¥ä¸‹æ˜¯ä¸»é¢˜ {topic_id} ä¸‹å„ä¸ªæ–‡æ¡£çš„æ‘˜è¦ï¼Œè¯·ç”Ÿæˆä¸€ä¸ªä¸»é¢˜çº§åˆ«çš„ç»¼åˆæ‘˜è¦ï¼š

{summaries_text}

è¦æ±‚ï¼š
1. ä»ä¸»é¢˜è§’åº¦æ•´åˆæ‰€æœ‰æ–‡æ¡£çš„ä¿¡æ¯
2. è¯†åˆ«å…±åŒæ¨¡å¼å’Œä¸»è¦è¶‹åŠ¿
3. çªå‡ºä¸»é¢˜çš„æ ¸å¿ƒæ¦‚å¿µå’Œå…³é”®æ´å¯Ÿ
4. æä¾›é«˜å±‚æ¬¡çš„æ¦‚æ‹¬æ€§æè¿°
5. é•¿åº¦æ§åˆ¶åœ¨400-600å­—
6. ä½¿ç”¨ä¸­æ–‡å›ç­”"""
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=800
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¸»é¢˜ç»¼åˆæ‘˜è¦å¤±è´¥: {e}")
            # å¦‚æœæ˜¯tokené™åˆ¶é”™è¯¯ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²
            if "context_length_exceeded" in str(e).lower():
                logger.warning("ğŸ”„ æ£€æµ‹åˆ°ä¸»é¢˜æ‘˜è¦tokenè¶…é™ï¼Œå°è¯•è¿›ä¸€æ­¥åˆ†å‰²")
                if len(summaries) > 1:
                    mid = len(summaries) // 2
                    left_part = await self._merge_topic_summaries_direct(summaries[:mid], topic_id)
                    right_part = await self._merge_topic_summaries_direct(summaries[mid:], topic_id)
                    return await self._merge_topic_summaries_direct([left_part, right_part], topic_id)
            return "ä¸»é¢˜ç»¼åˆæ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    async def _extract_key_topics(self, summary: str, content_preview: str) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        if not self.ai_client:
            return ["ä¸»é¢˜æå–å¤±è´¥"]
        
        prompt = f"""åŸºäºä»¥ä¸‹æ‘˜è¦å’Œå†…å®¹ç‰‡æ®µï¼Œæå–3-5ä¸ªå…³é”®ä¸»é¢˜è¯ï¼š

æ‘˜è¦ï¼š
{summary}

å†…å®¹ç‰‡æ®µï¼š
{content_preview}

è¦æ±‚ï¼š
1. æå–æœ€é‡è¦çš„ä¸»é¢˜å…³é”®è¯
2. æ¯ä¸ªå…³é”®è¯2-4ä¸ªå­—
3. æŒ‰é‡è¦æ€§æ’åº
4. ç”¨é€—å·åˆ†éš”
5. åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–æ–‡å­—"""
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=100
            )
            
            keywords_text = response.choices[0].message.content.strip()
            keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            return keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
            
        except Exception as e:
            logger.error(f"âŒ æå–å…³é”®ä¸»é¢˜å¤±è´¥: {e}")
            return ["å…³é”®è¯æå–å¤±è´¥"]
    
    async def _extract_topic_key_topics(
        self,
        topic_summary: str,
        document_summaries: List[str]
    ) -> List[str]:
        """æå–ä¸»é¢˜çº§åˆ«çš„å…³é”®è¯"""
        if not self.ai_client:
            return ["ä¸»é¢˜å…³é”®è¯æå–å¤±è´¥"]
        
        all_summaries = "\n".join(document_summaries)
        
        prompt = f"""åŸºäºä¸»é¢˜æ‘˜è¦å’Œç›¸å…³æ–‡æ¡£æ‘˜è¦ï¼Œæå–3-5ä¸ªæœ€é‡è¦çš„ä¸»é¢˜å…³é”®è¯ï¼š

ä¸»é¢˜æ‘˜è¦ï¼š
{topic_summary}

ç›¸å…³æ–‡æ¡£æ‘˜è¦ï¼š
{all_summaries[:1500]}...

è¦æ±‚ï¼š
1. æå–èƒ½ä»£è¡¨æ•´ä¸ªä¸»é¢˜çš„æ ¸å¿ƒå…³é”®è¯
2. æ¯ä¸ªå…³é”®è¯2-4ä¸ªå­—
3. æŒ‰é‡è¦æ€§æ’åº
4. ç”¨é€—å·åˆ†éš”
5. åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–æ–‡å­—"""
        
        try:
            response = await self.ai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=100
            )
            
            keywords_text = response.choices[0].message.content.strip()
            keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            return keywords[:5]
            
        except Exception as e:
            logger.error(f"âŒ æå–ä¸»é¢˜å…³é”®è¯å¤±è´¥: {e}")
            return ["ä¸»é¢˜å…³é”®è¯æå–å¤±è´¥"]
    
    async def _check_existing_summary(self, document_id: str) -> Optional[SummaryDocument]:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡æ¡£æ‘˜è¦"""
        try:
            # è¿™é‡Œåº”è¯¥æŸ¥è¯¢æ‘˜è¦ç´¢å¼•ï¼Œç®€åŒ–å®ç°å…ˆè¿”å›None
            # å®é™…å®ç°ä¸­éœ€è¦æŸ¥è¯¢Weaviateçš„summaries collection
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥å·²å­˜åœ¨æ‘˜è¦å¤±è´¥: {e}")
            return None