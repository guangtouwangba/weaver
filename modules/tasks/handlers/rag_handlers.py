"""
RAG Task Handlers

Contains various async task handlers for the RAG system:
- Document processing pipeline
- Embedding vector generation
- Vector storage operations
- Document search functionality
"""

from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from logging_system import (
    get_logger,
    log_errors,
    log_execution_time,
    task_context,
)
from modules import schemas
from modules.embedding import EmbeddingProvider
from modules.pipeline import (
    DocumentProcessingRequest,
    PipelineConfig,
    PipelineStatus,
)
from modules.vector_store import VectorStoreProvider
from modules.services.task_service import register_task_handler, task_handler
from modules.tasks.base import ITaskHandler, TaskPriority, TaskProgress

logger = get_logger(__name__)


@task_handler(
    schemas.TaskName.RAG_PROCESS_DOCUMENT.value,
    priority=TaskPriority.HIGH,
    max_retries=3,
    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
    queue="rag_queue",
)
@task_handler(
    schemas.TaskName.RAG_PROCESS_DOCUMENT_ASYNC.value, 
    priority=TaskPriority.HIGH,
    max_retries=3,
    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
    queue="rag_queue",
)
@register_task_handler
class DocumentProcessingHandler(ITaskHandler):
    """Document processing task handler - executes complete RAG document processing pipeline"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_PROCESS_DOCUMENT.value  # ä¸»è¦ä»»åŠ¡å

    @log_execution_time(threshold_ms=1000)
    async def handle(
        self,
        file_id: str,
        file_path: str,
        topic_id: Optional[int] = None,
        document_id: Optional[str] = None,  # æ”¯æŒä¼ å…¥document_id
        content_type: Optional[str] = None,  # æ”¯æŒä¼ å…¥content_type
        embedding_provider: str = "openai",
        vector_store_provider: str = "weaviate",
        chunking_config: Optional[Dict[str, Any]] = None,  # æ–°çš„chunkingé…ç½®
        **config,
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£çš„å®Œæ•´RAGç®¡é“

        Args:
            file_id: æ–‡ä»¶ID
            file_path: æ–‡ä»¶è·¯å¾„
            topic_id: ä¸»é¢˜ID
            document_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œç”¨äºå¼‚æ­¥å¤„ç†ï¼‰
            content_type: å†…å®¹ç±»å‹ï¼ˆå¯é€‰ï¼‰
            embedding_provider: åµŒå…¥æœåŠ¡æä¾›å•†
            vector_store_provider: å‘é‡å­˜å‚¨æä¾›å•†
            chunking_config: åˆ†å—é…ç½®ï¼ˆå¯é€‰ï¼‰
            **config: å…¶ä»–é…ç½®å‚æ•°

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            with task_context(
                task_id=f"rag-{file_id}",
                task_name="rag.process_document",
                queue="rag_queue",
            ):
                logger.info(f"å¼€å§‹RAGæ–‡æ¡£å¤„ç†: {file_id} at {file_path}")

                # åˆå§‹åŒ–Chunkingé›†æˆç³»ç»Ÿ
                from ...rag.chunking.integration import initialize_chunking_integration
                await initialize_chunking_integration()

                # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå¤„ç†ä¸­
                await self._update_file_status(
                    file_id, "processing", "RAGå¤„ç†ä¸­: åˆå§‹åŒ–ç»„ä»¶"
                )

            # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
            from ...file_loader import MultiFormatFileLoader
            from ...models import ModuleConfig
            from ...rag.processors.enhanced_chunking_processor import EnhancedChunkingProcessor
            from ...services.rag_service import create_document_pipeline

            # æ›´æ–°è¿›åº¦: åˆå§‹åŒ–
            await self._update_progress("åˆå§‹åŒ–RAGç®¡é“ç»„ä»¶", 0, 100)

            # åˆ›å»ºç»„ä»¶é…ç½®
            module_config = ModuleConfig("rag_task")
            pipeline_config = PipelineConfig(
                chunk_size=config.get("chunk_size", 1000),
                chunk_overlap=config.get("chunk_overlap", 200),
                enable_embeddings=config.get("enable_embeddings", True),
                enable_vector_storage=config.get("enable_vector_storage", True),
                batch_size=config.get("batch_size", 50),
                **config.get("pipeline_config", {}),
            )

            # æ›´æ–°æ–‡ä»¶çŠ¶æ€
            await self._update_file_status(
                file_id, "processing", "RAGå¤„ç†ä¸­: åˆ›å»ºç®¡é“ç»„ä»¶"
            )

            # åˆ›å»ºRAGç®¡é“ç»„ä»¶
            from ...services.rag_service import (
                create_embedding_service,
                create_vector_store,
            )

            # æ›´æ–°è¿›åº¦: åˆ›å»ºç»„ä»¶
            await self._update_progress("åˆ›å»ºRAGç®¡é“ç»„ä»¶", 10, 100)

            file_loader = MultiFormatFileLoader(module_config)
            
            # ä½¿ç”¨å¢å¼ºçš„åˆ†å—å¤„ç†å™¨
            processor_config = {
                "default_chunk_size": pipeline_config.chunk_size,
                "default_overlap": pipeline_config.chunk_overlap,
            }
            
            # åˆå¹¶chunking_configå‚æ•°
            if chunking_config:
                logger.info(f"åº”ç”¨æ™ºèƒ½åˆ†å—é…ç½®: {chunking_config}")
                processor_config.update(chunking_config)
                
                # å¦‚æœæœ‰æ¨èç­–ç•¥ï¼Œåº”ç”¨åˆ°å¤„ç†å™¨é…ç½®
                if "recommended_strategy" in chunking_config:
                    processor_config["preferred_strategy"] = chunking_config["recommended_strategy"]
            
            # åˆå¹¶å…¶ä»–é…ç½®
            processor_config.update(config.get("chunking_config", {}))
            
            document_processor = EnhancedChunkingProcessor(**processor_config)

            embedding_service = create_embedding_service(
                provider=EmbeddingProvider(embedding_provider),
                **config.get("embedding_config", {}),
            )

            vector_store = create_vector_store(
                provider=VectorStoreProvider(vector_store_provider),
                **config.get("vector_store_config", {}),
            )

            # åˆ›å»ºæ–‡æ¡£å¤„ç†ç®¡é“
            pipeline = create_document_pipeline(
                file_loader=file_loader,
                document_processor=document_processor,
                embedding_service=embedding_service,
                vector_store=vector_store,
                **pipeline_config.__dict__,
            )

            # æ›´æ–°è¿›åº¦: åˆå§‹åŒ–ç®¡é“
            await self._update_progress("åˆå§‹åŒ–å¤„ç†ç®¡é“", 20, 100)
            await self._update_file_status(
                file_id, "processing", "RAGå¤„ç†ä¸­: åˆå§‹åŒ–å¤„ç†ç®¡é“"
            )

            # åˆå§‹åŒ–ç®¡é“
            await pipeline.initialize()

            # åˆ›å»ºå¤„ç†è¯·æ±‚
            request = DocumentProcessingRequest(
                file_id=file_id,
                file_path=file_path,
                topic_id=topic_id,
                config=pipeline_config,
                metadata={
                    "task_type": "async_processing",
                    "embedding_provider": embedding_provider,
                    "vector_store_provider": vector_store_provider,
                    "started_at": datetime.now(timezone.utc).isoformat(),
                },
            )

            # æ›´æ–°è¿›åº¦: å¼€å§‹å¤„ç†
            await self._update_progress("å¼€å§‹æ–‡æ¡£å¤„ç†", 30, 100)
            await self._update_file_status(
                file_id, "processing", "RAGå¤„ç†ä¸­: æ–‡æ¡£è§£æå’Œåˆ†å—"
            )

            # æ‰§è¡Œå¤„ç†
            result = await pipeline.process_document(request)

            # æ£€æŸ¥å¤„ç†ç»“æœ
            if result.status == PipelineStatus.COMPLETED:
                # æ›´æ–°è¿›åº¦: å¤„ç†å®Œæˆ
                await self._update_progress("å¤„ç†å®Œæˆ", 100, 100)
                await self._update_file_status(
                    file_id,
                    "available",  # RAGå¤„ç†å®Œæˆåè®¾ç½®ä¸ºavailableçŠ¶æ€ï¼Œç”¨æˆ·å¯ä»¥å¼€å§‹æŸ¥è¯¢
                    f"RAGå¤„ç†å®Œæˆ: {result.total_chunks}å—, {result.stored_vectors}å‘é‡",
                )
                logger.info(f"ğŸ‰ æ–‡ä»¶ {file_id} RAGå¤„ç†å®Œæˆï¼ŒçŠ¶æ€æ›´æ–°ä¸ºavailableï¼Œç”¨æˆ·å¯ä»¥å¼€å§‹æŸ¥è¯¢")
            else:
                # å¤„ç†å¤±è´¥
                await self._update_file_status(
                    file_id, "failed", "RAGå¤„ç†å¤±è´¥", error_message=result.error_message
                )
                logger.error(f"âŒ æ–‡ä»¶ {file_id} RAGå¤„ç†å¤±è´¥ï¼ŒçŠ¶æ€æ›´æ–°ä¸ºfailed")

            # æ¸…ç†èµ„æº
            await pipeline.cleanup()

            # æ„å»ºè¿”å›ç»“æœ
            processing_result = {
                "success": result.status == PipelineStatus.COMPLETED,
                "file_id": file_id,
                "document_id": result.document_id,
                "request_id": result.request_id,
                "status": result.status.value,
                "chunks_created": result.total_chunks,
                "embeddings_created": result.embedded_chunks,
                "vectors_stored": result.stored_vectors,
                "processing_time_ms": result.total_processing_time_ms,
                "stage_results": [
                    {
                        "stage": stage.stage.value,
                        "status": stage.status.value,
                        "time_ms": stage.processing_time_ms,
                        "items_processed": stage.items_processed,
                    }
                    for stage in result.stage_results
                ],
                "metadata": result.metadata,
                "completed_at": datetime.now(timezone.utc).isoformat(),
            }

            if not processing_result["success"]:
                processing_result["error"] = result.error_message

            logger.info(
                f"RAGæ–‡æ¡£å¤„ç†å®Œæˆ: {file_id}, æˆåŠŸ: {processing_result['success']}"
            )
            return processing_result

        except Exception as e:
            logger.error(f"RAGæ–‡æ¡£å¤„ç†å¤±è´¥: {file_id}, {e}")

            # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå¤±è´¥
            await self._update_file_status(
                file_id, "failed", "RAGå¤„ç†å¤±è´¥", error_message=str(e)
            )
            logger.error(f"âŒ æ–‡ä»¶ {file_id} RAGå¤„ç†å¼‚å¸¸å¤±è´¥ï¼ŒçŠ¶æ€æ›´æ–°ä¸ºfailed")

            return {
                "success": False,
                "file_id": file_id,
                "error": str(e),
                "error_type": type(e).__name__,
                "failed_at": datetime.now(timezone.utc).isoformat(),
            }

    async def _update_progress(self, description: str, current: int, total: int):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        # è¿™é‡Œéœ€è¦è·å–å½“å‰ä»»åŠ¡çš„task_idï¼Œåœ¨å®é™…Celeryç¯å¢ƒä¸­å¯ä»¥è·å–
        # æš‚æ—¶ä½¿ç”¨å ä½ç¬¦å®ç°
        try:
            progress = TaskProgress(
                task_id="current_task",  # åœ¨å®é™…ä½¿ç”¨ä¸­ä¼šè¢«æ›¿æ¢
                current=current,
                total=total,
                description=description,
            )
            logger.info(f"è¿›åº¦æ›´æ–°: {description} ({current}/{total})")
        except Exception as e:
            logger.debug(f"è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

    async def _update_file_status(
        self,
        file_id: str,
        status: str,
        processing_status: str,
        error_message: Optional[str] = None,
    ):
        """æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        Args:
            file_id: æ–‡ä»¶ID
            status: æ–‡ä»¶çŠ¶æ€ï¼Œåº”ä½¿ç”¨FileStatusæšä¸¾å¯¹åº”çš„å­—ç¬¦ä¸²å€¼
            processing_status: å¤„ç†çŠ¶æ€æè¿°
            error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        try:
            from ...database import get_db_session
            from ...repository import FileRepository

            session_gen = get_db_session()
            session = await session_gen.__anext__()
            try:
                file_repo = FileRepository(session)
                await file_repo.update_file_status(
                    file_id=file_id,
                    status=status,  # æ¥æ”¶å­—ç¬¦ä¸²ï¼Œç”±repositoryå±‚å¤„ç†æšä¸¾æ˜ å°„
                    processing_status=processing_status,
                    error_message=error_message,
                )
                logger.info(f"ğŸ“‹ æ–‡ä»¶çŠ¶æ€æ›´æ–°: {file_id} -> {status}: {processing_status}")
            finally:
                await session.close()
        except Exception as e:
            logger.warning(f"âš ï¸ æ–‡ä»¶çŠ¶æ€æ›´æ–°å¤±è´¥: {file_id}, {e}")
            # ä¸è®©çŠ¶æ€æ›´æ–°å¤±è´¥å½±å“ä¸»æµç¨‹


@task_handler(
    schemas.TaskName.RAG_GENERATE_EMBEDDINGS.value,
    priority=TaskPriority.NORMAL,
    max_retries=2,
    timeout=300,
    queue="rag_queue",
)
@register_task_handler
class EmbeddingGenerationHandler(ITaskHandler):
    """åµŒå…¥å‘é‡ç”Ÿæˆä»»åŠ¡å¤„ç†å™¨"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_GENERATE_EMBEDDINGS.value

    @log_execution_time(threshold_ms=500)
    @log_errors()
    async def handle(
        self,
        texts: List[str],
        provider: str = "openai",
        model_name: Optional[str] = None,
        **config,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡

        Args:
            texts: å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            provider: åµŒå…¥æœåŠ¡æä¾›å•†
            model_name: æ¨¡å‹åç§°
            **config: å…¶ä»–é…ç½®

        Returns:
            Dict[str, Any]: åµŒå…¥ç»“æœ
        """
        try:
            with task_context(
                task_id=f"embedding-{len(texts)}-texts",
                task_name="rag.generate_embeddings",
                queue="rag_queue",
            ):
                logger.info(f"å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡: {len(texts)} ä¸ªæ–‡æœ¬")

            from ...rag.embedding import EmbeddingProvider
            from ...services.rag_service import create_embedding_service

            # åˆ›å»ºåµŒå…¥æœåŠ¡
            embedding_config = config.copy()
            if model_name:
                embedding_config["model_name"] = model_name

            embedding_service = create_embedding_service(
                provider=EmbeddingProvider(provider), **embedding_config
            )

            # åˆå§‹åŒ–æœåŠ¡
            await embedding_service.initialize()

            # ç”ŸæˆåµŒå…¥
            result = await embedding_service.generate_embeddings(texts)

            # æ¸…ç†æœåŠ¡
            await embedding_service.cleanup()

            return {
                "success": True,
                "vectors": result.vectors,
                "texts": result.texts,
                "model_name": result.model_name,
                "dimension": result.dimension,
                "processing_time_ms": result.processing_time_ms,
                "metadata": result.metadata,
                "provider": provider,
            }

        except Exception as e:
            logger.error(f"åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "provider": provider,
                "text_count": len(texts),
            }


@task_handler(
    schemas.TaskName.RAG_STORE_VECTORS.value,
    priority=TaskPriority.NORMAL,
    max_retries=3,
    timeout=180,
    queue="rag_queue",
)
@register_task_handler
class VectorStorageHandler(ITaskHandler):
    """å‘é‡å­˜å‚¨ä»»åŠ¡å¤„ç†å™¨"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_STORE_VECTORS.value

    @log_execution_time(threshold_ms=800)
    @log_errors()
    async def handle(
        self,
        vectors: List[List[float]],
        contents: List[str],
        document_id: str,
        provider: str = "weaviate",
        collection_name: str = "documents",
        **config,
    ) -> Dict[str, Any]:
        """
        å­˜å‚¨å‘é‡åˆ°å‘é‡æ•°æ®åº“

        Args:
            vectors: å‘é‡åˆ—è¡¨
            contents: å¯¹åº”çš„å†…å®¹åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            provider: å‘é‡å­˜å‚¨æä¾›å•†
            collection_name: é›†åˆåç§°
            **config: å…¶ä»–é…ç½®

        Returns:
            Dict[str, Any]: å­˜å‚¨ç»“æœ
        """
        try:
            with task_context(
                task_id=f"store-{document_id}-vectors",
                task_name="rag.store_vectors",
                queue="rag_queue",
            ):
                logger.info(f"å¼€å§‹å­˜å‚¨å‘é‡: {len(vectors)} ä¸ªå‘é‡")

            from ...rag.vector_store import VectorDocument, VectorStoreProvider
            from ...services.rag_service import create_vector_store

            # åˆ›å»ºå‘é‡å­˜å‚¨æœåŠ¡
            vector_store_config = config.copy()
            vector_store_config["collection_name"] = collection_name

            vector_store = create_vector_store(
                provider=VectorStoreProvider(provider), **vector_store_config
            )

            # åˆå§‹åŒ–æœåŠ¡
            await vector_store.initialize()

            # åˆ›å»ºå‘é‡æ–‡æ¡£
            vector_docs = []
            for i, (vector, content) in enumerate(zip(vectors, contents)):
                vector_doc = VectorDocument(
                    id=f"{document_id}_chunk_{i}",
                    vector=vector,
                    content=content,
                    metadata={
                        "document_id": document_id,
                        "chunk_index": i,
                        "stored_at": datetime.now(timezone.utc).isoformat(),
                    },
                    document_id=document_id,
                    chunk_index=i,
                )
                vector_docs.append(vector_doc)

            # æ‰¹é‡å­˜å‚¨å‘é‡
            result = await vector_store.upsert_vectors(vector_docs)

            # æ¸…ç†æœåŠ¡
            await vector_store.cleanup()

            return {
                "success": True,
                "document_id": document_id,
                "vectors_stored": result.success_count,
                "vectors_failed": result.failed_count,
                "total_vectors": result.total_count,
                "processing_time_ms": result.processing_time_ms,
                "errors": result.errors,
                "provider": provider,
                "collection_name": collection_name,
            }

        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "document_id": document_id,
                "vector_count": len(vectors),
            }


@task_handler(
    schemas.TaskName.RAG_SEMANTIC_SEARCH.value,
    priority=TaskPriority.HIGH,
    max_retries=2,
    timeout=60,
    queue="search_queue",
)
@register_task_handler
class SemanticSearchHandler(ITaskHandler):
    """è¯­ä¹‰æœç´¢ä»»åŠ¡å¤„ç†å™¨"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_SEMANTIC_SEARCH.value

    @log_execution_time(threshold_ms=300)
    @log_errors()
    async def handle(
        self,
        query: str,
        limit: int = 10,
        score_threshold: Optional[float] = None,
        embedding_provider: str = "openai",
        vector_store_provider: str = "weaviate",
        **config,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯­ä¹‰æœç´¢

        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            score_threshold: ç›¸ä¼¼æ€§åˆ†æ•°é˜ˆå€¼
            embedding_provider: åµŒå…¥æœåŠ¡æä¾›å•†
            vector_store_provider: å‘é‡å­˜å‚¨æä¾›å•†
            **config: å…¶ä»–é…ç½®

        Returns:
            Dict[str, Any]: æœç´¢ç»“æœ
        """
        try:
            with task_context(
                task_id=f"search-{hash(query) % 10000}",
                task_name="rag.semantic_search",
                queue="search_queue",
            ):
                logger.info(f"å¼€å§‹è¯­ä¹‰æœç´¢: {query}")

            from ...rag.embedding import EmbeddingProvider
            from ...rag.vector_store import SearchFilter, VectorStoreProvider
            from ...services.rag_service import (
                create_embedding_service,
                create_vector_store,
            )

            # åˆ›å»ºåµŒå…¥æœåŠ¡
            embedding_service = create_embedding_service(
                provider=EmbeddingProvider(embedding_provider),
                **config.get("embedding_config", {}),
            )

            # åˆ›å»ºå‘é‡å­˜å‚¨æœåŠ¡
            vector_store = create_vector_store(
                provider=VectorStoreProvider(vector_store_provider),
                **config.get("vector_store_config", {}),
            )

            # åˆå§‹åŒ–æœåŠ¡
            await embedding_service.initialize()
            await vector_store.initialize()

            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = await embedding_service.generate_single_embedding(query)

            # åˆ›å»ºæœç´¢è¿‡æ»¤å™¨
            filters = SearchFilter()
            if "document_ids" in config:
                filters.document_ids = config["document_ids"]
            if "metadata_filters" in config:
                filters.metadata_filters = config["metadata_filters"]

            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            search_results = await vector_store.search_similar(
                query_vector=query_vector,
                limit=limit,
                score_threshold=score_threshold,
                filters=filters,
            )

            # æ¸…ç†æœåŠ¡
            await embedding_service.cleanup()
            await vector_store.cleanup()

            # æ ¼å¼åŒ–ç»“æœ
            results = []
            for search_result in search_results:
                results.append(
                    {
                        "document_id": search_result.document.document_id,
                        "content": search_result.document.content,
                        "score": search_result.score,
                        "rank": search_result.rank,
                        "metadata": search_result.document.metadata,
                    }
                )

            return {
                "success": True,
                "query": query,
                "results": results,
                "total_results": len(results),
                "embedding_provider": embedding_provider,
                "vector_store_provider": vector_store_provider,
                "search_time": datetime.now(timezone.utc).isoformat(),
            }

        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "query": query,
                "error": str(e),
                "error_type": type(e).__name__,
            }


@task_handler(
    schemas.TaskName.RAG_CLEANUP_DOCUMENT.value,
    priority=TaskPriority.LOW,
    max_retries=2,
    timeout=120,
    queue="cleanup_queue",
)
@register_task_handler
class DocumentCleanupHandler(ITaskHandler):
    """æ–‡æ¡£æ¸…ç†ä»»åŠ¡å¤„ç†å™¨"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_CLEANUP_DOCUMENT.value

    @log_execution_time(threshold_ms=400)
    @log_errors()
    async def handle(
        self, document_id: str, vector_store_provider: str = "weaviate", **config
    ) -> Dict[str, Any]:
        """
        æ¸…ç†æ–‡æ¡£ç›¸å…³çš„å‘é‡å’Œæ•°æ®

        Args:
            document_id: æ–‡æ¡£ID
            vector_store_provider: å‘é‡å­˜å‚¨æä¾›å•†
            **config: å…¶ä»–é…ç½®

        Returns:
            Dict[str, Any]: æ¸…ç†ç»“æœ
        """
        try:
            with task_context(
                task_id=f"cleanup-{document_id}",
                task_name="rag.cleanup_document",
                queue="cleanup_queue",
            ):
                logger.info(f"å¼€å§‹æ¸…ç†æ–‡æ¡£: {document_id}")

            from ...rag.vector_store import VectorStoreProvider
            from ...services.rag_service import create_vector_store

            # åˆ›å»ºå‘é‡å­˜å‚¨æœåŠ¡
            vector_store = create_vector_store(
                provider=VectorStoreProvider(vector_store_provider),
                **config.get("vector_store_config", {}),
            )

            # åˆå§‹åŒ–æœåŠ¡
            await vector_store.initialize()

            # åˆ é™¤æ–‡æ¡£ç›¸å…³å‘é‡
            result = await vector_store.delete_vectors_by_document_id(document_id)

            # æ¸…ç†æœåŠ¡
            await vector_store.cleanup()

            return {
                "success": True,
                "document_id": document_id,
                "vectors_deleted": result.success_count,
                "cleanup_time": datetime.now(timezone.utc).isoformat(),
                "provider": vector_store_provider,
            }

        except Exception as e:
            logger.error(f"æ–‡æ¡£æ¸…ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "document_id": document_id,
                "error": str(e),
                "error_type": type(e).__name__,
            }


@task_handler(
    schemas.TaskName.RAG_PROCESS_DOCUMENT_ASYNC.value,
    priority=TaskPriority.HIGH,
    max_retries=3,
    timeout=600,
    queue="rag_queue",
)
@register_task_handler
class AsyncDocumentProcessingHandler(ITaskHandler):
    """å¼‚æ­¥æ–‡æ¡£RAGå¤„ç†ä»»åŠ¡å¤„ç†å™¨ - åœ¨ç‹¬ç«‹çš„å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­å¤„ç†RAG"""

    @property
    def task_name(self) -> str:
        return schemas.TaskName.RAG_PROCESS_DOCUMENT_ASYNC.value

    @log_execution_time(threshold_ms=1000)
    async def handle(
        self,
        file_id: str = None,
        document_id: str = None,
        file_path: str = None,
        content_type: str = "txt",
        topic_id: Optional[int] = None,
        embedding_provider: str = "openai",
        vector_store_provider: str = "weaviate",
        execution_id: str = None,
        workflow_id: str = None,
        orchestrated: bool = False,
        chunking_config: Optional[Dict[str, Any]] = None,
        **config,
    ) -> Dict[str, Any]:
        """
        åœ¨ç‹¬ç«‹çš„å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­å¤„ç†æ–‡æ¡£RAG

        Args:
            file_id: æ–‡ä»¶ID
            document_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
            content_type: å†…å®¹ç±»å‹
            topic_id: ä¸»é¢˜ID
            embedding_provider: åµŒå…¥æœåŠ¡æä¾›å•†
            vector_store_provider: å‘é‡å­˜å‚¨æä¾›å•†
            execution_id: å·¥ä½œæµæ‰§è¡ŒID (å½“è¢«å·¥ä½œæµè°ƒç”¨æ—¶)
            workflow_id: å·¥ä½œæµID (å½“è¢«å·¥ä½œæµè°ƒç”¨æ—¶)
            orchestrated: æ˜¯å¦ç”±å·¥ä½œæµç¼–æ’è°ƒç”¨
            **config: å…¶ä»–é…ç½®å‚æ•°

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        try:
            # å¤„ç†å·¥ä½œæµç¼–æ’çš„æƒ…å†µ
            if orchestrated and execution_id:
                context_info = f"workflow-{execution_id}"
                logger.info(f"RAGä»»åŠ¡ç”±å·¥ä½œæµç¼–æ’è°ƒç”¨: {execution_id}")
            else:
                context_info = f"standalone-{document_id or file_id}"
                logger.info(f"RAGä»»åŠ¡ç‹¬ç«‹è°ƒç”¨: {document_id or file_id}")

            with task_context(
                task_id=f"rag-async-{context_info}",
                task_name="rag.process_document_async",
                queue="rag_queue",
            ):
                logger.info(f"å¼€å§‹å¼‚æ­¥RAGæ–‡æ¡£å¤„ç†: {document_id} (file: {file_id})")

                # éªŒè¯å¿…è¦å‚æ•°
                if not document_id:
                    raise ValueError("document_id æ˜¯å¿…éœ€çš„å‚æ•°")
                if not file_path:
                    raise ValueError("file_path æ˜¯å¿…éœ€çš„å‚æ•°")

                # è·å–å­˜å‚¨æœåŠ¡å¹¶ä¸‹è½½æ–‡ä»¶
                from ...storage.base import create_storage_service

                storage = create_storage_service()

                # è·å–æ–‡ä»¶ä¿¡æ¯ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡ä»¶åï¼‰
                file_info = await self._get_file_info(file_id) if file_id else {}
                original_filename = file_info.get("original_name")

                # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                temp_file_path = await self._download_file_to_temp(
                    storage, file_path, file_id or document_id
                )

                try:
                    # ä½¿ç”¨å·¥å‚æ¨¡å¼åŠ è½½æ–‡æ¡£ï¼Œä¼ é€’åŸå§‹æ–‡ä»¶å
                    from modules.schemas.enums import ContentType

                    from ...file_loader import detect_content_type, load_document

                    # è½¬æ¢å†…å®¹ç±»å‹
                    try:
                        ct = ContentType(content_type)
                    except ValueError:
                        ct = detect_content_type(temp_file_path)

                    # åŠ è½½æ–‡æ¡£ï¼Œä¼ é€’åŸå§‹æ–‡ä»¶ååˆ°metadataä¸­
                    document = await load_document(
                        temp_file_path, 
                        ct, 
                        original_filename=original_filename
                    )
                    logger.info(
                        f"Document loaded: {document.title}, content length: {len(document.content)}"
                    )

                    # åˆå§‹åŒ–RAGç®¡é“
                    await self._initialize_rag_pipeline(
                        embedding_provider, vector_store_provider
                    )

                    # è®¾ç½®åˆ†å—é…ç½®
                    if chunking_config:
                        self._current_chunking_config = chunking_config
                        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰åˆ†å—é…ç½®: {chunking_config}")

                    # å¤„ç†æ–‡æ¡£
                    result = await self._process_with_rag_pipeline(
                        document, document_id, file_id, topic_id
                    )

                    # æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®
                    await self._update_document_metadata(document_id, result)

                    logger.info(f"å¼‚æ­¥RAGå¤„ç†å®Œæˆ: {document_id}")

                    return {
                        "success": True,
                        "document_id": document_id,
                        "file_id": file_id or document_id,
                        "chunks_created": result.get("chunks_created", 0),
                        "embeddings_generated": result.get("embeddings_generated", 0),
                        "vectors_stored": result.get("vectors_stored", 0),
                        "processing_time_ms": result.get("processing_time_ms", 0),
                        "processed_at": datetime.now(timezone.utc).isoformat(),
                        # å·¥ä½œæµç›¸å…³ä¿¡æ¯
                        "orchestrated": orchestrated,
                        "execution_id": execution_id,
                        "workflow_id": workflow_id,
                        "task_type": "rag.process_document_async",
                        "embedding_provider": embedding_provider,
                        "vector_store_provider": vector_store_provider,
                    }

                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    import os

                    if os.path.exists(temp_file_path):
                        os.remove(temp_file_path)
                        logger.debug(f"ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {temp_file_path}")

        except Exception as e:
            logger.error(f"å¼‚æ­¥RAGæ–‡æ¡£å¤„ç†å¤±è´¥: {document_id}, {e}")

            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†å¤±è´¥
            try:
                await self._update_document_metadata(
                    document_id,
                    {
                        "rag_processing_status": "failed",
                        "rag_error": str(e),
                        "rag_failed_at": datetime.now(timezone.utc).isoformat(),
                    },
                )
            except Exception as update_error:
                logger.error(f"æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {update_error}")

            return {
                "success": False,
                "document_id": document_id,
                "file_id": file_id or document_id,
                "error": str(e),
                "error_type": type(e).__name__,
                "failed_at": datetime.now(timezone.utc).isoformat(),
                # å·¥ä½œæµç›¸å…³ä¿¡æ¯
                "orchestrated": orchestrated,
                "execution_id": execution_id,
                "workflow_id": workflow_id,
                "task_type": "rag.process_document_async",
            }

    async def _download_file_to_temp(
        self, storage, file_path: str, file_id: str
    ) -> str:
        """ä»å­˜å‚¨ä¸­ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
        import os
        import tempfile

        import aiofiles

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        suffix = os.path.splitext(file_path)[1] or ".tmp"
        temp_fd, temp_file_path = tempfile.mkstemp(
            suffix=suffix, prefix=f"rag_async_{file_id}_"
        )
        os.close(temp_fd)

        try:
            # ä»å­˜å‚¨ä¸­è¯»å–æ–‡ä»¶å†…å®¹
            file_content = await storage.read_file(file_path)

            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            async with aiofiles.open(temp_file_path, "wb") as f:
                await f.write(file_content)

            logger.debug(f"æ–‡ä»¶å·²ä¸‹è½½åˆ°ä¸´æ—¶ä½ç½®: {temp_file_path}")
            return temp_file_path

        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
            raise Exception(f"ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

    async def _initialize_rag_pipeline(
        self, embedding_provider: str, vector_store_provider: str
    ):
        """åˆå§‹åŒ–RAGç®¡é“"""
        try:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ ¹æ®éœ€è¦åˆå§‹åŒ–å®Œæ•´çš„RAGç®¡é“
            logger.info(
                f"RAGç®¡é“é…ç½®: embedding={embedding_provider}, vector_store={vector_store_provider}"
            )

            # TODO: æ ¹æ®å®é™…éœ€è¦åˆå§‹åŒ–åµŒå…¥æœåŠ¡å’Œå‘é‡å­˜å‚¨
            # ç›®å‰ç”±äºCelery workerç¯å¢ƒçš„é™åˆ¶ï¼Œè¿™é‡Œåªåšé…ç½®æ£€æŸ¥

        except Exception as e:
            logger.warning(f"RAGç®¡é“åˆå§‹åŒ–è­¦å‘Š: {e}")

    async def _process_with_rag_pipeline(
        self, document, document_id: str, file_id: str, topic_id: Optional[int]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨RAGç®¡é“å¤„ç†æ–‡æ¡£"""
        try:
            # è·å–RAGå¤„ç†å™¨å®ä¾‹
            rag_processor = await self._get_rag_processor()
            
            # å‡†å¤‡åˆ†å—é…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            chunking_config = getattr(self, '_chunking_config', {})
            
            # å¦‚æœä»å¤–éƒ¨ä¼ å…¥äº†åˆ†å—é…ç½®ï¼Œåˆ™ä½¿ç”¨å®ƒ
            if hasattr(self, '_current_chunking_config') and self._current_chunking_config:
                chunking_config = self._current_chunking_config
            
            logger.info(f"å¼€å§‹RAGç®¡é“å¤„ç†æ–‡æ¡£: {document_id}")
            
            # è°ƒç”¨çœŸå®çš„RAGå¤„ç†å™¨
            result = await rag_processor.process_document(
                document=document,
                chunking_config=chunking_config,
                topic_id=str(topic_id) if topic_id else None,
                file_id=file_id
            )
            
            # è½¬æ¢ç»“æœæ ¼å¼
            return {
                "chunks_created": result.chunks_created,
                "embeddings_generated": result.embeddings_generated,
                "vectors_stored": result.vectors_stored,
                "processing_time_ms": result.processing_time_ms,
                "status": "completed" if result.status.value == "completed" else "failed",
                "strategy_used": result.strategy_used,
                "quality_score": result.quality_score,
                "error": result.error_message,
                "stage_details": result.stage_details,
            }

        except Exception as e:
            logger.error(f"RAGç®¡é“å¤„ç†å¤±è´¥: {e}")
            return {
                "chunks_created": 0,
                "embeddings_generated": 0,
                "vectors_stored": 0,
                "processing_time_ms": 0,
                "status": "failed",
                "error": str(e),
            }
    
    async def _get_rag_processor(self):
        """è·å–RAGå¤„ç†å™¨å®ä¾‹"""
        from modules.services.rag_processor import RAGProcessor, RAGProcessorConfig
        from modules.embedding.openai_service import OpenAIEmbeddingService
        from modules.vector_store.weaviate_service import WeaviateVectorStore
        from config import get_config
        
        config = get_config()
        
        # åˆ›å»ºåµŒå…¥æœåŠ¡
        openai_config = config.ai.embedding.openai
        embedding_service = OpenAIEmbeddingService(
            api_key=getattr(config, 'openai_api_key', None) or openai_config.api_key,
            model="text-embedding-3-small",
            max_batch_size=50,
            http_proxy=openai_config.http_proxy,
            https_proxy=openai_config.https_proxy,
            api_base=openai_config.api_base,
        )
        
        # åˆ›å»ºå‘é‡å­˜å‚¨æœåŠ¡
        vector_store = WeaviateVectorStore(
            url=getattr(config, 'weaviate_url', None) or config.vector_db.weaviate_url or "http://localhost:8080",
            api_key=getattr(config, 'weaviate_api_key', None),
            batch_size=50,
        )
        
        # åˆ›å»ºRAGå¤„ç†å™¨é…ç½®
        rag_config = RAGProcessorConfig(
            embedding_provider="openai",
            vector_store_provider="weaviate",
            collection_name="documents",
            batch_size=50,
            max_concurrent_embeddings=3,
        )
        
        # åˆ›å»ºRAGå¤„ç†å™¨
        rag_processor = RAGProcessor(
            config=rag_config,
            embedding_service=embedding_service,
            vector_store=vector_store,
        )
        
        return rag_processor

    async def _update_document_metadata(
        self, document_id: str, metadata: Dict[str, Any]
    ):
        """æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®"""
        try:
            from config import get_config

            from ...database.connection import DatabaseConnection
            from ...repository import DocumentRepository

            # åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥
            config = get_config()
            db = DatabaseConnection(config.database.url)
            await db.initialize()

            async with db.get_session() as session:
                doc_repo = DocumentRepository(session)

                # è·å–ç°æœ‰æ–‡æ¡£
                document = await doc_repo.get_document_by_id(document_id)
                if document:
                    # æ›´æ–°å…ƒæ•°æ®
                    current_metadata = document.doc_metadata or {}
                    current_metadata.update(
                        {
                            "rag_processing_status": metadata.get(
                                "status", "completed"
                            ),
                            "rag_chunks_created": metadata.get("chunks_created", 0),
                            "rag_embeddings_generated": metadata.get(
                                "embeddings_generated", 0
                            ),
                            "rag_vectors_stored": metadata.get("vectors_stored", 0),
                            "rag_processing_time_ms": metadata.get(
                                "processing_time_ms", 0
                            ),
                            "rag_processed_at": datetime.now(timezone.utc).isoformat(),
                            **metadata,
                        }
                    )

                    # æ›´æ–°æ–‡æ¡£
                    await doc_repo.update_document_metadata(
                        document_id, current_metadata
                    )
                    await session.commit()

                    logger.info(f"æ–‡æ¡£å…ƒæ•°æ®å·²æ›´æ–°: {document_id}")
                else:
                    logger.warning(f"æ–‡æ¡£æœªæ‰¾åˆ°: {document_id}")

            await db.close()

        except Exception as e:
            logger.error(f"æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®å¤±è´¥: {e}")

    async def _get_file_info(self, file_id: str) -> Dict[str, Any]:
        """ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            from ...database import get_db_session
            from ...repository import FileRepository
            
            session_gen = get_db_session()
            session = await session_gen.__anext__()
            try:
                file_repo = FileRepository(session)
                file_record = await file_repo.get_file_by_id(file_id)
                
                if file_record:
                    return {
                        "original_name": file_record.original_name,
                        "filename": file_record.filename,
                        "content_type": file_record.content_type,
                        "file_size": file_record.file_size,
                    }
                else:
                    logger.warning(f"æ–‡ä»¶è®°å½•æœªæ‰¾åˆ°: {file_id}")
                    return {}
            finally:
                await session.close()
                    
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {file_id}, {e}")
            return {}


logger.info("RAGä»»åŠ¡å¤„ç†å™¨æ¨¡å—å·²åŠ è½½")
