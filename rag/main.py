"""
RAGç³»ç»Ÿä¸»ç¨‹åº
æ¼”ç¤ºåŸºç¡€RAGåŠŸèƒ½çš„å®Œæ•´æµç¨‹
"""

import asyncio
import time
from pathlib import Path
from typing import List, Optional

# å¯¼å…¥åŸºç¡€æ¥å£
from rag.models import RAGConfig, Document, DocumentStatus
from rag.file_loader.base import BaseFileLoader
from rag.file_loader.text_loader import TextFileLoader, MarkdownLoader
from rag.document_spliter.base import BaseDocumentSplitter, FixedSizeSplitter, SentenceSplitter
from rag.vector_store import BaseVectorStore, InMemoryVectorStore
from rag.document_repository import BaseDocumentRepository, InMemoryDocumentRepository
from rag.retriever import BaseRetriever, SemanticRetriever, HybridRetriever
# RouteråŠŸèƒ½å·²é›†æˆåˆ°Retrieverä¸­


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config: Optional[RAGConfig] = None):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            config: RAGé…ç½®
        """
        self.config = config or RAGConfig()
        
        # éªŒè¯é…ç½®
        config_errors = self.config.validate()
        if config_errors:
            raise ValueError(f"Configuration errors: {'; '.join(config_errors)}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.file_loaders = self._initialize_file_loaders()
        self.document_splitter = self._initialize_document_splitter()
        self.vector_store = self._initialize_vector_store()
        self.document_repository = self._initialize_document_repository()
        self.retriever = self._initialize_retriever()
        
        print(f"âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ”¯æŒæ–‡ä»¶æ ¼å¼: {self.config.supported_file_types}")
        print(f"   - å—å¤§å°: {self.config.chunk_size}")
        print(f"   - å—é‡å : {self.config.chunk_overlap}")
        print(f"   - Top-K: {self.config.top_k}")
    
    def _initialize_file_loaders(self) -> dict:
        """åˆå§‹åŒ–æ–‡ä»¶åŠ è½½å™¨"""
        loaders = {
            'text': TextFileLoader(),
            'markdown': MarkdownLoader()
        }
        return loaders
    
    def _initialize_document_splitter(self) -> BaseDocumentSplitter:
        """åˆå§‹åŒ–æ–‡æ¡£åˆ†å‰²å™¨"""
        return SentenceSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap
        )
    
    def _initialize_vector_store(self) -> BaseVectorStore:
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        return InMemoryVectorStore(self.config.vector_store_config.__dict__)
    
    def _initialize_document_repository(self) -> BaseDocumentRepository:
        """åˆå§‹åŒ–æ–‡æ¡£ä»“å‚¨"""
        return InMemoryDocumentRepository()
    
    def _initialize_retriever(self) -> BaseRetriever:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        retriever_config = {
            'top_k': self.config.top_k,
            'similarity_threshold': self.config.similarity_threshold,
            'enable_reranking': self.config.enable_reranking
        }
        
        return SemanticRetriever(
            vector_store=self.vector_store,
            document_repository=self.document_repository,
            config=retriever_config
        )
    
    
    def _get_appropriate_loader(self, file_path: str) -> BaseFileLoader:
        """æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨"""
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext in ['.md', '.markdown']:
            return self.file_loaders['markdown']
        elif file_ext in ['.txt', '.text']:
            return self.file_loaders['text']
        else:
            # é»˜è®¤ä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨
            return self.file_loaders['text']
    
    async def process_document(self, file_path: str) -> str:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ–‡æ¡£ID
        """
        start_time = time.time()
        
        try:
            print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
            
            # 1. é€‰æ‹©åˆé€‚çš„æ–‡ä»¶åŠ è½½å™¨
            loader = self._get_appropriate_loader(file_path)
            
            # 2. åŠ è½½æ–‡æ¡£
            print("   ğŸ”„ åŠ è½½æ–‡æ¡£...")
            document = await loader.load(file_path)
            print(f"   âœ… æ–‡æ¡£åŠ è½½å®Œæˆ: {document.title}")
            print(f"      - æ–‡ä»¶å¤§å°: {document.file_size} bytes")
            print(f"      - å­—ç¬¦æ•°: {len(document.content)}")
            print(f"      - çŠ¶æ€: {document.status.value}")
            
            # 3. å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
            print("   ğŸ”„ å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®...")
            doc_id = await self.document_repository.save(document)
            
            # 4. åˆ†å‰²æ–‡æ¡£
            print("   ğŸ”„ åˆ†å‰²æ–‡æ¡£...")
            chunks = await self.document_splitter.split(document)
            print(f"   âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
            
            if chunks:
                avg_chunk_size = sum(len(chunk.content) for chunk in chunks) / len(chunks)
                print(f"      - å¹³å‡å—å¤§å°: {avg_chunk_size:.0f} å­—ç¬¦")
            
            # 5. å­˜å‚¨å‘é‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ç”ŸæˆçœŸå®çš„åµŒå…¥å‘é‡ï¼‰
            print("   ğŸ”„ å­˜å‚¨å‘é‡...")
            # ä¸ºæ¯ä¸ªå—ç”Ÿæˆç®€å•çš„ä¼ªå‘é‡ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
            for chunk in chunks:
                chunk.embedding = [0.1] * 768  # ç®€å•çš„å ä½å‘é‡
            
            vector_ids = await self.vector_store.store_chunks(chunks)
            print(f"   âœ… å‘é‡å­˜å‚¨å®Œæˆ: {len(vector_ids)} ä¸ªå‘é‡")
            
            # 6. æ›´æ–°æ–‡æ¡£çŠ¶æ€
            await self.document_repository.update_status(doc_id, DocumentStatus.COMPLETED)
            
            processing_time = (time.time() - start_time) * 1000
            print(f"   â±ï¸  å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ms")
            
            return doc_id
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")
            print(f"   â±ï¸  å¤±è´¥è€—æ—¶: {processing_time:.2f}ms")
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºé”™è¯¯
            if 'document' in locals():
                await self.document_repository.update_status(document.id, DocumentStatus.ERROR)
            
            raise
    
    async def query(self, question: str) -> dict:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            
        Returns:
            dict: æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        
        try:
            print(f"\nğŸ” æ‰§è¡ŒæŸ¥è¯¢: {question}")
            
            # 1. æ‰§è¡Œæ™ºèƒ½æ£€ç´¢ï¼ˆåŒ…å«å†…ç½®è·¯ç”±å†³ç­–ï¼‰
            print("   ğŸ”„ æ‰§è¡Œæ£€ç´¢...")
            result = await self.retriever.retrieve(question, top_k=self.config.top_k)
            
            query_time = (time.time() - start_time) * 1000
            
            print(f"   âœ… æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(result.chunks)} ä¸ªç›¸å…³å—")
            print(f"   ğŸ¯ ä½¿ç”¨ç­–ç•¥: {result.metadata.get('strategy', 'unknown')}")
            print(f"   â±ï¸  æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ms")
            
            # 2. æ ¼å¼åŒ–ç»“æœ
            formatted_result = {
                'query': question,
                'answer_chunks': [],
                'total_found': len(result.chunks),
                'query_time_ms': query_time,
                'strategy_used': result.metadata.get('strategy', 'unknown'),
                'preprocessing': result.metadata.get('pre_processing', {}),
                'postprocessing': result.metadata.get('post_processing', {})
            }
            
            for i, (chunk, score) in enumerate(zip(result.chunks, result.relevance_scores)):
                formatted_result['answer_chunks'].append({
                    'rank': i + 1,
                    'content': chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content,
                    'score': score,
                    'document_id': chunk.document_id,
                    'chunk_index': chunk.chunk_index
                })
            
            return formatted_result
            
        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            print(f"   â±ï¸  å¤±è´¥è€—æ—¶: {query_time:.2f}ms")
            
            return {
                'query': question,
                'error': str(e),
                'query_time_ms': query_time
            }
    
    async def get_system_status(self) -> dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        # è·å–å„ç»„ä»¶çŠ¶æ€
        vector_store_info = await self.vector_store.get_collection_info()
        document_repository_stats = await self.document_repository.get_statistics()
        
        return {
            'vector_store': vector_store_info,
            'document_repository': document_repository_stats,
            'config': {
                'chunk_size': self.config.chunk_size,
                'chunk_overlap': self.config.chunk_overlap,
                'top_k': self.config.top_k,
                'similarity_threshold': self.config.similarity_threshold
            }
        }


async def main():
    """ä¸»ç¨‹åºæ¼”ç¤º"""
    print("ğŸš€ å¯åŠ¨RAGç³»ç»Ÿæ¼”ç¤º")
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    config = RAGConfig(
        chunk_size=500,
        chunk_overlap=50,
        top_k=3,
        similarity_threshold=0.1  # è®¾ç½®è¾ƒä½çš„é˜ˆå€¼ä»¥ä¾¿æ¼”ç¤º
    )
    
    rag_system = RAGSystem(config)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£æ–‡ä»¶
    test_files = []
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£1
    test_file1 = "test_document1.txt"
    with open(test_file1, 'w', encoding='utf-8') as f:
        f.write("""
äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨ã€‚
è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œå®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
""")
    test_files.append(test_file1)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£2
    test_file2 = "test_document2.md"
    with open(test_file2, 'w', encoding='utf-8') as f:
        f.write("""
# RAGç³»ç»Ÿä»‹ç»

## ä»€ä¹ˆæ˜¯RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆçš„äººå·¥æ™ºèƒ½æ–¹æ³•ã€‚
å®ƒé¦–å…ˆä»å¤§å‹çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥ç”Ÿæˆå›ç­”ã€‚

## RAGçš„ä¼˜åŠ¿

- **å‡†ç¡®æ€§**: åŸºäºäº‹å®ä¿¡æ¯ç”Ÿæˆå›ç­”
- **å¯è§£é‡Šæ€§**: å¯ä»¥è¿½æº¯ä¿¡æ¯æ¥æº
- **æ—¶æ•ˆæ€§**: å¯ä»¥è·å–æœ€æ–°ä¿¡æ¯
- **æˆæœ¬æ•ˆç›Š**: ç›¸æ¯”è®­ç»ƒå¤§å‹æ¨¡å‹æ›´ç»æµ

## åº”ç”¨åœºæ™¯

RAGç³»ç»Ÿå¹¿æ³›åº”ç”¨äºï¼š
1. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
2. æ–‡æ¡£æŸ¥è¯¢ä¸åˆ†æ
3. çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
4. å®¢æˆ·æœåŠ¡æœºå™¨äºº
""")
    test_files.append(test_file2)
    
    try:
        # å¤„ç†æ–‡æ¡£
        for file_path in test_files:
            doc_id = await rag_system.process_document(file_path)
            print(f"ğŸ“‹ æ–‡æ¡£ID: {doc_id}")
        
        # æŸ¥è¯¢æ¼”ç¤º
        queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "RAGæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
            "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
            "RAGç³»ç»Ÿæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ"
        ]
        
        for query in queries:
            result = await rag_system.query(query)
            
            print("\n" + "="*60)
            print(f"â“ é—®é¢˜: {result['query']}")
            
            if 'error' in result:
                print(f"âŒ é”™è¯¯: {result['error']}")
            else:
                print(f"ğŸ“Š æ‰¾åˆ° {result['total_found']} ä¸ªç›¸å…³ç»“æœ")
                print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥: {result['strategy_used']}")
                
                # æ˜¾ç¤ºé¢„å¤„ç†ä¿¡æ¯
                if result['preprocessing']:
                    preprocessing = result['preprocessing']
                    if 'query_type' in preprocessing:
                        print(f"ğŸ§  æŸ¥è¯¢ç±»å‹: {preprocessing['query_type']}")
                
                # æ˜¾ç¤ºåå¤„ç†ä¿¡æ¯
                if result['postprocessing']:
                    postprocessing = result['postprocessing']
                    if 'reranked' in postprocessing:
                        print(f"ğŸ”„ é‡æ’åº: {'æ˜¯' if postprocessing['reranked'] else 'å¦'}")
                    if 'total_compressed' in postprocessing and postprocessing['total_compressed'] > 0:
                        print(f"ğŸ“¦ å‹ç¼©äº† {postprocessing['total_compressed']} ä¸ªç»“æœ")
                
                for chunk_info in result['answer_chunks']:
                    print(f"\nğŸ“„ ç»“æœ {chunk_info['rank']} (ç›¸å…³æ€§: {chunk_info['score']:.3f}):")
                    print(f"   {chunk_info['content']}")
        
        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        print("\n" + "="*60)
        print("ğŸ“ˆ ç³»ç»ŸçŠ¶æ€:")
        status = await rag_system.get_system_status()
        
        print(f"   å‘é‡å­˜å‚¨: {status['vector_store']['total_chunks']} ä¸ªå—")
        print(f"   æ–‡æ¡£æ•°é‡: {status['document_repository']['total_documents']} ä¸ª")
        print(f"   é…ç½®ä¿¡æ¯: å—å¤§å°={status['config']['chunk_size']}, Top-K={status['config']['top_k']}")
        
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        for file_path in test_files:
            try:
                Path(file_path).unlink()
                print(f"ğŸ—‘ï¸  å·²åˆ é™¤æµ‹è¯•æ–‡ä»¶: {file_path}")
            except:
                pass
    
    print("\nâœ… RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())