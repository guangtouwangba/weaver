version: '3.8'

services:
  # Hybrid Job Scheduler - Multi-threaded scheduler
  hybrid-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hybrid-job-scheduler
    environment:
      # Database Configuration
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      
      # OpenAI API Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-3.5-turbo}
      
      # DeepSeek API Configuration  
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DEEPSEEK_MODEL=${DEEPSEEK_MODEL:-deepseek-coder}
      
      # Anthropic API Configuration
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-sonnet-20240229}
      
      # Application Settings
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      
    volumes:
      # Configuration files
      - ./config.yaml:/app/config.yaml:ro
      - ./job_schedules.yaml:/app/job_schedules.yaml:ro
      
      # Data persistence
      - ./data:/app/data
      - ./downloaded_papers:/app/downloaded_papers
      - ./logs:/app/logs
      
      # SQLite database (if using SQLite instead of Supabase)
      - ./papers.db:/app/papers.db
      
    command: ["hybrid-scheduler"]
    restart: unless-stopped
    
    # Health check
    healthcheck:
      test: ["CMD", "python", "hybrid_job_scheduler.py", "--status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: SQLite backup service (only needed if using SQLite)
  sqlite-backup:
    image: alpine:3.18
    container_name: sqlite-backup
    volumes:
      - ./data:/data
      - ./backups:/backups
    command: >
      sh -c "
      while true; do
        if [ -f /data/papers.db ]; then
          echo '[$(date)] Creating SQLite backup...'
          cp /data/papers.db /backups/papers_$(date +%Y%m%d_%H%M%S).db
          # Keep only last 7 backups
          ls -t /backups/papers_*.db | tail -n +8 | xargs -r rm
        fi
        sleep 86400  # 24 hours
      done
      "
    restart: unless-stopped
    profiles: ["backup"]  # Only start with: docker-compose --profile backup up

  # Optional: Monitoring service
  scheduler-monitor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scheduler-monitor
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - PYTHONPATH=/app
    volumes:
      - ./config.yaml:/app/config.yaml:ro
      - ./logs:/app/logs
    command: >
      sh -c "
      while true; do
        echo '=== Hybrid Scheduler Status ===' 
        python hybrid_job_scheduler.py --status || echo 'Status check failed'
        echo
        echo '=== Job Statistics ==='
        python cloud_job_manager.py stats || echo 'Stats unavailable'
        echo
        sleep 300  # 5 minutes
      done
      "
    restart: unless-stopped
    profiles: ["monitor"]  # Only start with: docker-compose --profile monitor up

# Named volumes for data persistence
volumes:
  scheduler_data:
    driver: local
  scheduler_logs:
    driver: local

# Networks
networks:
  default:
    name: hybrid-scheduler-network