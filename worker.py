#!/usr/bin/env python3
"""
Celery Worker å¯åŠ¨è„šæœ¬ - æ¶æ„ä¼˜åŒ–ç‰ˆ

è¿™ä¸ªè„šæœ¬ç”¨äºå¯åŠ¨ Celery Worker è¿›ç¨‹æ¥ç›‘å¬å’Œå¤„ç†å¼‚æ­¥ä»»åŠ¡ã€‚
æ”¯æŒæ–°çš„ä»»åŠ¡åˆ†ç¦»æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š
1. ç‹¬ç«‹çš„æ–‡æ¡£åˆ›å»ºä»»åŠ¡ (document_queue)
2. ç‹¬ç«‹çš„RAGå¤„ç†ä»»åŠ¡ (rag_queue)
3. æ–‡ä»¶å¤„ç†ä»»åŠ¡ (file_queue)
4. å·¥ä½œæµåè°ƒä»»åŠ¡ (workflow_queue)

Worker è¿›ç¨‹ä¼šï¼š
1. è¿æ¥åˆ° Redis æ¶ˆæ¯é˜Ÿåˆ—
2. ç›‘å¬æŒ‡å®šé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
3. æ ¹æ®ä»»åŠ¡è·¯ç”±è§„åˆ™åˆ†å‘ä»»åŠ¡åˆ°å¯¹åº”çš„å¤„ç†å™¨
4. æ‰§è¡Œä»»åŠ¡å¹¶è¿”å›ç»“æœ

ä½¿ç”¨æ–¹æ³•:
python worker.py [é€‰é¡¹]

ç¤ºä¾‹:
python worker.py --loglevel=info                    # é»˜è®¤ç»Ÿä¸€worker
python worker.py --specialized=unified              # æ˜ç¡®æŒ‡å®šç»Ÿä¸€worker
python worker.py --specialized=document             # ä¸“ç”¨æ–‡æ¡£å¤„ç†worker
python worker.py --specialized=rag                  # ä¸“ç”¨RAGå¤„ç†worker
python worker.py --specialized=file                 # ä¸“ç”¨æ–‡ä»¶å¤„ç†worker
python worker.py --queues=document_queue,rag_queue  # è‡ªå®šä¹‰é˜Ÿåˆ—
"""

import sys
import os
import logging
import argparse
import platform
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# æ˜¾å¼åŠ è½½.envæ–‡ä»¶
from dotenv import load_dotenv
env_file = project_root / ".env"
if env_file.exists():
    load_dotenv(env_file)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶: {env_file}")
else:
    print(f"âš ï¸  ç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_file}")

from modules.services.task_service import CeleryTaskService
from config import get_config

# ç¡®ä¿æ–°çš„ä»»åŠ¡å¤„ç†å™¨è¢«åŠ è½½
try:
    # æ˜¾å¼å¯¼å…¥æ‰€æœ‰ä»»åŠ¡å¤„ç†å™¨ä»¥ç¡®ä¿å®ƒä»¬è¢«æ³¨å†Œ
    import modules.tasks.handlers.file_handlers
    import modules.tasks.handlers.rag_handlers  
    import modules.tasks.handlers.document_handlers
    import modules.tasks.handlers.summary_handlers  # æ–°å¢æ‘˜è¦å¤„ç†å™¨
    print("âœ… æ–°æ¶æ„ä»»åŠ¡å¤„ç†å™¨åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸  æ–°æ¶æ„ä»»åŠ¡å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
    print("   å°†ä½¿ç”¨åŸæœ‰ä»»åŠ¡å¤„ç†å™¨")

logger = logging.getLogger(__name__)


async def initialize_vector_collections():
    """åˆå§‹åŒ–å‘é‡å­˜å‚¨é›†åˆï¼Œåœ¨Workerå¯åŠ¨æ—¶åˆ›å»º"""
    try:
        from modules.vector_store.weaviate_service import WeaviateVectorStore
        from modules.vector_store.base import VectorStoreConfig, VectorStoreProvider, SimilarityMetric
        
        config = get_config()
        
        # åˆ›å»ºWeaviateVectorStoreå®ä¾‹ï¼Œå¯ç”¨é›†åˆåˆ›å»º
        weaviate_store = WeaviateVectorStore(
            url=getattr(config, 'weaviate_url', None) or 
                config.vector_db.weaviate_url or 
                "http://localhost:8080",
            api_key=getattr(config, 'weaviate_api_key', None),
            create_collections_on_init=True  # å¯åŠ¨æ—¶åˆ›å»ºé›†åˆ
        )
        
        # åˆå§‹åŒ–è¿æ¥å¹¶åˆ›å»ºé›†åˆ
        await weaviate_store.initialize()
        
        print("ğŸ‰ å‘é‡å­˜å‚¨æœåŠ¡å·²å¯åŠ¨ï¼Œé›†åˆå·²å‡†å¤‡å°±ç»ª")
        
        # æ¸…ç†è¿æ¥
        await weaviate_store.cleanup()
        
    except ImportError as e:
        print(f"å‘é‡å­˜å‚¨æ¨¡å—ä¸å¯ç”¨: {e}")
        raise
    except Exception as e:
        print(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


def setup_macos_compatibility():
    """è®¾ç½®macOSå…¼å®¹æ€§é…ç½®ä»¥é¿å…forkå®‰å…¨é—®é¢˜"""
    if platform.system() == "Darwin":  # macOS
        print("ğŸ æ£€æµ‹åˆ°macOSç³»ç»Ÿï¼Œè®¾ç½®forkå®‰å…¨é…ç½®...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥é¿å…CoreFoundation forké—®é¢˜
        os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"
        os.environ["PYTHONUNBUFFERED"] = "1"  # ç¡®ä¿è¾“å‡ºå®æ—¶æ˜¾ç¤º
        
        print("  âœ… å·²è®¾ç½® OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES")
        print("  âœ… å·²è®¾ç½® PYTHONUNBUFFERED=1")
        
        return True
    return False


def get_safe_pool_type(requested_pool: str) -> str:
    """ä¸ºmacOSè¿”å›å®‰å…¨çš„worker poolç±»å‹"""
    if platform.system() == "Darwin":  # macOS
        if requested_pool == "prefork":
            print("âš ï¸  macOSç³»ç»Ÿ: å°†prefork poolæ”¹ä¸ºthreadsä»¥é¿å…forkå®‰å…¨é—®é¢˜")
            return "threads"
        elif requested_pool in ["eventlet", "gevent"]:
            print(f"âš ï¸  macOSç³»ç»Ÿ: {requested_pool} poolå¯èƒ½æœ‰å…¼å®¹é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨threads")
    
    return requested_pool


def create_celery_app():
    """åˆ›å»ºå¹¶é…ç½® Celery åº”ç”¨"""
    config = get_config()

    # åˆ›å»ºä»»åŠ¡æœåŠ¡å®ä¾‹
    task_service = CeleryTaskService(
        broker_url=config.celery.broker_url,
        result_backend=config.celery.result_backend,
        app_name=config.celery.app_name,
    )

    # åŒæ­¥åˆå§‹åŒ–ï¼ˆæ³¨å†Œä»»åŠ¡å¤„ç†å™¨ï¼‰
    import asyncio

    asyncio.run(task_service.initialize())
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨é›†åˆ (fail fast)
    try:
        asyncio.run(initialize_vector_collections())
        print("âœ… Weaviateé›†åˆåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Weaviateé›†åˆåˆå§‹åŒ–å¤±è´¥: {e}")
        print("âš ï¸  Workerå°†åœ¨æ²¡æœ‰å‘é‡å­˜å‚¨çš„æƒ…å†µä¸‹å¯åŠ¨")

    # æ›´æ–°ä»»åŠ¡è·¯ç”±é…ç½®ä»¥æ”¯æŒæ–°æ¶æ„
    app = task_service.app
    app.conf.update(
        task_routes={
            # æ–‡æ¡£ç›¸å…³ä»»åŠ¡
            "document.create": {"queue": "document_queue"},
            "document.update_metadata": {"queue": "document_queue"},
            # RAGç›¸å…³ä»»åŠ¡
            "rag.process_document_async": {"queue": "rag_queue"},
            "rag.process_document": {"queue": "rag_queue"},
            "rag.generate_embeddings": {"queue": "rag_queue"},
            "rag.store_vectors": {"queue": "rag_queue"},
            "rag.semantic_search": {"queue": "rag_queue"},
            "rag.cleanup_document": {"queue": "rag_queue"},
            # æ‘˜è¦ç›¸å…³ä»»åŠ¡ (æ–°å¢)
            "summary.generate_document": {"queue": "summary_queue"},
            "summary.update_index": {"queue": "summary_queue"},
            # æ–‡ä»¶å¤„ç†ä»»åŠ¡
            "file_upload_confirm": {"queue": "file_queue"},
            "TaskName.FILE_UPLOAD_CONFIRM": {"queue": "file_queue"},
            "file.analyze_content": {"queue": "file_queue"},
            "file.cleanup_temp": {"queue": "file_queue"},
            "file.convert_format": {"queue": "file_queue"},
            "file.workflow_status": {"queue": "file_queue"},
            "file.cancel_workflow": {"queue": "file_queue"},
            # é€šé…ç¬¦è·¯ç”±ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            "workflow.*": {"queue": "workflow_queue"},
            "rag.*": {"queue": "rag_queue"},
            "summary.*": {"queue": "summary_queue"},  # æ–°å¢æ‘˜è¦é€šé…ç¬¦è·¯ç”±
            "file.*": {"queue": "file_queue"},
            "notification.*": {"queue": "notification_queue"},
        },
        # é˜Ÿåˆ—ä¼˜å…ˆçº§é…ç½®
        task_queue_max_priority=10,
        task_default_priority=5,
        worker_prefetch_multiplier=1,
        # ç»“æœé…ç½®
        result_expires=3600,  # ç»“æœä¿å­˜1å°æ—¶
        task_track_started=True,
        task_send_events=True,
        # åºåˆ—åŒ–é…ç½®
        task_serializer="json",
        result_serializer="json",
        accept_content=["json", "pickle"],  # ä¸´æ—¶å…è®¸pickleä»¥å…¼å®¹mingleè¿‡ç¨‹
        # æ—¶é—´é…ç½®
        timezone="UTC",
        enable_utc=True,
    )

    return app


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Celery Worker - æ¶æ„ä¼˜åŒ–ç‰ˆ")

    parser.add_argument(
        "--loglevel",
        default="info",
        choices=["debug", "info", "warning", "error"],
        help="æ—¥å¿—çº§åˆ« (default: info)",
    )

    parser.add_argument(
        "--concurrency", type=int, default=None, help="å¹¶å‘æ•° (default: ä»é…ç½®æ–‡ä»¶è¯»å–)"
    )

    parser.add_argument(
        "--queues", default=None, help="ç›‘å¬çš„é˜Ÿåˆ—ï¼Œé€—å·åˆ†éš” (default: æ‰€æœ‰é˜Ÿåˆ—)"
    )

    parser.add_argument(
        "--specialized",
        choices=["document", "rag", "summary", "file", "workflow", "unified"],
        help="workerç±»å‹: document, rag, summary, file, workflow, unified",
    )

    parser.add_argument(
        "--max-tasks-per-child",
        type=int,
        default=None,
        help="æ¯ä¸ªworkerè¿›ç¨‹å¤„ç†çš„æœ€å¤§ä»»åŠ¡æ•°",
    )

    parser.add_argument(
        "--pool",
        default="prefork",
        choices=["prefork", "eventlet", "gevent", "solo", "threads"],
        help="Workeræ± ç±»å‹ (default: prefork, macOSè‡ªåŠ¨è½¬æ¢ä¸ºthreads)",
    )

    return parser.parse_args()


def get_queue_config(specialized=None, custom_queues=None):
    """è·å–é˜Ÿåˆ—é…ç½®"""

    # æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—
    all_queues = [
        "default",  # é»˜è®¤é˜Ÿåˆ—
        "document_queue",  # æ–‡æ¡£åˆ›å»ºé˜Ÿåˆ—
        "rag_queue",  # RAGå¤„ç†é˜Ÿåˆ—
        "summary_queue",  # æ‘˜è¦å¤„ç†é˜Ÿåˆ— (æ–°å¢)
        "file_queue",  # æ–‡ä»¶å¤„ç†é˜Ÿåˆ—
        "workflow_queue",  # å·¥ä½œæµé˜Ÿåˆ—
        "notification_queue",  # é€šçŸ¥é˜Ÿåˆ—
    ]

    # ä¸“ç”¨workeré…ç½®ï¼ˆä¿ç•™å‘åå…¼å®¹æ€§ï¼‰
    specialized_configs = {
        "document": {
            "queues": ["document_queue", "default"],
            "concurrency": 2,  # é™ä½å¹¶å‘æ•°
            "description": "ä¸“ç”¨æ–‡æ¡£å¤„ç†Worker",
        },
        "rag": {
            "queues": ["rag_queue"],
            "concurrency": 1,  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…å†…å­˜é—®é¢˜
            "description": "ä¸“ç”¨RAGå¤„ç†Worker",
        },
        "summary": {  # æ–°å¢æ‘˜è¦ä¸“ç”¨workeré…ç½®
            "queues": ["summary_queue"],
            "concurrency": 1,  # æ‘˜è¦ç”Ÿæˆé€šå¸¸æ˜¯CPUå¯†é›†å‹
            "description": "ä¸“ç”¨æ‘˜è¦ç”ŸæˆWorker",
        },
        "file": {
            "queues": ["file_queue", "default"],
            "concurrency": 1,  # é™ä½å¹¶å‘æ•°ï¼Œé¿å…PDFå¤„ç†å†…å­˜é—®é¢˜
            "description": "ä¸“ç”¨æ–‡ä»¶å¤„ç†Worker",
        },
        "workflow": {
            "queues": ["workflow_queue", "default"],
            "concurrency": 1,  # é™ä½å¹¶å‘æ•°
            "description": "ä¸“ç”¨å·¥ä½œæµåè°ƒWorker",
        },
        "unified": {
            "queues": all_queues,
            "concurrency": 2,  # ç»Ÿä¸€workerï¼Œé€‚åº¦å¹¶å‘
            "description": "ç»Ÿä¸€å¤„ç†Workerï¼ˆæ¨èï¼‰",
        },
    }

    if custom_queues:
        # è‡ªå®šä¹‰é˜Ÿåˆ—
        return {"queues": custom_queues.split(","), "description": "è‡ªå®šä¹‰é˜Ÿåˆ—Worker"}
    elif specialized:
        # ä¸“ç”¨worker
        return specialized_configs.get(
            specialized, {"queues": all_queues, "description": "é€šç”¨Worker"}
        )
    else:
        # é»˜è®¤ï¼šç»Ÿä¸€workerç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
        return {
            "queues": all_queues, 
            "concurrency": 2,  # é»˜è®¤å¹¶å‘æ•°ï¼Œå¹³è¡¡æ€§èƒ½å’Œèµ„æºä½¿ç”¨
            "description": "ç»Ÿä¸€å¤„ç†Workerï¼ˆé»˜è®¤ï¼‰"
        }


def setup_enhanced_logging(loglevel="info"):
    """è®¾ç½®å¢å¼ºçš„æ—¥å¿—è®°å½•"""
    import logging
    from datetime import datetime
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    numeric_level = getattr(logging, loglevel.upper(), logging.INFO)
    
    # åˆ›å»ºè¯¦ç»†çš„æ—¥å¿—æ ¼å¼
    log_format = "[%(asctime)s: %(levelname)s/%(processName)s] %(name)s:%(lineno)d | %(message)s"
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    logging.basicConfig(
        level=numeric_level,
        format=log_format,
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    
    # è®¾ç½® Celery ç›¸å…³æ—¥å¿—
    celery_logger = logging.getLogger('celery')
    celery_logger.setLevel(numeric_level)
    
    # è®¾ç½®ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
    task_logger = logging.getLogger('celery.task')
    task_logger.setLevel(numeric_level)
    
    print(f"ğŸ“‹ æ—¥å¿—é…ç½®:")
    print(f"  - æ—¥å¿—çº§åˆ«: {loglevel.upper()}")
    print(f"  - æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  - è¿›ç¨‹åç§°: åŒ…å«åœ¨æ—¥å¿—ä¸­")
    print()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ Celery Worker"""

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    print("ğŸš€ å¯åŠ¨ Celery Worker - æ¶æ„ä¼˜åŒ–ç‰ˆ")
    print("=" * 60)

    # è®¾ç½®macOSå…¼å®¹æ€§é…ç½®
    is_macos = setup_macos_compatibility()

    # è®¾ç½®å¢å¼ºæ—¥å¿—
    setup_enhanced_logging(args.loglevel)

    # åˆ›å»º Celery åº”ç”¨
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Celery åº”ç”¨...")
    app = create_celery_app()
    print("âœ… Celery åº”ç”¨åˆå§‹åŒ–å®Œæˆ")

    # è·å–é…ç½®
    config = get_config()
    queue_config = get_queue_config(args.specialized, args.queues)

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"ğŸ“‹ Worker é…ç½®ä¿¡æ¯:")
    print(f"  - Workerç±»å‹: {queue_config.get('description', 'é€šç”¨Worker')}")
    print(f"  - Broker URL: {config.celery.broker_url}")
    print(f"  - Result Backend: {config.celery.result_backend}")
    print(f"  - App Name: {config.celery.app_name}")

    # å¹¶å‘é…ç½®
    concurrency = (
        args.concurrency
        or queue_config.get("concurrency")
        or config.celery.worker_concurrency
    )
    
    # è·å–å®‰å…¨çš„poolç±»å‹ï¼ˆmacOSå…¼å®¹ï¼‰
    safe_pool_type = get_safe_pool_type(args.pool)
    
    print(f"  - å¹¶å‘æ•°: {concurrency}")
    print(f"  - æ± ç±»å‹: {safe_pool_type}")
    if safe_pool_type != args.pool:
        print(f"    (åŸå§‹è¯·æ±‚: {args.pool}, å·²è°ƒæ•´ä¸ºmacOSå…¼å®¹)")

    # é˜Ÿåˆ—é…ç½®
    print(f"  - ç›‘å¬é˜Ÿåˆ—: {', '.join(queue_config['queues'])}")
    print()

    # æ˜¾ç¤ºå·²æ³¨å†Œçš„ä»»åŠ¡
    print("ğŸ“ å·²æ³¨å†Œçš„ä»»åŠ¡:")
    registered_tasks = list(app.tasks.keys())
    task_count = 0
    for task_name in sorted(registered_tasks):
        if not task_name.startswith("celery."):  # è·³è¿‡ Celery å†…ç½®ä»»åŠ¡
            print(f"  âœ… {task_name}")
            task_count += 1
    print(f"  ğŸ“Š æ€»è®¡: {task_count} ä¸ªä»»åŠ¡")
    print()

    # æ˜¾ç¤ºé˜Ÿåˆ—è·¯ç”±ä¿¡æ¯
    print("ğŸ”€ ä»»åŠ¡è·¯ç”±é…ç½®:")
    routes = app.conf.task_routes
    route_count = 0
    for pattern, route_config in routes.items():
        queue = route_config.get("queue", "default")
        print(f"  ğŸ“‚ {pattern} â†’ {queue}")
        route_count += 1
    print(f"  ğŸ“Š æ€»è®¡: {route_count} ä¸ªè·¯ç”±è§„åˆ™")
    print()

    print("ğŸ”„ Worker æ­£åœ¨ç›‘å¬ä»»åŠ¡...")
    print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢ Worker")
    print("=" * 60)

    # å¯åŠ¨ Worker
    worker_kwargs = {
        "loglevel": args.loglevel,
        "concurrency": concurrency,
        "queues": queue_config["queues"],
        "prefetch_multiplier": config.celery.worker_prefetch_multiplier,
        "max_tasks_per_child": (
            args.max_tasks_per_child or config.celery.worker_max_tasks_per_child
        ),
        "time_limit": config.celery.task_time_limit,
        "soft_time_limit": config.celery.task_soft_time_limit,
        "pool": safe_pool_type,
    }

    print(f"ğŸ”§ Workerå¯åŠ¨å‚æ•°è¯¦æƒ…:")
    print(f"  - é¢„å–ä¹˜æ•°: {worker_kwargs['prefetch_multiplier']}")
    print(f"  - æ¯ä¸ªå­è¿›ç¨‹æœ€å¤§ä»»åŠ¡æ•°: {worker_kwargs['max_tasks_per_child']}")
    print(f"  - ä»»åŠ¡æ—¶é—´é™åˆ¶: {worker_kwargs['time_limit']}ç§’")
    print(f"  - è½¯æ—¶é—´é™åˆ¶: {worker_kwargs['soft_time_limit']}ç§’")
    print(f"  - å®é™…æ± ç±»å‹: {worker_kwargs['pool']}")
    print()

    worker = app.Worker(**worker_kwargs)

    try:
        print("ğŸš€ æ­£åœ¨å¯åŠ¨ Celery Worker...")
        worker.start()
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­ Worker...")
        worker.stop()
        print("âœ… Worker å·²å®‰å…¨åœæ­¢")
    except Exception as e:
        print(f"\nâŒ Worker å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
