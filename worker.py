#!/usr/bin/env python3
"""
Celery Worker å¯åŠ¨è„šæœ¬ - æ¶æ„ä¼˜åŒ–ç‰ˆ

è¿™ä¸ªè„šæœ¬ç”¨äºå¯åŠ¨ Celery Worker è¿›ç¨‹æ¥ç›‘å¬å’Œå¤„ç†å¼‚æ­¥ä»»åŠ¡ã€‚
æ”¯æŒæ–°çš„ä»»åŠ¡åˆ†ç¦»æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š
1. ç‹¬ç«‹çš„æ–‡æ¡£åˆ›å»ºä»»åŠ¡ (document_queue)
2. ç‹¬ç«‹çš„RAGå¤„ç†ä»»åŠ¡ (rag_queue) 
3. æ–‡ä»¶å¤„ç†ä»»åŠ¡ (file_queue)
4. å·¥ä½œæµåè°ƒä»»åŠ¡ (workflow_queue)

Worker è¿›ç¨‹ä¼šï¼š
1. è¿æ¥åˆ° Redis æ¶ˆæ¯é˜Ÿåˆ—
2. ç›‘å¬æŒ‡å®šé˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
3. æ ¹æ®ä»»åŠ¡è·¯ç”±è§„åˆ™åˆ†å‘ä»»åŠ¡åˆ°å¯¹åº”çš„å¤„ç†å™¨
4. æ‰§è¡Œä»»åŠ¡å¹¶è¿”å›ç»“æœ

ä½¿ç”¨æ–¹æ³•:
python worker.py [é€‰é¡¹]

ç¤ºä¾‹:
python worker.py --loglevel=info
python worker.py --queues=document_queue,file_queue,rag_queue --concurrency=4
python worker.py --specialized=document  # ä¸“ç”¨æ–‡æ¡£å¤„ç†worker
python worker.py --specialized=rag       # ä¸“ç”¨RAGå¤„ç†worker
"""

import sys
import os
import logging
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from modules.services.task_service import CeleryTaskService
from config import get_config

# ç¡®ä¿æ–°çš„ä»»åŠ¡å¤„ç†å™¨è¢«åŠ è½½
try:
    from modules.tasks.handlers.document_handlers import DocumentCreateHandler
    from modules.tasks.handlers.rag_handlers import AsyncDocumentProcessingHandler
    from modules.tasks.handlers.file_handlers_v2 import OptimizedFileUploadCompleteHandler
    print("âœ… æ–°æ¶æ„ä»»åŠ¡å¤„ç†å™¨åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âš ï¸  æ–°æ¶æ„ä»»åŠ¡å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
    print("   å°†ä½¿ç”¨åŸæœ‰ä»»åŠ¡å¤„ç†å™¨")

logger = logging.getLogger(__name__)


def create_celery_app():
    """åˆ›å»ºå¹¶é…ç½® Celery åº”ç”¨"""
    config = get_config()
    
    # åˆ›å»ºä»»åŠ¡æœåŠ¡å®ä¾‹
    task_service = CeleryTaskService(
        broker_url=config.celery.broker_url,
        result_backend=config.celery.result_backend,
        app_name=config.celery.app_name
    )
    
    # åŒæ­¥åˆå§‹åŒ–ï¼ˆæ³¨å†Œä»»åŠ¡å¤„ç†å™¨ï¼‰
    import asyncio
    asyncio.run(task_service.initialize())
    
    # æ›´æ–°ä»»åŠ¡è·¯ç”±é…ç½®ä»¥æ”¯æŒæ–°æ¶æ„
    app = task_service.app
    app.conf.update(
        task_routes={
            # æ–‡æ¡£ç›¸å…³ä»»åŠ¡
            'document.create': {'queue': 'document_queue'},
            'document.update_metadata': {'queue': 'document_queue'},
            
            # RAGç›¸å…³ä»»åŠ¡  
            'rag.process_document_async': {'queue': 'rag_queue'},
            'rag.process_document': {'queue': 'rag_queue'},
            'rag.generate_embeddings': {'queue': 'rag_queue'},
            'rag.store_vectors': {'queue': 'rag_queue'},
            'rag.semantic_search': {'queue': 'rag_queue'},
            'rag.cleanup_document': {'queue': 'rag_queue'},
            
            # æ–‡ä»¶å¤„ç†ä»»åŠ¡
            'TaskName.FILE_UPLOAD_CONFIRM': {'queue': 'file_queue'},
            'file.analyze_content': {'queue': 'file_queue'},
            'file.cleanup_temp': {'queue': 'file_queue'},
            'file.convert_format': {'queue': 'file_queue'},
            'file.workflow_status': {'queue': 'file_queue'},
            'file.cancel_workflow': {'queue': 'file_queue'},
            
            # å·¥ä½œæµä»»åŠ¡
            'workflow.*': {'queue': 'workflow_queue'},
        },
        
        # é˜Ÿåˆ—ä¼˜å…ˆçº§é…ç½®
        task_queue_max_priority=10,
        task_default_priority=5,
        worker_prefetch_multiplier=1,
        
        # ç»“æœé…ç½®
        result_expires=3600,  # ç»“æœä¿å­˜1å°æ—¶
        task_track_started=True,
        task_send_events=True,
        
        # åºåˆ—åŒ–é…ç½®
        task_serializer='json',
        result_serializer='json',
        accept_content=['json'],
        
        # æ—¶é—´é…ç½®
        timezone='UTC',
        enable_utc=True,
    )
    
    return app


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Celery Worker - æ¶æ„ä¼˜åŒ–ç‰ˆ")
    
    parser.add_argument('--loglevel', default='info', 
                       choices=['debug', 'info', 'warning', 'error'],
                       help='æ—¥å¿—çº§åˆ« (default: info)')
    
    parser.add_argument('--concurrency', type=int, default=None,
                       help='å¹¶å‘æ•° (default: ä»é…ç½®æ–‡ä»¶è¯»å–)')
    
    parser.add_argument('--queues', default=None,
                       help='ç›‘å¬çš„é˜Ÿåˆ—ï¼Œé€—å·åˆ†éš” (default: æ‰€æœ‰é˜Ÿåˆ—)')
    
    parser.add_argument('--specialized', choices=['document', 'rag', 'file', 'workflow'],
                       help='ä¸“ç”¨workerç±»å‹')
    
    parser.add_argument('--max-tasks-per-child', type=int, default=None,
                       help='æ¯ä¸ªworkerè¿›ç¨‹å¤„ç†çš„æœ€å¤§ä»»åŠ¡æ•°')
    
    parser.add_argument('--pool', default='prefork',
                       choices=['prefork', 'eventlet', 'gevent', 'solo'],
                       help='Workeræ± ç±»å‹ (default: prefork)')
    
    return parser.parse_args()


def get_queue_config(specialized=None, custom_queues=None):
    """è·å–é˜Ÿåˆ—é…ç½®"""
    
    # æ‰€æœ‰å¯ç”¨é˜Ÿåˆ—
    all_queues = [
        'default',           # é»˜è®¤é˜Ÿåˆ—
        'document_queue',    # æ–‡æ¡£åˆ›å»ºé˜Ÿåˆ—
        'rag_queue',        # RAGå¤„ç†é˜Ÿåˆ—
        'file_queue',       # æ–‡ä»¶å¤„ç†é˜Ÿåˆ—
        'workflow_queue',   # å·¥ä½œæµé˜Ÿåˆ—
        'notification_queue' # é€šçŸ¥é˜Ÿåˆ—
    ]
    
    # ä¸“ç”¨workeré…ç½®
    specialized_configs = {
        'document': {
            'queues': ['document_queue', 'default'],
            'concurrency': 4,
            'description': 'ä¸“ç”¨æ–‡æ¡£å¤„ç†Worker'
        },
        'rag': {
            'queues': ['rag_queue'],
            'concurrency': 2,  # RAGä»»åŠ¡é€šå¸¸æ¶ˆè€—æ›´å¤šèµ„æº
            'description': 'ä¸“ç”¨RAGå¤„ç†Worker'
        },
        'file': {
            'queues': ['file_queue', 'default'],
            'concurrency': 3,
            'description': 'ä¸“ç”¨æ–‡ä»¶å¤„ç†Worker'
        },
        'workflow': {
            'queues': ['workflow_queue', 'default'],
            'concurrency': 2,
            'description': 'ä¸“ç”¨å·¥ä½œæµåè°ƒWorker'
        }
    }
    
    if custom_queues:
        # è‡ªå®šä¹‰é˜Ÿåˆ—
        return {
            'queues': custom_queues.split(','),
            'description': 'è‡ªå®šä¹‰é˜Ÿåˆ—Worker'
        }
    elif specialized:
        # ä¸“ç”¨worker
        return specialized_configs.get(specialized, {
            'queues': all_queues,
            'description': 'é€šç”¨Worker'
        })
    else:
        # é»˜è®¤ï¼šç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
        return {
            'queues': all_queues,
            'description': 'é€šç”¨Worker'
        }


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ Celery Worker"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print("ğŸš€ å¯åŠ¨ Celery Worker - æ¶æ„ä¼˜åŒ–ç‰ˆ")
    print("=" * 60)
    
    # åˆ›å»º Celery åº”ç”¨
    app = create_celery_app()
    
    # è·å–é…ç½®
    config = get_config()
    queue_config = get_queue_config(args.specialized, args.queues)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"ğŸ“‹ Worker é…ç½®ä¿¡æ¯:")
    print(f"  - Workerç±»å‹: {queue_config.get('description', 'é€šç”¨Worker')}")
    print(f"  - Broker URL: {config.celery.broker_url}")
    print(f"  - Result Backend: {config.celery.result_backend}")
    print(f"  - App Name: {config.celery.app_name}")
    
    # å¹¶å‘é…ç½®
    concurrency = (args.concurrency or 
                  queue_config.get('concurrency') or 
                  config.celery.worker_concurrency)
    print(f"  - å¹¶å‘æ•°: {concurrency}")
    print(f"  - æ± ç±»å‹: {args.pool}")
    
    # é˜Ÿåˆ—é…ç½®
    print(f"  - ç›‘å¬é˜Ÿåˆ—: {', '.join(queue_config['queues'])}")
    print()
    
    # æ˜¾ç¤ºå·²æ³¨å†Œçš„ä»»åŠ¡
    print("ğŸ“ å·²æ³¨å†Œçš„ä»»åŠ¡:")
    registered_tasks = list(app.tasks.keys())
    task_count = 0
    for task_name in sorted(registered_tasks):
        if not task_name.startswith('celery.'):  # è·³è¿‡ Celery å†…ç½®ä»»åŠ¡
            print(f"  âœ… {task_name}")
            task_count += 1
    print(f"  ğŸ“Š æ€»è®¡: {task_count} ä¸ªä»»åŠ¡")
    print()
    
    # æ˜¾ç¤ºé˜Ÿåˆ—è·¯ç”±ä¿¡æ¯
    print("ğŸ”€ ä»»åŠ¡è·¯ç”±é…ç½®:")
    routes = app.conf.task_routes
    route_count = 0
    for pattern, route_config in routes.items():
        queue = route_config.get('queue', 'default')
        print(f"  ğŸ“‚ {pattern} â†’ {queue}")
        route_count += 1
    print(f"  ğŸ“Š æ€»è®¡: {route_count} ä¸ªè·¯ç”±è§„åˆ™")
    print()
    
    print("ğŸ”„ Worker æ­£åœ¨ç›‘å¬ä»»åŠ¡...")
    print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢ Worker")
    print("=" * 60)
    
    # å¯åŠ¨ Worker
    worker_kwargs = {
        'loglevel': args.loglevel,
        'concurrency': concurrency,
        'queues': queue_config['queues'],
        'prefetch_multiplier': config.celery.worker_prefetch_multiplier,
        'max_tasks_per_child': (args.max_tasks_per_child or 
                               config.celery.worker_max_tasks_per_child),
        'time_limit': config.celery.task_time_limit,
        'soft_time_limit': config.celery.task_soft_time_limit,
        'pool': args.pool,
    }
    
    worker = app.Worker(**worker_kwargs)
    
    try:
        worker.start()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Worker åœæ­¢")
        worker.stop()


if __name__ == '__main__':
    main()
